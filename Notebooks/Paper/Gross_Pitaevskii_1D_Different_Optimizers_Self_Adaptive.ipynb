{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "efbqfOtOZasn",
      "metadata": {
        "id": "efbqfOtOZasn"
      },
      "source": [
        "This notebook implements supervised weights for approximating the solution to the one-dimensional Gross-Pitavskii equation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3nYdXyrr-Z7h",
      "metadata": {
        "id": "3nYdXyrr-Z7h"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch-optimizer\n",
        "!git clone https://github.com/facebookresearch/optimizers.git\n",
        "%cd optimizers\n",
        "!pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "10ZOj98FDjO5"
      },
      "id": "10ZOj98FDjO5",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "BMnx60Sj-Z7j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnx60Sj-Z7j",
        "outputId": "f6f4350a-6542-4cf6-d17b-fb03cce9a977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.autograd import grad\n",
        "from scipy.special import hermite\n",
        "# from adabelief_pytorch import AdaBelief\n",
        "from pytorch_optimizer import QHAdam, AdaHessian, Ranger21, SophiaH, Shampoo\n",
        "from distributed_shampoo import AdamGraftingConfig, DistributedShampoo, DefaultEigenvalueCorrectedShampooConfig\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hglIb1ul-Z7n",
      "metadata": {
        "id": "hglIb1ul-Z7n"
      },
      "source": [
        "# Physics Informed Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "NFVJh4jB-Z7o",
      "metadata": {
        "id": "NFVJh4jB-Z7o"
      },
      "outputs": [],
      "source": [
        "class GrossPitaevskiiPINN(nn.Module):\n",
        "    \"\"\"\n",
        "    Physics-Informed Neural Network (PINN) for solving the 1D Gross-Pitaevskii Equation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, hbar=1.0, m=1.0, g=100.0, mode=0, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        layers : list of int\n",
        "            Neural network architecture, each entry defines the number of neurons in that layer.\n",
        "        hbar : float, optional\n",
        "            Reduced Planck's constant (default is 1.0).\n",
        "        m : float, optional\n",
        "            Mass of the particle (default is 1.0).\n",
        "        g : float, optional\n",
        "            Interaction strength (default is 100.0).\n",
        "        mode : int, optional\n",
        "            Mode number (default is 0).\n",
        "        prev_prediction : callable, optional\n",
        "            A function representing the previous model's forward pass, used to incorporate past solutions\n",
        "            when computing the current solution (default is None).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.network = self.build_network()\n",
        "        self.g = g  # Interaction strength\n",
        "        self.hbar = hbar  # Planck's constant, fixed\n",
        "        self.m = m  # Particle mass, fixed\n",
        "        self.mode = mode  # Mode number (n)\n",
        "\n",
        "        # Trainable log-scale weights for adaptive balancing\n",
        "        self.log_alpha = nn.Parameter(torch.zeros(5, dtype=torch.float32))\n",
        "\n",
        "    def build_network(self):\n",
        "        \"\"\"\n",
        "        Build the neural network with sine activation functions between layers.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        nn.Sequential\n",
        "            A PyTorch sequential model representing the neural network architecture.\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            layers.append(nn.Linear(self.layers[i], self.layers[i + 1]))\n",
        "            if i < len(self.layers) - 2:\n",
        "                layers.append(nn.Tanh())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def weighted_hermite(self, x, n):\n",
        "        \"\"\"\n",
        "        Compute the weighted Hermite polynomial solution for the linear case (gamma = 0).\n",
        "        Equation (34) in https://www.sciencedirect.com/science/article/abs/pii/S0010465513001318.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of spatial coordinates (collocation points) or boundary points.\n",
        "        n : int\n",
        "            Mode of ground state solution to Gross-Pitavskii equation (0 for base ground state)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The weighted Hermite polynomial solution for the linear case (gamma = 0).\n",
        "        \"\"\"\n",
        "        H_n = hermite(n)(x.cpu().detach().numpy())  # Hermite polynomial evaluated at x\n",
        "        norm_factor = (2**n * math.factorial(n) * np.sqrt(np.pi))**(-0.5)\n",
        "        weighted_hermite = norm_factor * torch.exp(-x**2 / 2) * torch.tensor(H_n, dtype=torch.float32).to(device)\n",
        "\n",
        "        return weighted_hermite\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the neural network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor containing spatial points (collocation points).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor representing the predicted solution.\n",
        "        \"\"\"\n",
        "        return self.network(inputs)\n",
        "\n",
        "    def compute_potential(self, x, potential_type=\"harmonic\", **kwargs):\n",
        "        \"\"\"\n",
        "        Compute a symmetric or asymmetric potential function for the 1D domain.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of spatial coordinates.\n",
        "        potential_type : str, optional\n",
        "            Type of potential. Options are: \"harmonic\".\n",
        "        kwargs : dict\n",
        "            Additional parameters specific to each potential type.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        V : torch.Tensor\n",
        "            Tensor of potential values at the input points.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the potential type is not recognized.\n",
        "        \"\"\"\n",
        "        if potential_type == \"harmonic\":\n",
        "            omega = kwargs.get('omega', 1.0)  # Frequency for harmonic potential\n",
        "            V = 0.5 * omega ** 2 * x ** 2\n",
        "        elif potential_type == \"gaussian\":\n",
        "            a = kwargs.get('a', 0.0)  # Center of the Gaussian\n",
        "            V = torch.exp(-(x - a) ** 2)\n",
        "        elif potential_type == \"periodic\":\n",
        "            V0 = kwargs.get('V0', 1.0)  # Depth of the potential\n",
        "            k = kwargs.get('k', 2 * np.pi / 5.0)  # Wave number for periodic potential\n",
        "            V = V0 * torch.cos(k * x) ** 2\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown potential type: {potential_type}\")\n",
        "\n",
        "        return V\n",
        "\n",
        "    def compute_thomas_fermi_approx(self, lambda_pde, potential, eta):\n",
        "        \"\"\"\n",
        "        Calculate the Thomas–Fermi approximation for the given potential.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lambda_pde : float\n",
        "            Eigenvalue from lowest enegry ground state.\n",
        "        potential : torch.Tensor\n",
        "            Potential values corresponding to the spatial coordinates.\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Thomas–Fermi approximation of the wave function.\n",
        "        \"\"\"\n",
        "        tf_approx = torch.sqrt(torch.relu((lambda_pde - potential) / eta))\n",
        "        return tf_approx\n",
        "\n",
        "    def boundary_loss(self, boundary_points, boundary_values, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Compute the boundary loss (MSE) for the boundary conditions.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        boundary_points : torch.Tensor\n",
        "            Input tensor of boundary spatial points.\n",
        "        boundary_values : torch.Tensor\n",
        "            Tensor of boundary values (for Dirichlet conditions).\n",
        "        prev_prediction : GrossPitaevskiiPINN or None\n",
        "            Previously trained model whose predictions are used as part of the training process.\n",
        "            If None, the model starts training from scratch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Mean squared error (MSE) at the boundary points.\n",
        "        \"\"\"\n",
        "        u_pred = self.forward(boundary_points)\n",
        "\n",
        "        if prev_prediction is None:\n",
        "            u = self.weighted_hermite(boundary_points, self.mode) + u_pred\n",
        "        else:\n",
        "            u = prev_prediction(boundary_points) + u_pred  # Use model’s output from previous eta\n",
        "\n",
        "        return torch.mean((u - boundary_values) ** 2)\n",
        "\n",
        "    def riesz_loss(self, predictions, inputs, eta, potential_type, precomputed_potential=None, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Compute the Riesz energy loss for the Gross-Pitaevskii equation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : torch.Tensor\n",
        "            Predicted solution from the network.\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor of spatial coordinates (collocation points).\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "        potential_type : str\n",
        "            Type of potential function to use.\n",
        "        precomputed_potential : torch.Tensor\n",
        "            Precomputed potential. Default is None.\n",
        "        prev_prediction : GrossPitaevskiiPINN or None\n",
        "            Previously trained model whose predictions are used as part of the training process.\n",
        "            If None, the model starts training from scratch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Riesz energy loss value.\n",
        "        \"\"\"\n",
        "        if prev_prediction is None:\n",
        "            u = self.weighted_hermite(inputs, self.mode) + predictions\n",
        "        else:\n",
        "            scaling_factor = torch.sqrt(torch.tensor(eta, dtype=torch.float32, device=device))\n",
        "            u = prev_prediction(inputs) + predictions  # Use model’s output from previous eta\n",
        "\n",
        "        if not inputs.requires_grad:\n",
        "            inputs = inputs.clone().detach().requires_grad_(True)\n",
        "        u_x = torch.autograd.grad(outputs=u, inputs=inputs,\n",
        "                                  grad_outputs=torch.ones_like(predictions),\n",
        "                                  create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        laplacian_term = torch.mean(u_x ** 2)  # Kinetic term\n",
        "        if precomputed_potential is not None:\n",
        "            V = precomputed_potential\n",
        "        else:\n",
        "            V = self.compute_potential(inputs, potential_type)\n",
        "        potential_term = torch.mean(V * u ** 2)  # Potential term\n",
        "        interaction_term = 0.5 * eta * torch.mean(u ** 4)  # Interaction term\n",
        "\n",
        "        riesz_energy = 0.5 * (laplacian_term + potential_term + interaction_term)\n",
        "\n",
        "        return riesz_energy\n",
        "\n",
        "    def pde_loss(self, inputs, predictions, eta, potential_type, precomputed_potential=None, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Compute the PDE loss for the Gross-Pitaevskii equation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor of spatial coordinates (collocation points).\n",
        "        predictions : torch.Tensor\n",
        "            Predicted solution from the network.\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "        potential_type : str\n",
        "            Type of potential function to use.\n",
        "        precomputed_potential : torch.Tensor\n",
        "            Precomputed potential. Default is None.\n",
        "        prev_prediction : GrossPitaevskiiPINN or None\n",
        "            Previously trained model whose predictions are used as part of the training process.\n",
        "            If None, the model starts training from scratch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            Tuple containing:\n",
        "                - torch.Tensor: PDE loss value.\n",
        "                - torch.Tensor: PDE residual.\n",
        "                - torch.Tensor: Smallest eigenvalue (lambda).\n",
        "        \"\"\"\n",
        "        if prev_prediction is None:\n",
        "            u = self.weighted_hermite(inputs, self.mode) + predictions\n",
        "        else:\n",
        "            scaling_factor = torch.sqrt(torch.tensor(eta, dtype=torch.float32, device=device))\n",
        "            u = prev_prediction(inputs) + predictions # Use model’s output from previous eta\n",
        "\n",
        "        # Compute first and second derivatives with respect to x\n",
        "        u_x = grad(u, inputs, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        u_xx = grad(u_x, inputs, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
        "\n",
        "        # Compute λ from the energy functional\n",
        "        if precomputed_potential is not None:\n",
        "            V = precomputed_potential\n",
        "        else:\n",
        "            V = self.compute_potential(inputs, potential_type)\n",
        "        lambda_pde = torch.mean(u_x ** 2 + V * u ** 2 + eta * u ** 4) / torch.mean(u ** 2)\n",
        "\n",
        "        # Residual of the 1D Gross-Pitaevskii equation\n",
        "        if precomputed_potential is not None:\n",
        "            V = precomputed_potential\n",
        "        else:\n",
        "            V = self.compute_potential(inputs, potential_type)\n",
        "        pde_residual = -u_xx + V * u + eta * torch.abs(u ** 2) * u - lambda_pde * u\n",
        "\n",
        "        # Regularization: See https://arxiv.org/abs/2010.05075\n",
        "\n",
        "        # Term 1: L_f = 1 / (f(x, λ))^2, penalizes the network if the PDE residual is close to zero to avoid trivial eigenfunctions\n",
        "        L_f = 1 / (torch.mean(u ** 2) + 1e-2)\n",
        "\n",
        "        # Term 2: L_λ = 1 / λ^2, penalizes small eigenvalues λ, ensuring non-trivial eigenvalues\n",
        "        L_lambda = 1 / (lambda_pde ** 2 + 1e-6)\n",
        "\n",
        "        # Term 3: L_drive = e^(-λ + c), encourages λ to grow, preventing collapse to small values\n",
        "        L_drive = torch.exp(-lambda_pde + 1.0)\n",
        "\n",
        "        # PDE loss (residual plus regularization terms)\n",
        "        pde_loss = torch.mean(pde_residual ** 2)  #+ L_lambda + L_f\n",
        "\n",
        "        return pde_loss, pde_residual, lambda_pde\n",
        "\n",
        "    def symmetry_loss(self, collocation_points, lb, ub):\n",
        "        \"\"\"\n",
        "        Compute the symmetry loss to enforce u(x) = u((a+b)-x).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        collocation_points : torch.Tensor\n",
        "            Tensor of interior spatial points.\n",
        "        lb : torch.Tensor\n",
        "            Lower bound of interval.\n",
        "        ub: torch.Tensor\n",
        "            Upper bound of interval.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sym_loss : torch.Tensor\n",
        "            The mean squared error enforcing symmetry u(x) = u((a+b)-x).\n",
        "        \"\"\"\n",
        "        # Reflect points across the center of the domain\n",
        "        x_reflected = (lb + ub) - collocation_points\n",
        "\n",
        "        # Evaluate u(x) and u((a+b)-x)\n",
        "        u_original = self.forward(collocation_points)\n",
        "        u_reflected = self.forward(x_reflected)\n",
        "\n",
        "        # Compute MSE to enforce symmetry\n",
        "        sym_loss = torch.mean((u_original - u_reflected) ** 2)\n",
        "        return sym_loss\n",
        "\n",
        "    def total_loss(self, collocation_points, boundary_points, boundary_values, eta, lb, ub, weights, potential_type,\n",
        "                   precomputed_potential=None, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Compute the total loss combining multiple loss terms with self-adaptive weight balancing.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        collocation_points : torch.Tensor\n",
        "            Tensor of spatial coordinates for interior points.\n",
        "        boundary_points : torch.Tensor\n",
        "            Tensor of spatial coordinates at domain boundaries.\n",
        "        boundary_values : torch.Tensor\n",
        "            Expected function values at boundary points (Dirichlet conditions).\n",
        "        eta : float\n",
        "            Interaction strength parameter.\n",
        "        lb : torch.Tensor\n",
        "            Lower bound of the spatial domain.\n",
        "        ub : torch.Tensor\n",
        "            Upper bound of the spatial domain.\n",
        "        potential_type : str\n",
        "            Type of potential function used in the PDE.\n",
        "        precomputed_potential : torch.Tensor, optional\n",
        "            Precomputed potential values for efficiency (default: None, computed dynamically).\n",
        "        prev_prediction : GrossPitaevskiiPINN, optional\n",
        "            A previously trained model to incorporate past solutions (default: None).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        total_loss : torch.Tensor\n",
        "            Combined loss value weighted by dynamically learned coefficients.\n",
        "        data_loss : torch.Tensor\n",
        "            Loss enforcing boundary conditions.\n",
        "        riesz_energy_loss : torch.Tensor\n",
        "            Loss based on the Riesz energy formulation.\n",
        "        pde_loss : torch.Tensor\n",
        "            Loss from the PDE residual enforcement.\n",
        "        norm_loss : torch.Tensor\n",
        "            Regularization loss ensuring wave function normalization.\n",
        "        sym_loss : torch.Tensor\n",
        "            Loss enforcing domain symmetry.\n",
        "        alpha : torch.Tensor\n",
        "            Adaptive weights for balancing individual loss terms.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use precomputed potential if provided\n",
        "        if precomputed_potential is not None:\n",
        "            V = precomputed_potential\n",
        "        else:\n",
        "            V = self.compute_potential(collocation_points, potential_type)\n",
        "\n",
        "        # Compute individual loss components\n",
        "        data_loss = self.boundary_loss(boundary_points, boundary_values, prev_prediction)\n",
        "        riesz_energy_loss = self.riesz_loss(self.forward(collocation_points), collocation_points, eta, potential_type,V, prev_prediction)\n",
        "        pde_loss, _, _ = self.pde_loss(collocation_points, self.forward(collocation_points), eta, potential_type, V, prev_prediction)\n",
        "        norm_loss = (torch.norm(self.forward(collocation_points), p=2) - 1) ** 2\n",
        "        sym_loss = self.symmetry_loss(collocation_points, lb, ub)\n",
        "\n",
        "        # Compute self-adaptive weights\n",
        "        alpha = torch.exp(self.log_alpha)\n",
        "\n",
        "        # Scaling factor for pde loss and riesz energy loss\n",
        "        domain_length = ub - lb\n",
        "\n",
        "        # Compute weighted losses and total loss\n",
        "        losses = [data_loss, riesz_energy_loss  / domain_length, pde_loss / domain_length, norm_loss, sym_loss]\n",
        "        weighted_losses = [weights[i] * alpha[i] * loss for i, loss in enumerate(losses)]\n",
        "        #weighted_losses = [alpha[i] * losses[i] for i in range(len(losses))]\n",
        "        total_loss = sum(weighted_losses)\n",
        "\n",
        "        return total_loss, data_loss, riesz_energy_loss, pde_loss, norm_loss, sym_loss, alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzMOyXAj7jey",
      "metadata": {
        "id": "FzMOyXAj7jey"
      },
      "source": [
        "# Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "GyMrRGdE7lB2",
      "metadata": {
        "id": "GyMrRGdE7lB2"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    \"\"\"\n",
        "    Initialize the weights of the neural network layers using Xavier uniform initialization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : torch.nn.Module\n",
        "        A layer of the neural network. If it is a linear layer, its weights and biases are initialized.\n",
        "    \"\"\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AKKgTNHo24k4",
      "metadata": {
        "id": "AKKgTNHo24k4"
      },
      "source": [
        "# Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1kAWdOgL26tv",
      "metadata": {
        "id": "1kAWdOgL26tv"
      },
      "outputs": [],
      "source": [
        "def prepare_training_data(N_u, N_f, lb, ub):\n",
        "    \"\"\"\n",
        "    Prepare boundary and collocation points for training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_u : int\n",
        "        Number of boundary points.\n",
        "    N_f : int\n",
        "        Number of collocation points.\n",
        "    lb : np.ndarray\n",
        "        Lower bounds of the domain.\n",
        "    ub : np.ndarray\n",
        "        Upper bounds of the domain.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    collocation_points : np.ndarray\n",
        "        Collocation points.\n",
        "    boundary_points : np.ndarray\n",
        "        Boundary points.\n",
        "    boundary_values : np.ndarray\n",
        "        Boundary values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Boundary of interval\n",
        "    boundary_points = np.array([[lb], [ub]])\n",
        "    boundary_values = np.zeros((2, 1))\n",
        "\n",
        "    # Dynamically sample points inside the interval\n",
        "    collocation_points = np.random.rand(N_f, 1) * (ub - lb) + lb\n",
        "\n",
        "    return collocation_points, boundary_points, boundary_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B0dvfxBOmPlR",
      "metadata": {
        "id": "B0dvfxBOmPlR"
      },
      "source": [
        "# Train PINN with Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "FXsGZVhYmS1t",
      "metadata": {
        "id": "FXsGZVhYmS1t"
      },
      "outputs": [],
      "source": [
        "def train_pinn_with_optimizer(X, N_u, N_f, layers, eta, epochs, lb, ub, weights, model_save_path, potential_type,\n",
        "                              prev_prediction, optimizer_name):\n",
        "    \"\"\"\n",
        "    Train the Physics-Informed Neural Network (PINN) for the 1D Gross-Pitaevskii equation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.ndarray\n",
        "        Input data for the neural network\n",
        "    N_u : int\n",
        "        Number of boundary points\n",
        "    N_f : int\n",
        "        Number of collocation points (interior points) for the physics-based loss\n",
        "    layers : list of int\n",
        "        Architecture of the neural network\n",
        "    eta : float\n",
        "        Interaction strength\n",
        "    epochs: int\n",
        "        Number of epochs\n",
        "    lb : int\n",
        "        Lower bound of interval.\n",
        "    ub : int\n",
        "        Upper bound of interval.\n",
        "    weights : list\n",
        "        Weights for different loss terms.\n",
        "    model_save_path : str\n",
        "        Save path for trained model\n",
        "    potential_type: str\n",
        "        Type of potential function to use\n",
        "    prev_prediction : GrossPitaevskiiPINN or None\n",
        "        Previously trained model whose predictions are used as part of the training process.\n",
        "        If None, the model starts training from scratch.\n",
        "    optimizer_name : str\n",
        "        Name of optimizer to train.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : GrossPitaevskiiPINN\n",
        "        The trained model\n",
        "    loss_history : list of float\n",
        "        List of loss values recorded during training for the specific optimizer.\n",
        "    lambda_pde_history : list of float\n",
        "        List of λ_PDE values recorded during training for the specific optimizer.\n",
        "    \"\"\"\n",
        "    # Instantiate the PINN model for each optimizer and initialize its weights\n",
        "    model = GrossPitaevskiiPINN(layers).to(device)\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "    # Select the optimizer\n",
        "    optimizers = {\n",
        "        \"Adam\": optim.Adam(model.parameters(), lr=1e-3),\n",
        "        \"Shampoo\": DistributedShampoo( model.parameters(),\n",
        "                                       lr=0.001,\n",
        "                                       betas=(0.9, 0.999),\n",
        "                                       epsilon=1e-12,\n",
        "                                       weight_decay=1e-05,\n",
        "                                       max_preconditioner_dim=8192,\n",
        "                                       start_preconditioning_step=100,\n",
        "                                       precondition_frequency=100,\n",
        "                                       use_decoupled_weight_decay=False,\n",
        "                                       grafting_config=AdamGraftingConfig(beta2=0.999,epsilon=1e-08),\n",
        "                                       ),\n",
        "        \"Shampoo-Eigen\": DistributedShampoo(model.parameters(),\n",
        "                                      lr=0.001,\n",
        "                                      betas=(0.9, 0.999),\n",
        "                                      epsilon=1e-12,\n",
        "                                      weight_decay=1e-05,\n",
        "                                      max_preconditioner_dim=8192,\n",
        "                                      start_preconditioning_step=100,\n",
        "                                      precondition_frequency=100,\n",
        "                                      use_decoupled_weight_decay=True,\n",
        "                                      # This can also be set to `DefaultSOAPConfig` which uses QR decompositions, hence is\n",
        "                                      # less expensive and might thereby allow for a smaller `precondition_frequency`.\n",
        "                                      preconditioner_config=DefaultEigenvalueCorrectedShampooConfig\n",
        "                                      )\n",
        "\n",
        "    }\n",
        "\n",
        "    optimizer = optimizers[optimizer_name]\n",
        "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=epochs // 10, epochs=epochs)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=25, factor=0.5, verbose=True)\n",
        "\n",
        "    # Prepare training data (collocation and boundary points)\n",
        "    collocation_points, boundary_points, boundary_values = prepare_training_data(N_u, N_f, lb, ub)\n",
        "\n",
        "    # Convert data to PyTorch tensors and move to device\n",
        "    collocation_points_tensor = torch.tensor(collocation_points, dtype=torch.float32, requires_grad=True).to(device)\n",
        "    boundary_points_tensor = torch.tensor(boundary_points, dtype=torch.float32).to(device)\n",
        "    boundary_values_tensor = torch.tensor(boundary_values, dtype=torch.float32).to(device)\n",
        "    lb_tensor = torch.tensor(lb, dtype=torch.float32).to(device)\n",
        "    ub_tensor = torch.tensor(ub, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Precompute potential\n",
        "    V = model.compute_potential(collocation_points_tensor, potential_type).detach()\n",
        "    V.requires_grad = False\n",
        "\n",
        "    # Precompute Thomas-Fermi approximation\n",
        "    #tf_approx = model.compute_thomas_fermi_approx(collocation_points_tensor, V, eta).detach()\n",
        "    #tf_approx.requires_grad = False\n",
        "\n",
        "    loss_history = []\n",
        "    lambda_pde_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate the total loss (boundary, Riesz energy, PDE, normalization, and symmetry losses)\n",
        "        # with precomputed potential and Thomas-Fermi approximation\n",
        "        loss, data_loss, riesz_energy_loss, pde_loss, norm_loss, sym_loss, alpha = model.total_loss(collocation_points_tensor,\n",
        "                                                                              boundary_points_tensor,\n",
        "                                                                              boundary_values_tensor,\n",
        "                                                                              eta,\n",
        "                                                                              lb_tensor, ub_tensor,\n",
        "                                                                              weights, potential_type, V,\n",
        "                                                                              prev_prediction)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
        "        optimizer.step()\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        # Record the total loss and lambda_pde every 100 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            _, _, lambda_pde = model.pde_loss(\n",
        "                collocation_points_tensor, model.forward(collocation_points_tensor),\n",
        "                eta, potential_type, V, prev_prediction\n",
        "            )\n",
        "            loss_history.append(loss.item())\n",
        "            lambda_pde_history.append(lambda_pde.detach().cpu().item())\n",
        "\n",
        "        # Compute λ_PDE, η, and loss every 1,000 epochs\n",
        "        if epoch % 1000 == 0:\n",
        "            _, _, lambda_pde = model.pde_loss(\n",
        "                collocation_points_tensor, model.forward(collocation_points_tensor),\n",
        "                eta, potential_type, V, prev_prediction\n",
        "            )\n",
        "            print(f\"[{optimizer_name}] Epoch [{epoch}/{epochs}]: η = {eta}, λ_PDE = {lambda_pde.item():.6f},  Loss: {loss.item():.6f}, \"\n",
        "                  f\"Data Loss = {data_loss.item():.6f}, Riesz Loss = {riesz_energy_loss.item():.6f}, \"\n",
        "                  f\"PDE Loss = {pde_loss.item():.6f}, Norm Loss = {norm_loss.item():.6f}, \"\n",
        "                  f\"Sym Loss = {sym_loss.item():.6f}, Adaptive Weights: {alpha.tolist()}\")\n",
        "\n",
        "    return model, loss_history, lambda_pde_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8P5eHv-QCQiE",
      "metadata": {
        "id": "8P5eHv-QCQiE"
      },
      "source": [
        "# Normalize Wave Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "wUq16rY6CUd9",
      "metadata": {
        "id": "wUq16rY6CUd9"
      },
      "outputs": [],
      "source": [
        "def normalize_wave_function(u):\n",
        "    \"\"\"\n",
        "    Normalize the wave function with respect to its maximum value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    u : torch.Tensor\n",
        "        The predicted wave function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The normalized wave function.\n",
        "    \"\"\"\n",
        "    return np.abs(u) / np.max(np.abs(u))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t7r4H0TVkyZb",
      "metadata": {
        "id": "t7r4H0TVkyZb"
      },
      "source": [
        "# Plot Potential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "__9usRWzk15-",
      "metadata": {
        "id": "__9usRWzk15-"
      },
      "outputs": [],
      "source": [
        "def plot_potential_1D(X_test, potential):\n",
        "    \"\"\"\n",
        "    Plot the 1D potential function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_test : np.ndarray\n",
        "        The test points where the potential is computed.\n",
        "    potential : np.ndarray\n",
        "        The computed potential values at the test points.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6, 5))\n",
        "\n",
        "    # X_test is the x-values (positions) of the 1D potential\n",
        "    plt.plot(X_test, potential, label='Potential $V(x)$', color='green')\n",
        "\n",
        "    plt.title('Potential $V(x)$ in 1D')\n",
        "    plt.xlabel('$x$')\n",
        "    plt.ylabel('$V(x)$')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mS-f4Mkzk4KD",
      "metadata": {
        "id": "mS-f4Mkzk4KD"
      },
      "source": [
        "# Train and Save PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7XcQoQhFk78S",
      "metadata": {
        "id": "7XcQoQhFk78S"
      },
      "outputs": [],
      "source": [
        "def train_and_save_pinn(X, N_u, N_f, layers, eta, epochs, lb, ub, weights, model_save_path, potential_type,\n",
        "                        prev_model, optimizer_name):\n",
        "    \"\"\"\n",
        "    Train the Physics-Informed Neural Network (PINN) model and save it.\n",
        "\n",
        "    This function trains a PINN model for the 1D Gross-Pitaevskii equation with a specific interaction\n",
        "    strength (eta) and saves the trained model to a specified path. It also returns the trained model\n",
        "    and the loss history recorded during training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.ndarray\n",
        "        Input data for the training (e.g., spatial domain points).\n",
        "    N_u : int\n",
        "        Number of boundary points.\n",
        "    N_f : int\n",
        "        Number of collocation points (interior points) for physics-based loss.\n",
        "    layers : list of int\n",
        "        Architecture of the neural network, defined as a list of layer sizes.\n",
        "        For example, [1, 100, 100, 100, 1] represents an input layer with 1 neuron,\n",
        "        three hidden layers with 20 neurons each, and an output layer with 1 neuron.\n",
        "    eta : float\n",
        "        Interaction strength parameter for the Gross-Pitaevskii equation.\n",
        "    epochs : int\n",
        "        Number of training epochs.\n",
        "    lb : int\n",
        "        Lower bound of interval.\n",
        "    ub : int\n",
        "        Upper bound of interval.\n",
        "    weights : list\n",
        "        Weights for different loss terms.\n",
        "    model_save_path : str\n",
        "        File name to save the trained model weights (e.g., 'model_eta_1.pth').\n",
        "    potential_type: str\n",
        "        Type of potential function to use\n",
        "    prev_model : GrossPitaevskiiPINN or None\n",
        "        Previously trained model whose predictions are used as part of the training process.\n",
        "        If None, the model starts training from scratch.\n",
        "    optimizer_name : str\n",
        "        Name of optimizer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : GrossPitaevskiiPINN\n",
        "        The trained PINN model.\n",
        "    loss_history : dict of (eta, optimizer) -> list of float\n",
        "        A dictionary where each key is a tuple (eta, optimizer name) and\n",
        "        the corresponding value is a list of loss values recorded during training.\n",
        "    lambda_pde_history : dict of (eta, optimizer) -> list of float\n",
        "        A dictionary where each key is a tuple (eta, optimizer name) and\n",
        "        the corresponding value is a list of λ_PDE values recorded during training.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a new model, passing the previous model's forward function if available\n",
        "    prev_prediction = prev_model.forward if prev_model is not None else None\n",
        "    if prev_model is not None:\n",
        "        prev_model.eval()\n",
        "    model = GrossPitaevskiiPINN(layers, prev_prediction=prev_prediction).to(device)\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "    # Train the model\n",
        "    model, loss_history, lambda_pde_history = train_pinn_with_optimizer(X, N_u=N_u, N_f=N_f, layers=layers, eta=eta,\n",
        "                                      epochs=epochs, lb=lb, ub=ub, weights=weights, model_save_path=model_save_path,\n",
        "                                      potential_type=potential_type, prev_prediction=prev_prediction,\n",
        "                                      optimizer_name=optimizer_name)\n",
        "\n",
        "    # Directory to save the models\n",
        "    model_save_dir = 'models'\n",
        "    os.makedirs(model_save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    # Save model after training\n",
        "    save_model_path = os.path.join(model_save_dir, model_save_path)\n",
        "    torch.save(model.state_dict(), save_model_path)  # Save model weights\n",
        "\n",
        "    return model, loss_history, lambda_pde_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vjY89lOvYF0h",
      "metadata": {
        "id": "vjY89lOvYF0h"
      },
      "source": [
        "# Predict and Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "YZ-IcULzYIDu",
      "metadata": {
        "id": "YZ-IcULzYIDu"
      },
      "outputs": [],
      "source": [
        "def predict_and_plot(models, etas, optimizers, X_test, save_path='plots/predicted_solutions.png',\n",
        "                     potential_type='gaussian', prev_prediction=None):\n",
        "    \"\"\"\n",
        "    Predict and plot the solutions for all models, making separate plots for each optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    models : dict\n",
        "        A dictionary where keys are η values, and values are dictionaries mapping optimizer names to trained models.\n",
        "        Example: models[eta][optimizer_name] -> model\n",
        "    etas : list of float\n",
        "        A list of η values used during training.\n",
        "    optimizers : list of str\n",
        "        A list of optimizer names used during training.\n",
        "    X_test : np.ndarray\n",
        "        Test points along the 1D interval.\n",
        "    save_path : str, optional\n",
        "        The path template to save the plot image (default is 'plots/predicted_solutions_{optimizer}.png').\n",
        "    potential_type : str, optional\n",
        "        Type of potential function to use. Default is 'gaussian'.\n",
        "    prev_prediction : callable, optional\n",
        "        A function representing the previous model's forward pass.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Ensure the plots directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    for optimizer_name in optimizers:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        for eta in etas:\n",
        "            if eta in models and optimizer_name in models[eta]:\n",
        "                model = models[eta][optimizer_name]  # Retrieve model for optimizer and eta\n",
        "                model.eval()\n",
        "\n",
        "                # Prepare test data\n",
        "                X_test_tensor = torch.tensor(X_test, dtype=torch.float32, requires_grad=True).to(device)\n",
        "                u_pred = model(X_test_tensor).detach().cpu().numpy()\n",
        "                u_pred_normalized = normalize_wave_function(u_pred)\n",
        "\n",
        "                # Calculate the potential\n",
        "                potential = model.compute_potential(X_test_tensor, potential_type)\n",
        "\n",
        "                # Calculate λ_PDE and the Thomas-Fermi approximation\n",
        "                _, _, lambda_pde = model.pde_loss(X_test_tensor,\n",
        "                                                  model.forward(X_test_tensor), eta,\n",
        "                                                  potential_type, potential, prev_prediction)\n",
        "                tf_approx = model.compute_thomas_fermi_approx(X_test_tensor, potential, eta).detach().cpu().numpy()\n",
        "                tf_approx_normalized = normalize_wave_function(tf_approx)\n",
        "\n",
        "                # Plot predicted solution for this optimizer\n",
        "                plt.plot(X_test, u_pred_normalized, label=f\"η={eta}\", linestyle=\"-\")\n",
        "\n",
        "        # Plot Weighted Hermite Approximation (η=0)\n",
        "        wh_approx = model.weighted_hermite(X_test_tensor, 0).detach().cpu().numpy()\n",
        "        wh_approx_normalized = normalize_wave_function(wh_approx)\n",
        "        plt.plot(X_test, wh_approx_normalized, linestyle=\"--\", color=\"black\",\n",
        "                 label=\"Weighted Hermite (η=0)\")\n",
        "\n",
        "        plt.title(f\"PINN Solutions - {potential_type.capitalize()} - {optimizer_name}\", fontsize=\"xx-large\")\n",
        "        plt.xlabel(\"$x$\", fontsize=\"xx-large\")\n",
        "        plt.ylabel(\"Normalized $u(x)$\", fontsize=\"xx-large\")\n",
        "        plt.grid(True)\n",
        "        plt.legend(fontsize=\"large\")\n",
        "\n",
        "        # Set tick sizes\n",
        "        plt.xticks(fontsize=\"x-large\")\n",
        "        plt.yticks(fontsize=\"x-large\")\n",
        "\n",
        "        # Save the plot for this optimizer\n",
        "        optimizer_plot_path = save_path.format(optimizer=optimizer_name, potential_type=potential_type)\n",
        "        plt.savefig(optimizer_plot_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kE42RFBuYLPG",
      "metadata": {
        "id": "kE42RFBuYLPG"
      },
      "source": [
        "# Plot Loss History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "WnUKr0ukYOex",
      "metadata": {
        "id": "WnUKr0ukYOex"
      },
      "outputs": [],
      "source": [
        "def plot_loss_history(loss_histories, etas, optimizer_names, save_path='plots/loss_history.png', potential_type='gaussian'):\n",
        "    \"\"\"\n",
        "    Plot the training loss history for all optimizers and interaction strengths (etas) in a single plot.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    loss_histories : dict\n",
        "        Dictionary where keys are eta values, and values are dictionaries mapping optimizer names to loss history lists.\n",
        "    etas : list of float\n",
        "        List of eta values used in training.\n",
        "    optimizer_names : list of str\n",
        "        List of optimizer names used during training.\n",
        "    save_path : str, optional\n",
        "        File path to save the plot, by default 'plots/loss_history.png'.\n",
        "    potential_type : str, optional\n",
        "        Type of potential function to use. Default is 'gaussian'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the plots directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for optimizer_name in optimizer_names:\n",
        "        for eta in etas:\n",
        "            if eta in loss_histories and optimizer_name in loss_histories[eta]:\n",
        "                loss = loss_histories[eta][optimizer_name]\n",
        "                plt.plot(np.arange(len(loss)), loss, marker='o', label=f\"{optimizer_name}, η={eta}\")\n",
        "\n",
        "    plt.xlabel('Training step (x 100)', fontsize=\"xx-large\")\n",
        "    plt.ylabel('Total Loss', fontsize=\"xx-large\")\n",
        "    plt.yscale('log')\n",
        "    plt.title(f'Loss History for Different Interaction Strengths ($\\\\eta$) for {potential_type.capitalize()} Potential',\n",
        "              fontsize=\"xx-large\")\n",
        "    plt.legend(fontsize=\"large\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Set larger tick sizes\n",
        "    plt.xticks(fontsize=\"x-large\")\n",
        "    plt.yticks(fontsize=\"x-large\")\n",
        "\n",
        "    # Save the plot\n",
        "    save_path = save_path.format(potential_type=potential_type)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Lambda PDE"
      ],
      "metadata": {
        "id": "Jexl0921gSNq"
      },
      "id": "Jexl0921gSNq"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_lambda_pde(lambda_pde_histories, etas, optimizer_names, save_path=\"plots/lambda_pde.png\", potential_type=\"harmonic\"):\n",
        "    \"\"\"\n",
        "    Plot the evolution of λ_PDE over training iterations for all optimizers and interaction strengths in one figure.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lambda_pde_histories : dict\n",
        "        Dictionary where keys are eta values, and values are dictionaries mapping optimizer names to lambda_pde history lists.\n",
        "    etas : list of float\n",
        "        List of eta values used in training.\n",
        "    optimizer_names : list of str\n",
        "        List of optimizer names used during training.\n",
        "    save_path : str, optional\n",
        "        File path to save the plot, by default 'plots/lambda_pde.png'.\n",
        "    potential_type : str, optional\n",
        "        Type of potential function to use. Default is 'gaussian'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the plots directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for optimizer_name in optimizer_names:\n",
        "        for eta in etas:\n",
        "            if eta in lambda_pde_histories and optimizer_name in lambda_pde_histories[eta]:\n",
        "                lambda_pde = lambda_pde_histories[eta][optimizer_name]\n",
        "                plt.plot(np.arange(len(lambda_pde)), lambda_pde, marker='o', label=f\"{optimizer_name}, η={eta}\")\n",
        "\n",
        "    plt.xlabel('Training step (x 100)', fontsize=\"xx-large\")\n",
        "    plt.ylabel(r\"$\\lambda_{PDE}$\", fontsize=\"xx-large\")\n",
        "    plt.title(\n",
        "        r\"$\\lambda_{PDE}$ for Different Interaction Strengths ($\\eta$) for \" + f\"{potential_type.capitalize()} Potential\",\n",
        "        fontsize=\"xx-large\")\n",
        "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    plt.legend()\n",
        "\n",
        "    # Set larger tick sizes\n",
        "    plt.xticks(fontsize=\"x-large\")\n",
        "    plt.yticks(fontsize=\"x-large\")\n",
        "\n",
        "    # Save the plot\n",
        "    save_path = save_path.format(potential_type=potential_type)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WD8_M1ajgWQq"
      },
      "id": "WD8_M1ajgWQq",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7RmggB2e-Z7r",
      "metadata": {
        "id": "7RmggB2e-Z7r"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "js3eLQbPlIgm",
      "metadata": {
        "id": "js3eLQbPlIgm"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "N_u = 100  # Number of boundary points\n",
        "N_f = 2000  # Number of collocation points\n",
        "epochs = 5001 # Number of iterations of training\n",
        "layers = [1, 100, 100, 100, 1]  # Neural network architecture\n",
        "lb, ub = -10, 10  # Boundary limits\n",
        "X = np.linspace(lb, ub, N_f).reshape(-1, 1)  # Input grid for training\n",
        "\n",
        "# Test points\n",
        "X_test = np.linspace(lb, ub, N_f).reshape(-1, 1)  # Test points for prediction\n",
        "etas = [1, 10, 50, 100]  # Interaction strengths\n",
        "\n",
        "# Weights for loss terms\n",
        "weights = [5.0, 1.0, 2.0, 5.0, 5.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2AqQL8hWphaH",
      "metadata": {
        "id": "2AqQL8hWphaH"
      },
      "source": [
        "# Loop through and plot all potential types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7oJgKs8Ejr2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oJgKs8Ejr2d",
        "outputId": "eb5c83ed-a54f-4a3c-d495-85afbf342599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Adam optimizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Adam] Epoch [0/5001]: η = 1, λ_PDE = 0.609756,  Loss: 1078.825195, Data Loss = 0.113384, Riesz Loss = 0.041691, PDE Loss = 0.134776, Norm Loss = 215.157837, Sym Loss = 0.490709, Adaptive Weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[Adam] Epoch [1000/5001]: η = 1, λ_PDE = 1.581277,  Loss: 0.008848, Data Loss = 0.000010, Riesz Loss = 0.039818, PDE Loss = 0.073295, Norm Loss = 0.000000, Sym Loss = 0.000001, Adaptive Weights: [0.9380176067352295, 0.945516049861908, 0.9434726238250732, 0.9618750810623169, 0.9499884843826294]\n",
            "[Adam] Epoch [2000/5001]: η = 1, λ_PDE = 1.579426,  Loss: 0.008651, Data Loss = 0.000001, Riesz Loss = 0.039747, PDE Loss = 0.073255, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9370959997177124, 0.9295802116394043, 0.9275801181793213, 0.9618695378303528, 0.9494527578353882]\n",
            "[Adam] Epoch [3000/5001]: η = 1, λ_PDE = 1.576802,  Loss: 0.008529, Data Loss = 0.000001, Riesz Loss = 0.039568, PDE Loss = 0.073074, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9364975690841675, 0.9192200899124146, 0.9172295331954956, 0.9618381857872009, 0.9489608407020569]\n",
            "[Adam] Epoch [4000/5001]: η = 1, λ_PDE = 1.575531,  Loss: 0.008483, Data Loss = 0.000001, Riesz Loss = 0.039478, PDE Loss = 0.072982, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.936103880405426, 0.9156121611595154, 0.9136251211166382, 0.9618381857872009, 0.9487748146057129]\n",
            "[Adam] Epoch [5000/5001]: η = 1, λ_PDE = 1.573481,  Loss: 0.008429, Data Loss = 0.000002, Riesz Loss = 0.039327, PDE Loss = 0.072825, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9354081749916077, 0.9120495319366455, 0.910065770149231, 0.9618381857872009, 0.9485101103782654]\n",
            "[Adam] Epoch [0/5001]: η = 10, λ_PDE = 1.115388,  Loss: 2209.909912, Data Loss = 0.478766, Riesz Loss = 0.206250, PDE Loss = 0.272550, Norm Loss = 440.529724, Sym Loss = 0.965984, Adaptive Weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[Adam] Epoch [1000/5001]: η = 10, λ_PDE = 0.743681,  Loss: 0.000023, Data Loss = 0.000004, Riesz Loss = 0.000001, PDE Loss = 0.000002, Norm Loss = 0.000000, Sym Loss = 0.000001, Adaptive Weights: [0.9282733201980591, 0.971186101436615, 0.9734651446342468, 0.957734227180481, 0.9374999403953552]\n",
            "[Adam] Epoch [2000/5001]: η = 10, λ_PDE = 0.637699,  Loss: 0.000001, Data Loss = 0.000000, Riesz Loss = 0.000002, PDE Loss = 0.000003, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9281256794929504, 0.9628511667251587, 0.9617840051651001, 0.9577324390411377, 0.9373120069503784]\n",
            "[Adam] Epoch [3000/5001]: η = 10, λ_PDE = 0.638424,  Loss: 0.000001, Data Loss = 0.000000, Riesz Loss = 0.000002, PDE Loss = 0.000003, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9281256794929504, 0.9571296572685242, 0.9550237059593201, 0.957732081413269, 0.9372662305831909]\n",
            "[Adam] Epoch [4000/5001]: η = 10, λ_PDE = 0.639244,  Loss: 0.000000, Data Loss = 0.000000, Riesz Loss = 0.000002, PDE Loss = 0.000003, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9281256794929504, 0.9554915428161621, 0.9532175064086914, 0.957732081413269, 0.9372569918632507]\n",
            "[Adam] Epoch [5000/5001]: η = 10, λ_PDE = 0.640171,  Loss: 0.000000, Data Loss = 0.000000, Riesz Loss = 0.000002, PDE Loss = 0.000003, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9281256794929504, 0.9538329839706421, 0.9514283537864685, 0.957732081413269, 0.9372429847717285]\n",
            "[Adam] Epoch [0/5001]: η = 50, λ_PDE = 37.330902,  Loss: 0.009193, Data Loss = 0.000165, Riesz Loss = 0.000236, PDE Loss = 0.000173, Norm Loss = 0.000032, Sym Loss = 0.001636, Adaptive Weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[Adam] Epoch [1000/5001]: η = 50, λ_PDE = 0.359680,  Loss: 0.001836, Data Loss = 0.000363, Riesz Loss = 0.000042, PDE Loss = 0.000056, Norm Loss = 0.000000, Sym Loss = 0.000013, Adaptive Weights: [0.9701011180877686, 0.9807173013687134, 0.9864410758018494, 0.97456955909729, 0.9836593866348267]\n",
            "[Adam] Epoch [2000/5001]: η = 50, λ_PDE = 0.402003,  Loss: 0.000096, Data Loss = 0.000019, Riesz Loss = 0.000006, PDE Loss = 0.000009, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9687979817390442, 0.9797722101211548, 0.9852238893508911, 0.9745693802833557, 0.9836055040359497]\n",
            "[Adam] Epoch [3000/5001]: η = 50, λ_PDE = 0.650096,  Loss: 0.000001, Data Loss = 0.000000, Riesz Loss = 0.000003, PDE Loss = 0.000004, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9687436819076538, 0.979432225227356, 0.9847911596298218, 0.9745692014694214, 0.9835989475250244]\n",
            "[Adam] Epoch [4000/5001]: η = 50, λ_PDE = 0.667826,  Loss: 0.000001, Data Loss = 0.000000, Riesz Loss = 0.000003, PDE Loss = 0.000003, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9687436819076538, 0.979174792766571, 0.9844947457313538, 0.9745691418647766, 0.9835925698280334]\n",
            "[Adam] Epoch [5000/5001]: η = 50, λ_PDE = 0.668763,  Loss: 0.000001, Data Loss = 0.000000, Riesz Loss = 0.000003, PDE Loss = 0.000003, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9687436819076538, 0.9788798093795776, 0.9841580986976624, 0.9745690822601318, 0.98358553647995]\n",
            "[Adam] Epoch [0/5001]: η = 100, λ_PDE = 12.390255,  Loss: 2238.824463, Data Loss = 0.434391, Riesz Loss = 2.051095, PDE Loss = 23.575769, Norm Loss = 445.860535, Sym Loss = 0.977934, Adaptive Weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[Adam] Epoch [1000/5001]: η = 100, λ_PDE = 0.814522,  Loss: 0.000338, Data Loss = 0.000038, Riesz Loss = 0.000569, PDE Loss = 0.000811, Norm Loss = 0.000000, Sym Loss = 0.000011, Adaptive Weights: [0.9369750022888184, 0.964763343334198, 0.9796847701072693, 0.9734120965003967, 0.9406503438949585]\n",
            "[Adam] Epoch [2000/5001]: η = 100, λ_PDE = 0.895823,  Loss: 0.000124, Data Loss = 0.000000, Riesz Loss = 0.000620, PDE Loss = 0.000858, Norm Loss = 0.000000, Sym Loss = 0.000002, Adaptive Weights: [0.936582624912262, 0.9560877680778503, 0.9709663987159729, 0.973410964012146, 0.9377089738845825]\n",
            "[Adam] Epoch [3000/5001]: η = 100, λ_PDE = 0.889234,  Loss: 0.000112, Data Loss = 0.000000, Riesz Loss = 0.000616, PDE Loss = 0.000846, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9365659952163696, 0.9483615159988403, 0.9631931185722351, 0.9734041094779968, 0.9369838237762451]\n",
            "[Adam] Epoch [4000/5001]: η = 100, λ_PDE = 0.884513,  Loss: 0.000110, Data Loss = 0.000000, Riesz Loss = 0.000613, PDE Loss = 0.000838, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9365624189376831, 0.9446431994438171, 0.9594406485557556, 0.9734039306640625, 0.9368770122528076]\n",
            "[Adam] Epoch [5000/5001]: η = 100, λ_PDE = 0.879603,  Loss: 0.000108, Data Loss = 0.000000, Riesz Loss = 0.000610, PDE Loss = 0.000829, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [0.9365624189376831, 0.9420298933982849, 0.9568017721176147, 0.9734036922454834, 0.9368163347244263]\n",
            "Training with Shampoo optimizer...\n",
            "[Shampoo] Epoch [0/5001]: η = 1, λ_PDE = 0.733466,  Loss: 369.448853, Data Loss = 0.015299, Riesz Loss = 0.041128, PDE Loss = 0.118898, Norm Loss = 73.691254, Sym Loss = 0.180431, Adaptive Weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[Shampoo] Epoch [1000/5001]: η = 1, λ_PDE = 1.646588,  Loss: 0.006934, Data Loss = 0.000086, Riesz Loss = 0.031020, PDE Loss = 0.053567, Norm Loss = 0.000000, Sym Loss = 0.000002, Adaptive Weights: [0.9721041917800903, 0.9586389064788818, 0.9368981122970581, 0.9572082757949829, 0.941699206829071]\n",
            "[Shampoo] Epoch [2000/5001]: η = 1, λ_PDE = 1.644883,  Loss: 0.006261, Data Loss = 0.000004, Riesz Loss = 0.030124, PDE Loss = 0.052069, Norm Loss = 0.000000, Sym Loss = 0.000001, Adaptive Weights: [0.9897797703742981, 0.9491727352142334, 0.9231420755386353, 0.9571933150291443, 0.9425531625747681]\n",
            "[Shampoo] Epoch [3000/5001]: η = 1, λ_PDE = 1.639129,  Loss: 0.006071, Data Loss = 0.000000, Riesz Loss = 0.029632, PDE Loss = 0.051484, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [1.0013329982757568, 0.9396021366119385, 0.9084880352020264, 0.957186222076416, 0.9427333474159241]\n",
            "[Shampoo] Epoch [4000/5001]: η = 1, λ_PDE = 1.632946,  Loss: 0.005929, Data Loss = 0.000000, Riesz Loss = 0.029290, PDE Loss = 0.051139, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [1.0080641508102417, 0.928713858127594, 0.8930522203445435, 0.9571827054023743, 0.9429357647895813]\n",
            "[Shampoo] Epoch [5000/5001]: η = 1, λ_PDE = 1.624750,  Loss: 0.005785, Data Loss = 0.000000, Riesz Loss = 0.028930, PDE Loss = 0.050785, Norm Loss = 0.000000, Sym Loss = 0.000000, Adaptive Weights: [1.0120536088943481, 0.9176642298698425, 0.8774709701538086, 0.9571819305419922, 0.94312584400177]\n",
            "[Shampoo] Epoch [0/5001]: η = 10, λ_PDE = 5.639691,  Loss: 208.469879, Data Loss = 0.046887, Riesz Loss = 0.003306, PDE Loss = 0.000603, Norm Loss = 41.537567, Sym Loss = 0.109477, Adaptive Weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[Shampoo] Epoch [1000/5001]: η = 10, λ_PDE = 0.506198,  Loss: 0.015100, Data Loss = 0.001249, Riesz Loss = 0.000208, PDE Loss = 0.000238, Norm Loss = 0.000007, Sym Loss = 0.001930, Adaptive Weights: [0.9416056275367737, 0.9986348152160645, 0.9983668327331543, 0.9543524384498596, 0.947969377040863]\n",
            "[Shampoo] Epoch [2000/5001]: η = 10, λ_PDE = 0.506830,  Loss: 0.012887, Data Loss = 0.000780, Riesz Loss = 0.000208, PDE Loss = 0.000232, Norm Loss = 0.000000, Sym Loss = 0.001937, Adaptive Weights: [0.9416139125823975, 0.9986364245414734, 0.9983672499656677, 0.9543524384498596, 0.9479382038116455]\n",
            "[Shampoo] Epoch [3000/5001]: η = 10, λ_PDE = 0.498739,  Loss: 0.011794, Data Loss = 0.000552, Riesz Loss = 0.000204, PDE Loss = 0.000228, Norm Loss = 0.000001, Sym Loss = 0.001933, Adaptive Weights: [0.9416220784187317, 0.9986385703086853, 0.998367428779602, 0.9543524384498596, 0.947903573513031]\n",
            "[Shampoo] Epoch [4000/5001]: η = 10, λ_PDE = 0.491179,  Loss: 0.011164, Data Loss = 0.000425, Riesz Loss = 0.000199, PDE Loss = 0.000224, Norm Loss = 0.000001, Sym Loss = 0.001925, Adaptive Weights: [0.9416325688362122, 0.9986409544944763, 0.9983681440353394, 0.9543524384498596, 0.947874903678894]\n",
            "[Shampoo] Epoch [5000/5001]: η = 10, λ_PDE = 0.484950,  Loss: 0.010737, Data Loss = 0.000344, Riesz Loss = 0.000195, PDE Loss = 0.000221, Norm Loss = 0.000001, Sym Loss = 0.001916, Adaptive Weights: [0.9416430592536926, 0.9986429214477539, 0.9983690977096558, 0.9543524384498596, 0.9478501677513123]\n",
            "[Shampoo] Epoch [0/5001]: η = 50, λ_PDE = 13.945677,  Loss: 354.493591, Data Loss = 0.080793, Riesz Loss = 0.039296, PDE Loss = 0.042929, Norm Loss = 70.640602, Sym Loss = 0.176074, Adaptive Weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[Shampoo] Epoch [1000/5001]: η = 50, λ_PDE = 0.452315,  Loss: 0.000435, Data Loss = 0.000029, Riesz Loss = 0.000151, PDE Loss = 0.000143, Norm Loss = 0.000000, Sym Loss = 0.000062, Adaptive Weights: [0.9248684644699097, 0.9503110647201538, 0.9176278710365295, 0.946135401725769, 0.9125878214836121]\n",
            "[Shampoo] Epoch [2000/5001]: η = 50, λ_PDE = 0.448996,  Loss: 0.000337, Data Loss = 0.000019, Riesz Loss = 0.000150, PDE Loss = 0.000129, Norm Loss = 0.000000, Sym Loss = 0.000054, Adaptive Weights: [0.9111883044242859, 0.8970719575881958, 0.8602046370506287, 0.9462810158729553, 0.8750931620597839]\n",
            "[Shampoo] Epoch [3000/5001]: η = 50, λ_PDE = 0.444070,  Loss: 0.000320, Data Loss = 0.000017, Riesz Loss = 0.000148, PDE Loss = 0.000124, Norm Loss = 0.000000, Sym Loss = 0.000053, Adaptive Weights: [0.9066253304481506, 0.882283627986908, 0.8458479642868042, 0.9461918473243713, 0.8556215167045593]\n"
          ]
        }
      ],
      "source": [
        "potential_types = ['gaussian', 'harmonic', 'periodic']\n",
        "# potential_types = ['harmonic']\n",
        "\n",
        "# Optimizers to compare\n",
        "optimizer_names = [\"Adam\", \"Shampoo\", \"Shampoo-Eigen\"]\n",
        "\n",
        "# Store results for plotting\n",
        "all_loss_histories = {}\n",
        "all_lambda_pde_histories = {}\n",
        "models = {eta: {} for eta in etas}  # Store models for all η and optimizers\n",
        "\n",
        "# Loop through each potential type\n",
        "for potential_type in potential_types:\n",
        "\n",
        "    for optimizer_name in optimizer_names:\n",
        "        print(f\"Training with {optimizer_name} optimizer...\")\n",
        "\n",
        "        # Store results for each optimizer separately\n",
        "        prev_model = None\n",
        "        loss_histories = {}\n",
        "        lambda_pde_histories = {}\n",
        "\n",
        "        for eta in etas:\n",
        "            model_save_path = f\"trained_model_eta_{eta}_{optimizer_name}.pth\"\n",
        "            model, loss_history, lambda_pde_history = train_and_save_pinn(X, N_u=N_u, N_f=N_f, layers=layers, eta=eta,\n",
        "                                                                          epochs=epochs, lb=lb, ub=ub,\n",
        "                                                                          weights=weights, model_save_path=model_save_path,\n",
        "                                                                          potential_type=potential_type, prev_model=prev_model,\n",
        "                                                                          optimizer_name=optimizer_name)\n",
        "            prev_model = model  # Store the trained model for the next iteration\n",
        "\n",
        "            # Store models by eta and optimizer\n",
        "            models[eta][optimizer_name] = model\n",
        "\n",
        "            # Store loss and λ_PDE history under each eta for this optimizer\n",
        "            if potential_type not in all_loss_histories:\n",
        "                all_loss_histories[potential_type] = {}\n",
        "            if potential_type not in all_lambda_pde_histories:\n",
        "                all_lambda_pde_histories[potential_type] = {}\n",
        "\n",
        "            all_loss_histories[potential_type].setdefault(eta, {})[optimizer_name] = loss_history\n",
        "            all_lambda_pde_histories[potential_type].setdefault(eta, {})[optimizer_name] = lambda_pde_history\n",
        "\n",
        "    # Predict and plot the solutions for each optimizer separately\n",
        "    predict_and_plot(models, etas, optimizer_names, X_test, save_path='plots/predicted_solutions_{potential_type}.png',\n",
        "                     potential_type=potential_type, prev_prediction=prev_model)\n",
        "\n",
        "    # Plot the loss history for all etas\n",
        "    plot_loss_history(all_loss_histories[potential_type], etas, optimizer_names,\n",
        "                      save_path='plots/loss_history_{potential_type}.png', potential_type=potential_type)\n",
        "\n",
        "    # Plot lambda_pde history for all etas\n",
        "    plot_lambda_pde(all_lambda_pde_histories[potential_type], etas, optimizer_names,\n",
        "                    save_path=f'plots/lambda_pde_{potential_type}.png', potential_type=potential_type)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}