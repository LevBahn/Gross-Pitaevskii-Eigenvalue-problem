{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "efbqfOtOZasn",
      "metadata": {
        "id": "efbqfOtOZasn"
      },
      "source": [
        "This notebook implements concepts from the [Multi-Objective Loss Balancing for Physics-Informed Deep Learning paper](https://arxiv.org/abs/2110.09813). It highlights the gains in performance when applying Loss Balancing Schemes to PINN training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3nYdXyrr-Z7h",
      "metadata": {
        "id": "3nYdXyrr-Z7h"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BMnx60Sj-Z7j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnx60Sj-Z7j",
        "outputId": "9cb8b636-6a27-492d-c64a-6166bdd5b1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import grad\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hglIb1ul-Z7n",
      "metadata": {
        "id": "hglIb1ul-Z7n"
      },
      "source": [
        "# Physics Informed Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NFVJh4jB-Z7o",
      "metadata": {
        "id": "NFVJh4jB-Z7o"
      },
      "outputs": [],
      "source": [
        "class GrossPitaevskiiPINN(nn.Module):\n",
        "    \"\"\"\n",
        "    Physics-Informed Neural Network (PINN) for solving the 2D Gross-Pitaevskii Equation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, hbar=1.0, m=1.0, g=100.0, alpha=0.999, temperature=1., rho=0.9999):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        layers : list of int\n",
        "            Neural network architecture, each entry defines the number of neurons in that layer.\n",
        "        hbar : float, optional\n",
        "            Reduced Planck's constant (default is 1.0).\n",
        "        m : float, optional\n",
        "            Mass of the particle (default is 1.0).\n",
        "        g : float, optional\n",
        "            Interaction strength (default is 100.0).\n",
        "        alpha, optional : float\n",
        "                Controls the exponential weight decay rate.\n",
        "                Value between 0 and 1. The smaller, the more stochasticity.\n",
        "                0 means no historical information is transmitted to the next iteration.\n",
        "                1 means only first calculation is retained. Defaults to 0.999.\n",
        "        temperature, optional : float\n",
        "                Softmax temperature coefficient. Controlls the \"sharpness\" of the softmax operation.\n",
        "                Defaults to 1.\n",
        "        rho, optional : float\n",
        "                Probability of the Bernoulli random variable controlling the frequency of random lookbacks.\n",
        "                Value berween 0 and 1. The smaller, the fewer lookbacks happen.\n",
        "                0 means lambdas are always calculated w.r.t. the initial loss values.\n",
        "                1 means lambdas are always calculated w.r.t. the loss values in the previous training iteration.\n",
        "                Defaults to 0.9999.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.network = self.build_network()\n",
        "        self.g = g  # Interaction strength\n",
        "        self.hbar = hbar  # Planck's constant, fixed\n",
        "        self.m = m  # Particle mass, fixed\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.rho = rho\n",
        "        self.call_count = 0  # Track the number of forward calls\n",
        "\n",
        "        # Initialize dynamic weights and loss history\n",
        "        self.lambdas = [1.0] * 5  # For boundary, PDE, riesz, symmetry, and normalization losses\n",
        "        self.last_losses = [1.0] * 5\n",
        "        self.init_losses = [1.0] * 5\n",
        "\n",
        "    def build_network(self):\n",
        "        \"\"\"\n",
        "        Build the neural network with sine activation functions between layers.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        nn.Sequential\n",
        "            A PyTorch sequential model representing the neural network architecture.\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            layers.append(nn.Linear(self.layers[i], self.layers[i + 1]))\n",
        "            if i < len(self.layers) - 2:\n",
        "                layers.append(nn.Tanh())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the neural network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor containing spatial points (collocation points).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor representing the predicted solution.\n",
        "        \"\"\"\n",
        "        return self.network(inputs)\n",
        "\n",
        "    def compute_potential(self, x, potential_type=\"gaussian\", a=0.5, l=1.0):\n",
        "        \"\"\"\n",
        "        Compute a symmetric potential function for the 1D domain.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of spatial coordinates.\n",
        "        potential_type : str, optional\n",
        "            Type of potential ('gaussian' or 'sine'), by default \"gaussian\".\n",
        "        a : float, optional\n",
        "            Center of the potential (for Gaussian), by default 0.5.\n",
        "        l : float, optional\n",
        "            Length scale for sine potential, by default 1.0.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        V : torch.Tensor\n",
        "            Tensor of symmetric potential values at the input points.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the potential type is not recognized.\n",
        "        \"\"\"\n",
        "        # Gaussian potential centered at `a`\n",
        "        if potential_type == \"gaussian\":\n",
        "            V = torch.exp(-(x - a) ** 2)\n",
        "        # Sine potential symmetric about the center of the domain\n",
        "        else:\n",
        "            V = torch.sin(torch.pi * (x - (a - l / 2)) / l)\n",
        "\n",
        "        return V\n",
        "\n",
        "    def boundary_loss(self, boundary_points, boundary_values):\n",
        "        \"\"\"\n",
        "        Compute the boundary loss (MSE) for the boundary conditions.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        boundary_points : torch.Tensor\n",
        "            Input tensor of boundary spatial points.\n",
        "        boundary_values : torch.Tensor\n",
        "            Tensor of boundary values (for Dirichlet conditions).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Mean squared error (MSE) at the boundary points.\n",
        "        \"\"\"\n",
        "        u_pred = self.forward(boundary_points)\n",
        "        return torch.mean((u_pred - boundary_values) ** 2)\n",
        "\n",
        "    def riesz_loss(self, predictions, inputs, eta):\n",
        "        \"\"\"\n",
        "        Compute the Riesz energy loss for the Gross-Pitaevskii equation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : torch.Tensor\n",
        "            Predicted solution from the network.\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor of spatial coordinates (collocation points).\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Riesz energy loss value.\n",
        "        \"\"\"\n",
        "        u = predictions\n",
        "\n",
        "        if not inputs.requires_grad:\n",
        "            inputs = inputs.clone().detach().requires_grad_(True)\n",
        "        u_x = torch.autograd.grad(outputs=predictions, inputs=inputs,\n",
        "                                  grad_outputs=torch.ones_like(predictions),\n",
        "                                  create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        laplacian_term = torch.mean(u_x ** 2)  # Kinetic term\n",
        "        V = self.compute_potential(inputs)\n",
        "        potential_term = torch.mean(V * u ** 2)  # Potential term\n",
        "        interaction_term = 0.5 * eta * torch.mean(u ** 4)  # Interaction term\n",
        "\n",
        "        riesz_energy = 0.5 * (laplacian_term + potential_term + interaction_term)\n",
        "\n",
        "        return riesz_energy\n",
        "\n",
        "    def pde_loss(self, inputs, predictions, eta):\n",
        "        \"\"\"\n",
        "        Compute the PDE loss for the Gross-Pitaevskii equation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor of spatial coordinates (collocation points).\n",
        "        predictions : torch.Tensor\n",
        "            Predicted solution from the network.\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            Tuple containing:\n",
        "                - torch.Tensor: PDE loss value.\n",
        "                - torch.Tensor: PDE residual.\n",
        "                - torch.Tensor: Smallest eigenvalue (lambda).\n",
        "        \"\"\"\n",
        "        u = predictions\n",
        "\n",
        "        # Compute first and second derivatives with respect to x\n",
        "        u_x = grad(u, inputs, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        u_xx = grad(u_x, inputs, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
        "\n",
        "        # Compute λ from the energy functional\n",
        "        V = self.compute_potential(inputs)\n",
        "        lambda_pde = torch.mean(u_x ** 2 + V * u ** 2 + eta * u ** 4) / torch.mean(u ** 2)\n",
        "\n",
        "        # Residual of the 1D Gross-Pitaevskii equation\n",
        "        pde_residual = -u_xx + V * u + eta * torch.abs(u ** 2) * u - lambda_pde * u\n",
        "\n",
        "        # Regularization: See https://arxiv.org/abs/2010.05075\n",
        "\n",
        "        # Term 1: L_f = 1 / (f(x, λ))^2, penalizes the network if the PDE residual is close to zero to avoid trivial eigenfunctions\n",
        "        L_f = 1 / (torch.mean(u ** 2) + 1e-2)\n",
        "\n",
        "        # Term 2: L_λ = 1 / λ^2, penalizes small eigenvalues λ, ensuring non-trivial eigenvalues\n",
        "        L_lambda = 1 / (lambda_pde ** 2 + 1e-6)\n",
        "\n",
        "        # Term 3: L_drive = e^(-λ + c), encourages λ to grow, preventing collapse to small values\n",
        "        L_drive = torch.exp(-lambda_pde + 1.0)\n",
        "\n",
        "        # PDE loss (residual plus regularization terms)\n",
        "        pde_loss = torch.mean(pde_residual ** 2)  # + L_lambda + L_f\n",
        "\n",
        "        return pde_loss, pde_residual, lambda_pde\n",
        "\n",
        "    def symmetry_loss(self, collocation_points):\n",
        "        \"\"\"\n",
        "        Compute the symmetry loss to enforce u(x, y) = u(x, -y).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        collocation_points : torch.Tensor\n",
        "            Tensor of interior spatial points with shape (N, 2).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sym_loss : torch.Tensor\n",
        "            The mean squared error enforcing symmetry u(x, y) = u(x, -y).\n",
        "        \"\"\"\n",
        "        # Reflect points across the x-axis\n",
        "        reflected_points = collocation_points.clone()\n",
        "        reflected_points[:, 1] = -reflected_points[:, 1]  # Negate the y-coordinate\n",
        "\n",
        "        # Predict u(x, y) and u(x, -y) using the model\n",
        "        u_original = self.forward(collocation_points)\n",
        "        u_reflected = self.forward(reflected_points)\n",
        "\n",
        "        # Compute mean squared difference to enforce symmetry\n",
        "        sym_loss = torch.mean((u_original - u_reflected) ** 2)\n",
        "\n",
        "        return sym_loss\n",
        "\n",
        "    def total_loss(self, collocation_points, boundary_points, boundary_values, eta):\n",
        "        \"\"\"\n",
        "        Compute the total loss combining boundary loss, Riesz energy loss,\n",
        "        PDE loss, L^2 norm regularization loss, and symmetry loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        collocation_points : torch.Tensor\n",
        "            Input tensor of spatial coordinates for the interior points.\n",
        "        boundary_points : torch.Tensor\n",
        "            Input tensor of boundary spatial points.\n",
        "        boundary_values : torch.Tensor\n",
        "            Tensor of boundary values (for Dirichlet conditions).\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        total_loss : torch.Tensor\n",
        "            Total loss value.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute individual loss components\n",
        "        data_loss = self.boundary_loss(boundary_points, boundary_values)\n",
        "        riesz_energy = self.riesz_loss(self.forward(collocation_points), collocation_points, eta)\n",
        "        pde_loss, _, _ = self.pde_loss(collocation_points, self.forward(collocation_points), eta)\n",
        "        norm_loss = (torch.norm(self.forward(collocation_points), p=2) - 1) ** 2\n",
        "\n",
        "        # Symmetry loss for collocation and boundary points\n",
        "        #sym_loss_collocation = self.symmetry_loss(collocation_points)\n",
        "        #sym_loss_boundary = self.symmetry_loss(boundary_points)\n",
        "        #sym_loss = (sym_loss_collocation + sym_loss_boundary) / 2\n",
        "        sym_loss = self.symmetry_loss(collocation_points)\n",
        "\n",
        "\n",
        "        # Collect losses in a list\n",
        "        losses = [data_loss, riesz_energy, pde_loss, norm_loss, sym_loss]\n",
        "\n",
        "        # Define manual weights for each term (adjust as needed)\n",
        "        manual_weights = [500.0, 1.0, 2.0, 100.0, 500]\n",
        "\n",
        "        # Initialize lambdas and histories\n",
        "        if self.call_count == 0:\n",
        "            num_terms = len(losses)\n",
        "            self.lambdas = [1.0] * num_terms\n",
        "            self.last_losses = [loss.item() for loss in losses]\n",
        "            self.init_losses = [loss.item() for loss in losses]\n",
        "\n",
        "        # Compute lambdas_hat (relative to the last losses)\n",
        "        lambdas_hat = [\n",
        "            losses[i].item() / (self.last_losses[i] * self.temperature + 1e-8) for i in range(len(losses))\n",
        "        ]\n",
        "        lambdas_hat = torch.softmax(torch.tensor(lambdas_hat) - max(lambdas_hat), dim=0).tolist()\n",
        "\n",
        "        # Compute init_lambdas_hat (relative to the initial losses)\n",
        "        init_lambdas_hat = [\n",
        "            losses[i].item() / (self.init_losses[i] * self.temperature + 1e-8) for i in range(len(losses))\n",
        "        ]\n",
        "        init_lambdas_hat = torch.softmax(torch.tensor(init_lambdas_hat) - max(init_lambdas_hat), dim=0).tolist()\n",
        "\n",
        "        # Random lookbacks controlled by rho\n",
        "        rho = torch.bernoulli(torch.tensor(self.rho))\n",
        "        alpha = self.alpha if self.call_count > 1 else (0.0 if self.call_count == 1 else 1.0)\n",
        "\n",
        "        # Update lambdas\n",
        "        self.lambdas = [\n",
        "            float(rho * alpha * self.lambdas[i] +\n",
        "                  (1 - rho) * alpha * init_lambdas_hat[i] +\n",
        "                  (1 - alpha) * lambdas_hat[i])\n",
        "            for i in range(len(losses))\n",
        "        ]\n",
        "\n",
        "        # Update loss history\n",
        "        self.last_losses = [loss.item() for loss in losses]\n",
        "\n",
        "        # Apply manual weights to the dynamically computed lambdas\n",
        "        weighted_lambdas = [lambda_i * weight for lambda_i, weight in zip(self.lambdas, manual_weights)]\n",
        "\n",
        "        # Compute total loss\n",
        "        total_loss = sum(lambda_i * loss for lambda_i, loss in zip(weighted_lambdas, losses))\n",
        "        self.call_count += 1\n",
        "\n",
        "        return total_loss, data_loss, riesz_energy, pde_loss, norm_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzMOyXAj7jey",
      "metadata": {
        "id": "FzMOyXAj7jey"
      },
      "source": [
        "# Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GyMrRGdE7lB2",
      "metadata": {
        "id": "GyMrRGdE7lB2"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    \"\"\"\n",
        "    Initialize the weights of the neural network layers using Xavier uniform initialization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : torch.nn.Module\n",
        "        A layer of the neural network. If it is a linear layer, its weights and biases are initialized.\n",
        "    \"\"\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AKKgTNHo24k4",
      "metadata": {
        "id": "AKKgTNHo24k4"
      },
      "source": [
        "# Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1kAWdOgL26tv",
      "metadata": {
        "id": "1kAWdOgL26tv"
      },
      "outputs": [],
      "source": [
        "def prepare_training_data(N_u, N_f, center=(np.pi / 2, np.pi / 2), radius=np.pi / 2):\n",
        "    \"\"\"\n",
        "    Generate training data including boundary points and interior collocation points.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_u : int\n",
        "        Number of boundary points.\n",
        "    N_f : int\n",
        "        Number of collocation points (interior points).\n",
        "    center : tuple of float, optional\n",
        "        Center of the circular region (default is (pi/2, pi/2)).\n",
        "    radius : float, optional\n",
        "        Radius of the circular region (default is pi/2).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple of np.ndarray\n",
        "        Tuple containing:\n",
        "            - X_f_train: Interior points (collocation points).\n",
        "            - X_u_train: Boundary points.\n",
        "            - u_train: Boundary conditions (Dirichlet).\n",
        "    \"\"\"\n",
        "    # Generate boundary points along the domain of the potential\n",
        "    theta = np.linspace(0, 2 * np.pi, N_u)\n",
        "    circle_x = center[0] + radius * np.cos(theta)\n",
        "    circle_y = center[1] + radius * np.sin(theta)\n",
        "    X_u_train = np.column_stack((circle_x, circle_y))\n",
        "    u_train = np.zeros((X_u_train.shape[0], 1))  # Boundary condition u=0\n",
        "\n",
        "    # Generate collocation points within the domain of the potential\n",
        "    collocation_points_x = np.random.uniform(center[0] - radius, center[0] + radius, N_f)\n",
        "    collocation_points_y = np.random.uniform(center[1] - radius, center[1] + radius, N_f)\n",
        "    interior_mask = (collocation_points_x - center[0]) ** 2 + (collocation_points_y - center[1]) ** 2 <= radius ** 2\n",
        "    X_f_train = np.column_stack((collocation_points_x[interior_mask], collocation_points_y[interior_mask]))\n",
        "\n",
        "    return X_f_train, X_u_train, u_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B0dvfxBOmPlR",
      "metadata": {
        "id": "B0dvfxBOmPlR"
      },
      "source": [
        "# Train PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FXsGZVhYmS1t",
      "metadata": {
        "id": "FXsGZVhYmS1t"
      },
      "outputs": [],
      "source": [
        "def train_pinn(N_u, N_f, layers, eta, epochs, model_save_path):\n",
        "    \"\"\"\n",
        "    Train the Physics-Informed Neural Network (PINN) for the 1D Gross-Pitaevskii equation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_u : int\n",
        "        Number of boundary points\n",
        "    N_f : int\n",
        "        Number of collocation points (interior points) for the physics-based loss\n",
        "    layers : list of int\n",
        "        Architecture of the neural network\n",
        "    eta : float\n",
        "        Interaction strength\n",
        "    epochs: int\n",
        "        Number of epochs\n",
        "    model_save_path : str\n",
        "        Save path for trained model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : GrossPitaevskiiPINN\n",
        "        The trained model\n",
        "    loss_history : list\n",
        "        List of loss values recorded during training\n",
        "    \"\"\"\n",
        "    # Instantiate the PINN model and initialize its weights\n",
        "    model = GrossPitaevskiiPINN(layers).to(device)\n",
        "    model.apply(initialize_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=25, factor=0.5, verbose=True)\n",
        "\n",
        "    # Prepare training data (collocation and boundary points)\n",
        "    collocation_points, boundary_points, boundary_values = prepare_training_data(N_u, N_f)\n",
        "\n",
        "    # Convert data to PyTorch tensors and move to device\n",
        "    collocation_points_tensor = torch.tensor(collocation_points, dtype=torch.float32, requires_grad=True).to(device)\n",
        "    boundary_points_tensor = torch.tensor(boundary_points, dtype=torch.float32).to(device)\n",
        "    boundary_values_tensor = torch.tensor(boundary_values, dtype=torch.float32).to(device)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate the total loss (boundary, Riesz energy, PDE, normalization, and symmetry losses)\n",
        "        loss, data_loss, riesz_energy, pde_loss, norm_loss = model.total_loss(collocation_points_tensor, boundary_points_tensor, boundary_values_tensor, eta)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Scheduler step (for ReduceLROnPlateau)\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        # Record the total loss every 100 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "        # Record the pde loss and lambda every 10000 epochs\n",
        "        if epoch % 10000 == 0:\n",
        "            print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.6f}')\n",
        "            pde_loss, _, lambda_pde = model.pde_loss(collocation_points_tensor, model.forward(collocation_points_tensor), eta)\n",
        "\n",
        "    return model, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8P5eHv-QCQiE",
      "metadata": {
        "id": "8P5eHv-QCQiE"
      },
      "source": [
        "# Normalize Wave Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wUq16rY6CUd9",
      "metadata": {
        "id": "wUq16rY6CUd9"
      },
      "outputs": [],
      "source": [
        "def normalize_wave_function(u):\n",
        "    \"\"\"\n",
        "    Normalize the wave function with respect to its maximum value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    u : torch.Tensor\n",
        "        The predicted wave function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The normalized wave function.\n",
        "    \"\"\"\n",
        "    return np.abs(u) / np.max(np.abs(u))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t7r4H0TVkyZb",
      "metadata": {
        "id": "t7r4H0TVkyZb"
      },
      "source": [
        "# Plot Potential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__9usRWzk15-",
      "metadata": {
        "id": "__9usRWzk15-"
      },
      "outputs": [],
      "source": [
        "def plot_potential_1D(X_test, potential):\n",
        "    \"\"\"\n",
        "    Plot the 1D potential function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_test : np.ndarray\n",
        "        The test points where the potential is computed.\n",
        "    potential : np.ndarray\n",
        "        The computed potential values at the test points.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6, 5))\n",
        "\n",
        "    # X_test is the x-values (positions) of the 1D potential\n",
        "    plt.plot(X_test, potential, label='Potential $V(x)$', color='green')\n",
        "\n",
        "    plt.title('Potential $V(x)$ in 1D')\n",
        "    plt.xlabel('$x$')\n",
        "    plt.ylabel('$V(x)$')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mS-f4Mkzk4KD",
      "metadata": {
        "id": "mS-f4Mkzk4KD"
      },
      "source": [
        "# Train and Save PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7XcQoQhFk78S",
      "metadata": {
        "id": "7XcQoQhFk78S"
      },
      "outputs": [],
      "source": [
        "def train_and_save_pinn(N_u, N_f, layers, eta, epochs, model_save_path):\n",
        "    \"\"\"\n",
        "    Train the Physics-Informed Neural Network (PINN) model and save it.\n",
        "\n",
        "    This function trains a PINN model for the 2D Gross-Pitaevskii equation with a specific interaction\n",
        "    strength (eta) and saves the trained model to a specified path. It also returns the trained model\n",
        "    and the loss history recorded during training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_u : int\n",
        "        Number of boundary points.\n",
        "    N_f : int\n",
        "        Number of collocation points (interior points) for physics-based loss.\n",
        "    layers : list of int\n",
        "        Architecture of the neural network, defined as a list of layer sizes.\n",
        "        For example, [1, 100, 100, 100, 1] represents an input layer with 1 neuron,\n",
        "        three hidden layers with 20 neurons each, and an output layer with 1 neuron.\n",
        "    eta : float\n",
        "        Interaction strength parameter for the Gross-Pitaevskii equation.\n",
        "    epochs : int\n",
        "        Number of training epochs.\n",
        "    model_save_path : str\n",
        "        File name to save the trained model weights (e.g., 'model_eta_1.pth').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : GrossPitaevskiiPINN\n",
        "        The trained PINN model.\n",
        "    loss_history : list of float\n",
        "        A list of loss values recorded during training for each epoch\n",
        "    \"\"\"\n",
        "    model = GrossPitaevskiiPINN(layers).to(device)\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "    # Train the model\n",
        "    model, loss_history = train_pinn(N_u=N_u, N_f=N_f, layers=layers, eta=eta, epochs=epochs, model_save_path=model_save_path)\n",
        "\n",
        "    # Directory to save the models\n",
        "    model_save_dir = 'models'\n",
        "    os.makedirs(model_save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    # Save model after training\n",
        "    save_model_path = os.path.join(model_save_dir, model_save_path)\n",
        "    torch.save(model.state_dict(), save_model_path)  # Save model weights\n",
        "\n",
        "    return model, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vjY89lOvYF0h",
      "metadata": {
        "id": "vjY89lOvYF0h"
      },
      "source": [
        "# Predict and Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YZ-IcULzYIDu",
      "metadata": {
        "id": "YZ-IcULzYIDu"
      },
      "outputs": [],
      "source": [
        "def predict_and_plot(models, etas, grid_resolution, center=(np.pi / 2, np.pi / 2), radius=np.pi / 2, save_path='plots/predicted_solutions_2d.png'):\n",
        "    \"\"\"\n",
        "    Predict and plot the solutions for all models and save the plot for 2D data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    models : list of models\n",
        "        A list of trained models.\n",
        "    etas : list of float\n",
        "        A list of eta values corresponding to each model.\n",
        "    grid_resolution : int\n",
        "        Resolution of the grid for predictions in each dimension.\n",
        "    center : tuple of float, optional\n",
        "        Center of the circular domain for predictions (default is (pi/2, pi/2)).\n",
        "    radius : float, optional\n",
        "        Radius of the circular domain for predictions (default is pi/2).\n",
        "    save_path : str, optional\n",
        "        The path to save the plot image (default is 'plots/predicted_solutions_2d.png').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Ensure the plots directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    # Generate grid points for predictions\n",
        "    x_vals = np.linspace(center[0] - radius, center[0] + radius, grid_resolution)\n",
        "    y_vals = np.linspace(center[1] - radius, center[1] + radius, grid_resolution)\n",
        "    X, Y = np.meshgrid(x_vals, y_vals)\n",
        "    X_test = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    for model, eta in zip(models, etas):\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "        # Make predictions and normalize\n",
        "        u_pred = model(X_test_tensor).detach().cpu().numpy()\n",
        "        u_pred_normalized = u_pred / np.max(np.abs(u_pred))\n",
        "\n",
        "        # Reshape and plot the normalized solution\n",
        "        u_pred_normalized = u_pred_normalized.reshape((grid_resolution, grid_resolution))\n",
        "        plt.contourf(X, Y, u_pred_normalized, levels=50, cmap='viridis', alpha=0.7)\n",
        "        plt.colorbar(label=f'Predicted Solution ($\\\\eta$ ≈ {eta})')\n",
        "\n",
        "    plt.title('Ground State Solution by PINN (2D Riesz Method)', fontsize=\"xx-large\")\n",
        "    plt.xlabel('x', fontsize=\"xx-large\")\n",
        "    plt.ylabel('y', fontsize=\"xx-large\")\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "    # Set larger tick sizes\n",
        "    plt.xticks(fontsize=\"x-large\")\n",
        "    plt.yticks(fontsize=\"x-large\")\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kE42RFBuYLPG",
      "metadata": {
        "id": "kE42RFBuYLPG"
      },
      "source": [
        "# Plot Loss History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WnUKr0ukYOex",
      "metadata": {
        "id": "WnUKr0ukYOex"
      },
      "outputs": [],
      "source": [
        "def plot_loss_history(loss_histories, etas, save_path='plots/loss_history.png'):\n",
        "    \"\"\"\n",
        "    Plot the training loss history for different values of eta and save the plot.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    loss_histories : list of lists\n",
        "        A list where each element is a list of loss values recorded during training\n",
        "        for a specific eta.\n",
        "    etas : list of float\n",
        "        A list of eta values corresponding to each loss history.\n",
        "    save_path : str, optional\n",
        "        The path to save the plot image (default is 'plots/loss_history.png').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Ensure the plots directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for loss_history, eta in zip(loss_histories, etas):\n",
        "        plt.plot(loss_history, label=f'Loss ($\\\\eta$ ≈ {eta})')\n",
        "\n",
        "    plt.xlabel('Training step (x 100)', fontsize=\"xx-large\")\n",
        "    plt.ylabel('Total Loss', fontsize=\"xx-large\")\n",
        "    plt.yscale('log')\n",
        "    plt.title('Loss History for Different Interaction Strengths ($\\\\eta$)', fontsize=\"xx-large\")\n",
        "    plt.legend(fontsize=\"large\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Set larger tick sizes\n",
        "    plt.xticks(fontsize=\"x-large\")\n",
        "    plt.yticks(fontsize=\"x-large\")\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7RmggB2e-Z7r",
      "metadata": {
        "id": "7RmggB2e-Z7r"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "js3eLQbPlIgm",
      "metadata": {
        "id": "js3eLQbPlIgm"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "N_u = 500  # Number of boundary points\n",
        "N_f = 10000  # Number of collocation points\n",
        "epochs = 20001  # Number of iterations of training\n",
        "layers = [2, 50, 50, 50, 1]  # Neural network architecture\n",
        "etas = [1, 10, 100, 1000]  # Interaction strengths\n",
        "grid_resolution = 100  # Resolution for 2D prediction grid\n",
        "center = (np.pi / 2, np.pi / 2)  # Center of prediction domain\n",
        "radius = np.pi / 2  # Radius of prediction domain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2AqQL8hWphaH",
      "metadata": {
        "id": "2AqQL8hWphaH"
      },
      "source": [
        "# Loop through and plot all potential types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7oJgKs8Ejr2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oJgKs8Ejr2d",
        "outputId": "1af7d01a-cec0-4488-9175-2471d30dc5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/20001], Loss: 1412.895142\n"
          ]
        }
      ],
      "source": [
        "# Train and save models and loss history for different interaction strengths\n",
        "models = []\n",
        "loss_histories = []\n",
        "for eta in etas:\n",
        "    model_save_path = f\"trained_model_eta_{eta}_2d.pth\"\n",
        "    model, loss_history = train_and_save_pinn(\n",
        "        N_u=N_u, N_f=N_f, layers=layers, eta=eta, epochs=epochs, model_save_path=model_save_path\n",
        "    )\n",
        "    models.append(model)\n",
        "    loss_histories.append(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 2D grid for predictions\n",
        "x_vals = np.linspace(center[0] - radius, center[0] + radius, grid_resolution)\n",
        "y_vals = np.linspace(center[1] - radius, center[1] + radius, grid_resolution)\n",
        "X_test = np.hstack(np.meshgrid(x_vals, y_vals)).reshape(-1, 2)\n",
        "\n",
        "# Predict and plot the solutions for all models\n",
        "predict_and_plot(models, etas, grid_resolution, center=center, radius=radius, save_path='plots/predicted_solutions_2d.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "LmbxiUbExRgW",
        "outputId": "3a4e6253-8f44-497f-b65a-7a5055a25db0"
      },
      "id": "LmbxiUbExRgW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-1-fd503cd0cf43>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-fd503cd0cf43>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    x_vals = np.linspace(center[0] - radius, center[0] + radius, grid_resolution)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss history for all etas\n",
        "plot_loss_history(loss_histories, etas, save_path='plots/loss_history_2d.png')"
      ],
      "metadata": {
        "id": "sAdU8HVXxVE8"
      },
      "id": "sAdU8HVXxVE8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}