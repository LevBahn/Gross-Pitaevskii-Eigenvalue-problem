{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "efbqfOtOZasn",
      "metadata": {
        "id": "efbqfOtOZasn"
      },
      "source": [
        "This notebook implements supervised weights for approximating the solution to the one-dimensional Gross-Pitavskii equation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3nYdXyrr-Z7h",
      "metadata": {
        "id": "3nYdXyrr-Z7h"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch-optimizer\n",
        "!git clone https://github.com/facebookresearch/optimizers.git\n",
        "%cd optimizers\n",
        "!pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "10ZOj98FDjO5"
      },
      "id": "10ZOj98FDjO5",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "BMnx60Sj-Z7j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnx60Sj-Z7j",
        "outputId": "8e9b5ead-56c4-46bf-d7cf-ac405c5182de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.autograd import grad\n",
        "from scipy.special import hermite\n",
        "# from adabelief_pytorch import AdaBelief\n",
        "from pytorch_optimizer import QHAdam, AdaHessian, Ranger21, SophiaH, Shampoo\n",
        "from distributed_shampoo import AdamGraftingConfig, DistributedShampoo, DefaultEigenvalueCorrectedShampooConfig\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hglIb1ul-Z7n",
      "metadata": {
        "id": "hglIb1ul-Z7n"
      },
      "source": [
        "# Physics Informed Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "NFVJh4jB-Z7o",
      "metadata": {
        "id": "NFVJh4jB-Z7o"
      },
      "outputs": [],
      "source": [
        "class GrossPitaevskiiPINN(nn.Module):\n",
        "    \"\"\"\n",
        "    Physics-Informed Neural Network (PINN) for solving the 1D Gross-Pitaevskii Equation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, hbar=1.0, m=1.0, g=100.0, mode=0, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        layers : list of int\n",
        "            Neural network architecture, each entry defines the number of neurons in that layer.\n",
        "        hbar : float, optional\n",
        "            Reduced Planck's constant (default is 1.0).\n",
        "        m : float, optional\n",
        "            Mass of the particle (default is 1.0).\n",
        "        g : float, optional\n",
        "            Interaction strength (default is 100.0).\n",
        "        mode : int, optional\n",
        "            Mode number (default is 0).\n",
        "        prev_prediction : callable, optional\n",
        "            A function representing the previous model's forward pass, used to incorporate past solutions\n",
        "            when computing the current solution (default is None).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.network = self.build_network()\n",
        "        self.g = g  # Interaction strength\n",
        "        self.hbar = hbar  # Planck's constant, fixed\n",
        "        self.m = m  # Particle mass, fixed\n",
        "        self.mode = mode  # Mode number (n)\n",
        "\n",
        "    def build_network(self):\n",
        "        \"\"\"\n",
        "        Build the neural network with sine activation functions between layers.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        nn.Sequential\n",
        "            A PyTorch sequential model representing the neural network architecture.\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            layers.append(nn.Linear(self.layers[i], self.layers[i + 1]))\n",
        "            if i < len(self.layers) - 2:\n",
        "                layers.append(nn.Tanh())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def weighted_hermite(self, x, n):\n",
        "        \"\"\"\n",
        "        Compute the weighted Hermite polynomial solution for the linear case (gamma = 0).\n",
        "        Equation (34) in https://www.sciencedirect.com/science/article/abs/pii/S0010465513001318.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of spatial coordinates (collocation points) or boundary points.\n",
        "        n : int\n",
        "            Mode of ground state solution to Gross-Pitavskii equation (0 for base ground state)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The weighted Hermite polynomial solution for the linear case (gamma = 0).\n",
        "        \"\"\"\n",
        "        H_n = hermite(n)(x.cpu().detach().numpy())  # Hermite polynomial evaluated at x\n",
        "        norm_factor = (2**n * math.factorial(n) * np.sqrt(np.pi))**(-0.5)\n",
        "        weighted_hermite = norm_factor * torch.exp(-x**2 / 2) * torch.tensor(H_n, dtype=torch.float32).to(device)\n",
        "\n",
        "        return weighted_hermite\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the neural network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor containing spatial points (collocation points).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor representing the predicted solution.\n",
        "        \"\"\"\n",
        "        return self.network(inputs)\n",
        "\n",
        "    def compute_potential(self, x, potential_type=\"harmonic\", **kwargs):\n",
        "        \"\"\"\n",
        "        Compute a symmetric or asymmetric potential function for the 1D domain.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of spatial coordinates.\n",
        "        potential_type : str, optional\n",
        "            Type of potential. Options are: \"harmonic\".\n",
        "        kwargs : dict\n",
        "            Additional parameters specific to each potential type.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        V : torch.Tensor\n",
        "            Tensor of potential values at the input points.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the potential type is not recognized.\n",
        "        \"\"\"\n",
        "        if potential_type == \"harmonic\":\n",
        "            omega = kwargs.get('omega', 1.0)  # Frequency for harmonic potential\n",
        "            V = 0.5 * omega ** 2 * x ** 2\n",
        "        elif potential_type == \"gaussian\":\n",
        "            a = kwargs.get('a', 0.0)  # Center of the Gaussian\n",
        "            V = torch.exp(-(x - a) ** 2)\n",
        "        elif potential_type == \"periodic\":\n",
        "            V0 = kwargs.get('V0', 1.0)  # Depth of the potential\n",
        "            k = kwargs.get('k', 2 * np.pi / 5.0)  # Wave number for periodic potential\n",
        "            V = V0 * torch.cos(k * x) ** 2\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown potential type: {potential_type}\")\n",
        "\n",
        "        return V\n",
        "\n",
        "    def compute_thomas_fermi_approx(self, lambda_pde, potential, eta):\n",
        "        \"\"\"\n",
        "        Calculate the Thomas–Fermi approximation for the given potential.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lambda_pde : float\n",
        "            Eigenvalue from lowest enegry ground state.\n",
        "        potential : torch.Tensor\n",
        "            Potential values corresponding to the spatial coordinates.\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Thomas–Fermi approximation of the wave function.\n",
        "        \"\"\"\n",
        "        tf_approx = torch.sqrt(torch.relu((lambda_pde - potential) / eta))\n",
        "        return tf_approx\n",
        "\n",
        "    def boundary_loss(self, boundary_points, boundary_values, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Compute the boundary loss (MSE) for the boundary conditions.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        boundary_points : torch.Tensor\n",
        "            Input tensor of boundary spatial points.\n",
        "        boundary_values : torch.Tensor\n",
        "            Tensor of boundary values (for Dirichlet conditions).\n",
        "        prev_prediction : GrossPitaevskiiPINN or None\n",
        "            Previously trained model whose predictions are used as part of the training process.\n",
        "            If None, the model starts training from scratch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Mean squared error (MSE) at the boundary points.\n",
        "        \"\"\"\n",
        "        u_pred = self.forward(boundary_points)\n",
        "\n",
        "        if prev_prediction is None:\n",
        "            u = self.weighted_hermite(boundary_points, self.mode) + u_pred\n",
        "        else:\n",
        "            u = prev_prediction(boundary_points) + u_pred  # Use model’s output from previous eta\n",
        "\n",
        "        return torch.mean((u - boundary_values) ** 2)\n",
        "\n",
        "    def riesz_loss(self, predictions, inputs, eta, potential_type, precomputed_potential=None, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Compute the Riesz energy loss for the Gross-Pitaevskii equation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : torch.Tensor\n",
        "            Predicted solution from the network.\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor of spatial coordinates (collocation points).\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "        potential_type : str\n",
        "            Type of potential function to use.\n",
        "        precomputed_potential : torch.Tensor\n",
        "            Precomputed potential. Default is None.\n",
        "        prev_prediction : GrossPitaevskiiPINN or None\n",
        "            Previously trained model whose predictions are used as part of the training process.\n",
        "            If None, the model starts training from scratch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Riesz energy loss value.\n",
        "        \"\"\"\n",
        "        if prev_prediction is None:\n",
        "            u = self.weighted_hermite(inputs, self.mode) + predictions\n",
        "        else:\n",
        "            scaling_factor = torch.sqrt(torch.tensor(eta, dtype=torch.float32, device=device))\n",
        "            u = prev_prediction(inputs) + predictions  # Use model’s output from previous eta\n",
        "\n",
        "        if not inputs.requires_grad:\n",
        "            inputs = inputs.clone().detach().requires_grad_(True)\n",
        "        u_x = torch.autograd.grad(outputs=u, inputs=inputs,\n",
        "                                  grad_outputs=torch.ones_like(predictions),\n",
        "                                  create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        laplacian_term = torch.mean(u_x ** 2)  # Kinetic term\n",
        "        if precomputed_potential is not None:\n",
        "            V = precomputed_potential\n",
        "        else:\n",
        "            V = self.compute_potential(inputs, potential_type)\n",
        "        potential_term = torch.mean(V * u ** 2)  # Potential term\n",
        "        interaction_term = 0.5 * eta * torch.mean(u ** 4)  # Interaction term\n",
        "\n",
        "        riesz_energy = 0.5 * (laplacian_term + potential_term + interaction_term)\n",
        "\n",
        "        return riesz_energy\n",
        "\n",
        "    def pde_loss(self, inputs, predictions, eta, potential_type, precomputed_potential=None, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Compute the PDE loss for the Gross-Pitaevskii equation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            Input tensor of spatial coordinates (collocation points).\n",
        "        predictions : torch.Tensor\n",
        "            Predicted solution from the network.\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "        potential_type : str\n",
        "            Type of potential function to use.\n",
        "        precomputed_potential : torch.Tensor\n",
        "            Precomputed potential. Default is None.\n",
        "        prev_prediction : GrossPitaevskiiPINN or None\n",
        "            Previously trained model whose predictions are used as part of the training process.\n",
        "            If None, the model starts training from scratch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            Tuple containing:\n",
        "                - torch.Tensor: PDE loss value.\n",
        "                - torch.Tensor: PDE residual.\n",
        "                - torch.Tensor: Smallest eigenvalue (lambda).\n",
        "        \"\"\"\n",
        "\n",
        "        base_solution = self.weighted_hermite(inputs, self.mode)\n",
        "\n",
        "        if prev_prediction is None:\n",
        "            u = base_solution + predictions\n",
        "        else:\n",
        "            # Should be fixed by prev_model = prev_model + model in main()\n",
        "            #u = base_solution + prev_prediction(inputs) + predictions # Use model’s output from previous eta\n",
        "            u = prev_prediction(inputs) + predictions\n",
        "\n",
        "        # Compute first and second derivatives with respect to x\n",
        "        u_x = grad(u, inputs, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        u_xx = grad(u_x, inputs, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
        "\n",
        "        # Compute λ from the energy functional\n",
        "        if precomputed_potential is not None:\n",
        "            V = precomputed_potential\n",
        "        else:\n",
        "            V = self.compute_potential(inputs, potential_type)\n",
        "        lambda_pde = torch.mean(u_x ** 2 + V * u ** 2 + eta * u ** 4) / torch.mean(u ** 2)\n",
        "\n",
        "        # Residual of the 1D Gross-Pitaevskii equation\n",
        "        if precomputed_potential is not None:\n",
        "            V = precomputed_potential\n",
        "        else:\n",
        "            V = self.compute_potential(inputs, potential_type)\n",
        "        pde_residual = -u_xx + V * u + eta * torch.abs(u ** 2) * u - lambda_pde * u\n",
        "\n",
        "        # Regularization: See https://arxiv.org/abs/2010.05075\n",
        "\n",
        "        # Term 1: L_f = 1 / (f(x, λ))^2, penalizes the network if the PDE residual is close to zero to avoid trivial eigenfunctions\n",
        "        L_f = 1 / (torch.mean(u ** 2) + 1e-2)\n",
        "\n",
        "        # Term 2: L_λ = 1 / λ^2, penalizes small eigenvalues λ, ensuring non-trivial eigenvalues\n",
        "        L_lambda = 1 / (lambda_pde ** 2 + 1e-6)\n",
        "\n",
        "        # Term 3: L_drive = e^(-λ + c), encourages λ to grow, preventing collapse to small values\n",
        "        L_drive = torch.exp(-lambda_pde + 1.0)\n",
        "\n",
        "        # PDE loss (residual plus regularization terms)\n",
        "        pde_loss = torch.mean(pde_residual ** 2)  #+ L_lambda + L_f\n",
        "\n",
        "        return pde_loss, pde_residual, lambda_pde\n",
        "\n",
        "    def symmetry_loss(self, collocation_points, lb, ub):\n",
        "        \"\"\"\n",
        "        Compute the symmetry loss to enforce u(x) = u((a+b)-x).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        collocation_points : torch.Tensor\n",
        "            Tensor of interior spatial points.\n",
        "        lb : torch.Tensor\n",
        "            Lower bound of interval.\n",
        "        ub: torch.Tensor\n",
        "            Upper bound of interval.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sym_loss : torch.Tensor\n",
        "            The mean squared error enforcing symmetry u(x) = u((a+b)-x).\n",
        "        \"\"\"\n",
        "        # Reflect points across the center of the domain\n",
        "        x_reflected = (lb + ub) - collocation_points\n",
        "\n",
        "        # Evaluate u(x) and u((a+b)-x)\n",
        "        u_original = self.forward(collocation_points)\n",
        "        u_reflected = self.forward(x_reflected)\n",
        "\n",
        "        # Compute MSE to enforce symmetry\n",
        "        sym_loss = torch.mean((u_original - u_reflected) ** 2)\n",
        "        return sym_loss\n",
        "\n",
        "    def total_loss(self, collocation_points, boundary_points, boundary_values, eta, lb, ub, weights, potential_type,\n",
        "                   precomputed_potential=None, prev_prediction=None):\n",
        "        \"\"\"\n",
        "        Compute the total loss combining boundary loss, Riesz energy loss,\n",
        "        PDE loss, L^2 norm regularization loss, and symmetry loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        collocation_points : torch.Tensor\n",
        "            Input tensor of spatial coordinates for the interior points.\n",
        "        boundary_points : torch.Tensor\n",
        "            Input tensor of boundary spatial points.\n",
        "        boundary_values : torch.Tensor\n",
        "            Tensor of boundary values (for Dirichlet conditions).\n",
        "        eta : float\n",
        "            Interaction strength.\n",
        "        lb : torch.Tensor\n",
        "            Lower bound of interval.\n",
        "        ub : torch.Tensor\n",
        "            Upper bound of interval.\n",
        "        weights : list\n",
        "            Weights for different loss terms.\n",
        "        potential_type : str\n",
        "            Type of potential function to use\n",
        "        precomputed_potential : torch.Tensor\n",
        "            Precomputed potential. Default is None.\n",
        "        prev_prediction : GrossPitaevskiiPINN or None\n",
        "            Previously trained model whose predictions are used as part of the training process.\n",
        "            If None, the model starts training from scratch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        total_loss : torch.Tensor\n",
        "            Total loss value.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use precomputed potential if provided\n",
        "        if precomputed_potential is not None:\n",
        "            V = precomputed_potential\n",
        "        else:\n",
        "            V = self.compute_potential(collocation_points, potential_type)\n",
        "\n",
        "        # Compute individual loss components\n",
        "        data_loss = self.boundary_loss(boundary_points, boundary_values, prev_prediction)\n",
        "        riesz_energy_loss = self.riesz_loss(self.forward(collocation_points), collocation_points, eta, potential_type,V, prev_prediction)\n",
        "        pde_loss, _, _ = self.pde_loss(collocation_points, self.forward(collocation_points), eta, potential_type, V, prev_prediction)\n",
        "        norm_loss = (torch.norm(self.forward(collocation_points), p=2) - 1) ** 2\n",
        "        sym_loss = self.symmetry_loss(collocation_points, lb, ub)\n",
        "\n",
        "        # Scaling factor for pde loss and riesz energy loss\n",
        "        domain_length = ub - lb\n",
        "\n",
        "        # Compute weighted losses and total loss\n",
        "        losses = [data_loss, riesz_energy_loss  / domain_length, pde_loss / domain_length, norm_loss, sym_loss]\n",
        "        weighted_losses = [weights[i] * loss for i, loss in enumerate(losses)]\n",
        "        total_loss = sum(weighted_losses)\n",
        "\n",
        "        return total_loss, data_loss, riesz_energy_loss, pde_loss, norm_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzMOyXAj7jey",
      "metadata": {
        "id": "FzMOyXAj7jey"
      },
      "source": [
        "# Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "GyMrRGdE7lB2",
      "metadata": {
        "id": "GyMrRGdE7lB2"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    \"\"\"\n",
        "    Initialize the weights of the neural network layers using Xavier uniform initialization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : torch.nn.Module\n",
        "        A layer of the neural network. If it is a linear layer, its weights and biases are initialized.\n",
        "    \"\"\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AKKgTNHo24k4",
      "metadata": {
        "id": "AKKgTNHo24k4"
      },
      "source": [
        "# Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "1kAWdOgL26tv",
      "metadata": {
        "id": "1kAWdOgL26tv"
      },
      "outputs": [],
      "source": [
        "def prepare_training_data(N_u, N_f, lb, ub):\n",
        "    \"\"\"\n",
        "    Prepare boundary and collocation points for training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_u : int\n",
        "        Number of boundary points.\n",
        "    N_f : int\n",
        "        Number of collocation points.\n",
        "    lb : np.ndarray\n",
        "        Lower bounds of the domain.\n",
        "    ub : np.ndarray\n",
        "        Upper bounds of the domain.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    collocation_points : np.ndarray\n",
        "        Collocation points.\n",
        "    boundary_points : np.ndarray\n",
        "        Boundary points.\n",
        "    boundary_values : np.ndarray\n",
        "        Boundary values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Boundary of interval\n",
        "    boundary_points = np.array([[lb], [ub]])\n",
        "    boundary_values = np.zeros((2, 1))\n",
        "\n",
        "    # Dynamically sample points inside the interval\n",
        "    collocation_points = np.random.rand(N_f, 1) * (ub - lb) + lb\n",
        "\n",
        "    return collocation_points, boundary_points, boundary_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B0dvfxBOmPlR",
      "metadata": {
        "id": "B0dvfxBOmPlR"
      },
      "source": [
        "# Train PINN with Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "FXsGZVhYmS1t",
      "metadata": {
        "id": "FXsGZVhYmS1t"
      },
      "outputs": [],
      "source": [
        "def train_pinn_with_optimizer(X, N_u, N_f, layers, eta, epochs, lb, ub, weights, model_save_path, potential_type,\n",
        "                              prev_prediction, optimizer_name):\n",
        "    \"\"\"\n",
        "    Train the Physics-Informed Neural Network (PINN) for the 1D Gross-Pitaevskii equation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.ndarray\n",
        "        Input data for the neural network\n",
        "    N_u : int\n",
        "        Number of boundary points\n",
        "    N_f : int\n",
        "        Number of collocation points (interior points) for the physics-based loss\n",
        "    layers : list of int\n",
        "        Architecture of the neural network\n",
        "    eta : float\n",
        "        Interaction strength\n",
        "    epochs: int\n",
        "        Number of epochs\n",
        "    lb : int\n",
        "        Lower bound of interval.\n",
        "    ub : int\n",
        "        Upper bound of interval.\n",
        "    weights : list\n",
        "        Weights for different loss terms.\n",
        "    model_save_path : str\n",
        "        Save path for trained model\n",
        "    potential_type: str\n",
        "        Type of potential function to use\n",
        "    prev_prediction : GrossPitaevskiiPINN or None\n",
        "        Previously trained model whose predictions are used as part of the training process.\n",
        "        If None, the model starts training from scratch.\n",
        "    optimizer_name : str\n",
        "        Name of optimizer to train.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : GrossPitaevskiiPINN\n",
        "        The trained model\n",
        "    loss_history : list of float\n",
        "        List of loss values recorded during training for the specific optimizer.\n",
        "    lambda_pde_history : list of float\n",
        "        List of λ_PDE values recorded during training for the specific optimizer.\n",
        "    \"\"\"\n",
        "    # Instantiate the PINN model for each optimizer and initialize its weights\n",
        "    model = GrossPitaevskiiPINN(layers).to(device)\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "    # Select the optimizer\n",
        "    optimizers = {\n",
        "        \"Adam\": optim.Adam(model.parameters(), lr=1e-3),\n",
        "        \"Shampoo\": DistributedShampoo( model.parameters(),\n",
        "                                       lr=0.001,\n",
        "                                       betas=(0.9, 0.999),\n",
        "                                       epsilon=1e-12,\n",
        "                                       weight_decay=1e-05,\n",
        "                                       max_preconditioner_dim=8192,\n",
        "                                       start_preconditioning_step=100,\n",
        "                                       precondition_frequency=100,\n",
        "                                       use_decoupled_weight_decay=False,\n",
        "                                       grafting_config=AdamGraftingConfig(beta2=0.999,epsilon=1e-08),\n",
        "                                       ),\n",
        "        \"Shampoo-Eigen\": DistributedShampoo(model.parameters(),\n",
        "                                      lr=0.001,\n",
        "                                      betas=(0.9, 0.999),\n",
        "                                      epsilon=1e-12,\n",
        "                                      weight_decay=1e-05,\n",
        "                                      max_preconditioner_dim=8192,\n",
        "                                      start_preconditioning_step=100,\n",
        "                                      precondition_frequency=100,\n",
        "                                      use_decoupled_weight_decay=True,\n",
        "                                      # This can also be set to `DefaultSOAPConfig` which uses QR decompositions, hence is\n",
        "                                      # less expensive and might thereby allow for a smaller `precondition_frequency`.\n",
        "                                      preconditioner_config=DefaultEigenvalueCorrectedShampooConfig\n",
        "                                      )\n",
        "\n",
        "    }\n",
        "\n",
        "    optimizer = optimizers[optimizer_name]\n",
        "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=epochs // 10, epochs=epochs)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=25, factor=0.5, verbose=True)\n",
        "\n",
        "    # Prepare training data (collocation and boundary points)\n",
        "    collocation_points, boundary_points, boundary_values = prepare_training_data(N_u, N_f, lb, ub)\n",
        "\n",
        "    # Convert data to PyTorch tensors and move to device\n",
        "    collocation_points_tensor = torch.tensor(collocation_points, dtype=torch.float32, requires_grad=True).to(device)\n",
        "    boundary_points_tensor = torch.tensor(boundary_points, dtype=torch.float32).to(device)\n",
        "    boundary_values_tensor = torch.tensor(boundary_values, dtype=torch.float32).to(device)\n",
        "    lb_tensor = torch.tensor(lb, dtype=torch.float32).to(device)\n",
        "    ub_tensor = torch.tensor(ub, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Precompute potential\n",
        "    V = model.compute_potential(collocation_points_tensor, potential_type).detach()\n",
        "    V.requires_grad = False\n",
        "\n",
        "    # Precompute Thomas-Fermi approximation\n",
        "    #tf_approx = model.compute_thomas_fermi_approx(collocation_points_tensor, V, eta).detach()\n",
        "    #tf_approx.requires_grad = False\n",
        "\n",
        "    loss_history = []\n",
        "    lambda_pde_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate the total loss (boundary, Riesz energy, PDE, normalization, and symmetry losses)\n",
        "        # with precomputed potential and Thomas-Fermi approximation\n",
        "        loss, data_loss, riesz_energy, pde_loss, norm_loss = model.total_loss(collocation_points_tensor,\n",
        "                                                                              boundary_points_tensor,\n",
        "                                                                              boundary_values_tensor,\n",
        "                                                                              eta,\n",
        "                                                                              lb_tensor, ub_tensor,\n",
        "                                                                              weights, potential_type, V,\n",
        "                                                                              prev_prediction)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
        "        optimizer.step()\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        # Record the total loss and lambda_pde every 100 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            _, _, lambda_pde = model.pde_loss(\n",
        "                collocation_points_tensor, model.forward(collocation_points_tensor),\n",
        "                eta, potential_type, V, prev_prediction\n",
        "            )\n",
        "            loss_history.append(loss.item())\n",
        "            lambda_pde_history.append(lambda_pde.detach().cpu().item())\n",
        "\n",
        "        # Compute λ_PDE, η, and loss every 1,000 epochs\n",
        "        if epoch % 1000 == 0:\n",
        "            _, _, lambda_pde = model.pde_loss(\n",
        "                collocation_points_tensor, model.forward(collocation_points_tensor),\n",
        "                eta, potential_type, V, prev_prediction\n",
        "            )\n",
        "            print(f\"[{optimizer_name}] Epoch [{epoch}/{epochs}]: η = {eta}, λ_PDE = {lambda_pde.item():.6f}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    return model, loss_history, lambda_pde_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8P5eHv-QCQiE",
      "metadata": {
        "id": "8P5eHv-QCQiE"
      },
      "source": [
        "# Normalize Wave Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "wUq16rY6CUd9",
      "metadata": {
        "id": "wUq16rY6CUd9"
      },
      "outputs": [],
      "source": [
        "def normalize_wave_function(u):\n",
        "    \"\"\"\n",
        "    Normalize the wave function with respect to its maximum value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    u : torch.Tensor\n",
        "        The predicted wave function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The normalized wave function.\n",
        "    \"\"\"\n",
        "    return np.abs(u) / np.max(np.abs(u))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t7r4H0TVkyZb",
      "metadata": {
        "id": "t7r4H0TVkyZb"
      },
      "source": [
        "# Plot Potential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "__9usRWzk15-",
      "metadata": {
        "id": "__9usRWzk15-"
      },
      "outputs": [],
      "source": [
        "def plot_potential_1D(X_test, potential):\n",
        "    \"\"\"\n",
        "    Plot the 1D potential function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_test : np.ndarray\n",
        "        The test points where the potential is computed.\n",
        "    potential : np.ndarray\n",
        "        The computed potential values at the test points.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6, 5))\n",
        "\n",
        "    # X_test is the x-values (positions) of the 1D potential\n",
        "    plt.plot(X_test, potential, label='Potential $V(x)$', color='green')\n",
        "\n",
        "    plt.title('Potential $V(x)$ in 1D')\n",
        "    plt.xlabel('$x$')\n",
        "    plt.ylabel('$V(x)$')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mS-f4Mkzk4KD",
      "metadata": {
        "id": "mS-f4Mkzk4KD"
      },
      "source": [
        "# Train and Save PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "7XcQoQhFk78S",
      "metadata": {
        "id": "7XcQoQhFk78S"
      },
      "outputs": [],
      "source": [
        "def train_and_save_pinn(X, N_u, N_f, layers, eta, epochs, lb, ub, weights, model_save_path, potential_type,\n",
        "                        prev_model, optimizer_name):\n",
        "    \"\"\"\n",
        "    Train the Physics-Informed Neural Network (PINN) model and save it.\n",
        "\n",
        "    This function trains a PINN model for the 1D Gross-Pitaevskii equation with a specific interaction\n",
        "    strength (eta) and saves the trained model to a specified path. It also returns the trained model\n",
        "    and the loss history recorded during training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.ndarray\n",
        "        Input data for the training (e.g., spatial domain points).\n",
        "    N_u : int\n",
        "        Number of boundary points.\n",
        "    N_f : int\n",
        "        Number of collocation points (interior points) for physics-based loss.\n",
        "    layers : list of int\n",
        "        Architecture of the neural network, defined as a list of layer sizes.\n",
        "        For example, [1, 100, 100, 100, 1] represents an input layer with 1 neuron,\n",
        "        three hidden layers with 20 neurons each, and an output layer with 1 neuron.\n",
        "    eta : float\n",
        "        Interaction strength parameter for the Gross-Pitaevskii equation.\n",
        "    epochs : int\n",
        "        Number of training epochs.\n",
        "    lb : int\n",
        "        Lower bound of interval.\n",
        "    ub : int\n",
        "        Upper bound of interval.\n",
        "    weights : list\n",
        "        Weights for different loss terms.\n",
        "    model_save_path : str\n",
        "        File name to save the trained model weights (e.g., 'model_eta_1.pth').\n",
        "    potential_type: str\n",
        "        Type of potential function to use\n",
        "    prev_model : GrossPitaevskiiPINN or None\n",
        "        Previously trained model whose predictions are used as part of the training process.\n",
        "        If None, the model starts training from scratch.\n",
        "    optimizer_name : str\n",
        "        Name of optimizer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : GrossPitaevskiiPINN\n",
        "        The trained PINN model.\n",
        "    loss_history : dict of (eta, optimizer) -> list of float\n",
        "        A dictionary where each key is a tuple (eta, optimizer name) and\n",
        "        the corresponding value is a list of loss values recorded during training.\n",
        "    lambda_pde_history : dict of (eta, optimizer) -> list of float\n",
        "        A dictionary where each key is a tuple (eta, optimizer name) and\n",
        "        the corresponding value is a list of λ_PDE values recorded during training.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a new model, passing the previous model's forward function if available\n",
        "    prev_prediction = prev_model.forward if prev_model is not None else None\n",
        "    if prev_model is not None:\n",
        "        prev_model.eval()\n",
        "    model = GrossPitaevskiiPINN(layers, prev_prediction=prev_prediction).to(device)\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "    # Train the model\n",
        "    model, loss_history, lambda_pde_history = train_pinn_with_optimizer(X, N_u=N_u, N_f=N_f, layers=layers, eta=eta,\n",
        "                                      epochs=epochs, lb=lb, ub=ub, weights=weights, model_save_path=model_save_path,\n",
        "                                      potential_type=potential_type, prev_prediction=prev_prediction,\n",
        "                                      optimizer_name=optimizer_name)\n",
        "\n",
        "    # Directory to save the models\n",
        "    model_save_dir = 'models'\n",
        "    os.makedirs(model_save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    # Save model after training\n",
        "    save_model_path = os.path.join(model_save_dir, model_save_path)\n",
        "    torch.save(model.state_dict(), save_model_path)  # Save model weights\n",
        "\n",
        "    return model, loss_history, lambda_pde_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vjY89lOvYF0h",
      "metadata": {
        "id": "vjY89lOvYF0h"
      },
      "source": [
        "# Predict and Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "YZ-IcULzYIDu",
      "metadata": {
        "id": "YZ-IcULzYIDu"
      },
      "outputs": [],
      "source": [
        "def predict_and_plot(models, etas, optimizers, X_test, save_path='plots/predicted_solutions.png',\n",
        "                     potential_type='gaussian', prev_prediction=None):\n",
        "    \"\"\"\n",
        "    Predict and plot the solutions for all models, making separate plots for each optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    models : dict\n",
        "        A dictionary where keys are η values, and values are dictionaries mapping optimizer names to trained models.\n",
        "        Example: models[eta][optimizer_name] -> model\n",
        "    etas : list of float\n",
        "        A list of η values used during training.\n",
        "    optimizers : list of str\n",
        "        A list of optimizer names used during training.\n",
        "    X_test : np.ndarray\n",
        "        Test points along the 1D interval.\n",
        "    save_path : str, optional\n",
        "        The path template to save the plot image (default is 'plots/predicted_solutions_{optimizer}.png').\n",
        "    potential_type : str, optional\n",
        "        Type of potential function to use. Default is 'gaussian'.\n",
        "    prev_prediction : callable, optional\n",
        "        A function representing the previous model's forward pass.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Ensure the plots directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    for optimizer_name in optimizers:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        for eta in etas:\n",
        "            if eta in models and optimizer_name in models[eta]:\n",
        "                model = models[eta][optimizer_name]  # Retrieve model for optimizer and eta\n",
        "                model.eval()\n",
        "\n",
        "                # Prepare test data\n",
        "                X_test_tensor = torch.tensor(X_test, dtype=torch.float32, requires_grad=True).to(device)\n",
        "                # This should be adding up all perturbations to the weighted self-hermite approximation\n",
        "                u_pred = model(X_test_tensor).detach().cpu().numpy() #+ prev_prediction(X_test_tensor).detach().cpu().numpy() if prev_prediction is not None else model(X_test_tensor).detach().cpu().numpy()\n",
        "                #u_pred_normalized = normalize_wave_function(u_pred)\n",
        "\n",
        "                # Calculate the potential\n",
        "                potential = model.compute_potential(X_test_tensor, potential_type)\n",
        "\n",
        "                # Calculate λ_PDE and the Thomas-Fermi approximation\n",
        "                _, _, lambda_pde = model.pde_loss(X_test_tensor,\n",
        "                                                  model.forward(X_test_tensor), eta,\n",
        "                                                  potential_type, potential, prev_prediction)\n",
        "                tf_approx = model.compute_thomas_fermi_approx(X_test_tensor, potential, eta).detach().cpu().numpy()\n",
        "                tf_approx_normalized = normalize_wave_function(tf_approx)\n",
        "\n",
        "                # Plot predicted solution for this optimizer\n",
        "                plt.plot(X_test, u_pred, label=f\"η={eta}\", linestyle=\"-\")\n",
        "\n",
        "        # Plot Weighted Hermite Approximation (η=0)\n",
        "        wh_approx = model.weighted_hermite(X_test_tensor, 0).detach().cpu().numpy()\n",
        "        wh_approx_normalized = normalize_wave_function(wh_approx)\n",
        "        plt.plot(X_test, wh_approx_normalized, linestyle=\"--\", color=\"black\",\n",
        "                 label=\"Weighted Hermite (η=0)\")\n",
        "\n",
        "        plt.title(f\"PINN Solutions - {potential_type.capitalize()} - {optimizer_name}\", fontsize=\"xx-large\")\n",
        "        plt.xlabel(\"$x$\", fontsize=\"xx-large\")\n",
        "        plt.ylabel(\"Normalized $u(x)$\", fontsize=\"xx-large\")\n",
        "        plt.grid(True)\n",
        "        plt.legend(fontsize=\"large\")\n",
        "\n",
        "        # Set tick sizes\n",
        "        plt.xticks(fontsize=\"x-large\")\n",
        "        plt.yticks(fontsize=\"x-large\")\n",
        "\n",
        "        # Save the plot for this optimizer\n",
        "        optimizer_plot_path = save_path.format(optimizer=optimizer_name, potential_type=potential_type)\n",
        "        plt.savefig(optimizer_plot_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kE42RFBuYLPG",
      "metadata": {
        "id": "kE42RFBuYLPG"
      },
      "source": [
        "# Plot Loss History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "WnUKr0ukYOex",
      "metadata": {
        "id": "WnUKr0ukYOex"
      },
      "outputs": [],
      "source": [
        "def plot_loss_history(loss_histories, etas, optimizer_names, save_path='plots/loss_history.png', potential_type='gaussian'):\n",
        "    \"\"\"\n",
        "    Plot the training loss history for all optimizers and interaction strengths (etas) in a single plot.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    loss_histories : dict\n",
        "        Dictionary where keys are eta values, and values are dictionaries mapping optimizer names to loss history lists.\n",
        "    etas : list of float\n",
        "        List of eta values used in training.\n",
        "    optimizer_names : list of str\n",
        "        List of optimizer names used during training.\n",
        "    save_path : str, optional\n",
        "        File path to save the plot, by default 'plots/loss_history.png'.\n",
        "    potential_type : str, optional\n",
        "        Type of potential function to use. Default is 'gaussian'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the plots directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for optimizer_name in optimizer_names:\n",
        "        for eta in etas:\n",
        "            if eta in loss_histories and optimizer_name in loss_histories[eta]:\n",
        "                loss = loss_histories[eta][optimizer_name]\n",
        "                plt.plot(np.arange(len(loss)), loss, marker='o', label=f\"{optimizer_name}, η={eta}\")\n",
        "\n",
        "    plt.xlabel('Training step (x 100)', fontsize=\"xx-large\")\n",
        "    plt.ylabel('Total Loss', fontsize=\"xx-large\")\n",
        "    plt.yscale('log')\n",
        "    plt.title(f'Loss History for Different Interaction Strengths ($\\\\eta$) for {potential_type.capitalize()} Potential',\n",
        "              fontsize=\"xx-large\")\n",
        "    plt.legend(fontsize=\"large\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Set larger tick sizes\n",
        "    plt.xticks(fontsize=\"x-large\")\n",
        "    plt.yticks(fontsize=\"x-large\")\n",
        "\n",
        "    # Save the plot\n",
        "    save_path = save_path.format(potential_type=potential_type)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Lambda PDE"
      ],
      "metadata": {
        "id": "Jexl0921gSNq"
      },
      "id": "Jexl0921gSNq"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_lambda_pde(lambda_pde_histories, etas, optimizer_names, save_path=\"plots/lambda_pde.png\", potential_type=\"harmonic\"):\n",
        "    \"\"\"\n",
        "    Plot the evolution of λ_PDE over training iterations for all optimizers and interaction strengths in one figure.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lambda_pde_histories : dict\n",
        "        Dictionary where keys are eta values, and values are dictionaries mapping optimizer names to lambda_pde history lists.\n",
        "    etas : list of float\n",
        "        List of eta values used in training.\n",
        "    optimizer_names : list of str\n",
        "        List of optimizer names used during training.\n",
        "    save_path : str, optional\n",
        "        File path to save the plot, by default 'plots/lambda_pde.png'.\n",
        "    potential_type : str, optional\n",
        "        Type of potential function to use. Default is 'gaussian'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the plots directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for optimizer_name in optimizer_names:\n",
        "        for eta in etas:\n",
        "            if eta in lambda_pde_histories and optimizer_name in lambda_pde_histories[eta]:\n",
        "                lambda_pde = lambda_pde_histories[eta][optimizer_name]\n",
        "                plt.plot(np.arange(len(lambda_pde)), lambda_pde, marker='o', label=f\"{optimizer_name}, η={eta}\")\n",
        "\n",
        "    plt.xlabel('Training step (x 100)', fontsize=\"xx-large\")\n",
        "    plt.ylabel(r\"$\\lambda_{PDE}$\", fontsize=\"xx-large\")\n",
        "    plt.title(\n",
        "        r\"$\\lambda_{PDE}$ for Different Interaction Strengths ($\\eta$) for \" + f\"{potential_type.capitalize()} Potential\",\n",
        "        fontsize=\"xx-large\")\n",
        "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    plt.legend()\n",
        "\n",
        "    # Set larger tick sizes\n",
        "    plt.xticks(fontsize=\"x-large\")\n",
        "    plt.yticks(fontsize=\"x-large\")\n",
        "\n",
        "    # Save the plot\n",
        "    save_path = save_path.format(potential_type=potential_type)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WD8_M1ajgWQq"
      },
      "id": "WD8_M1ajgWQq",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7RmggB2e-Z7r",
      "metadata": {
        "id": "7RmggB2e-Z7r"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "js3eLQbPlIgm",
      "metadata": {
        "id": "js3eLQbPlIgm"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "N_u = 100  # Number of boundary points\n",
        "N_f = 2000  # Number of collocation points\n",
        "epochs = 2001 # Number of iterations of training\n",
        "layers = [1, 100, 100, 100, 1]  # Neural network architecture\n",
        "lb, ub = -10, 10  # Boundary limits\n",
        "X = np.linspace(lb, ub, N_f).reshape(-1, 1)  # Input grid for training\n",
        "\n",
        "# Test points\n",
        "X_test = np.linspace(lb, ub, N_f).reshape(-1, 1)  # Test points for prediction\n",
        "etas = [1, 10, 50, 100]  # Interaction strengths\n",
        "\n",
        "# Weights for loss terms\n",
        "weights = [50.0, 1.0, 2.0, 10.0, 50.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2AqQL8hWphaH",
      "metadata": {
        "id": "2AqQL8hWphaH"
      },
      "source": [
        "# Loop through and plot all potential types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "7oJgKs8Ejr2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "7oJgKs8Ejr2d",
        "outputId": "75c1d20f-bfc5-481d-d585-a2f6b690963a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Adam optimizer...\n",
            "[Adam] Epoch [0/2001]: η = 1, λ_PDE = 8.643791, Loss: 795.866882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Adam] Epoch [1000/2001]: η = 1, λ_PDE = 1.160068, Loss: 0.002386\n",
            "[Adam] Epoch [2000/2001]: η = 1, λ_PDE = 1.136793, Loss: 0.002145\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'NoneType' and 'GrossPitaevskiiPINN'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-781132509c74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                                           \u001b[0mpotential_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpotential_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                           optimizer_name=optimizer_name)\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mprev_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_model\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;31m# Store the trained model for the next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Store models by eta and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'GrossPitaevskiiPINN'"
          ]
        }
      ],
      "source": [
        "#potential_types = ['gaussian', 'harmonic', 'periodic']\n",
        "potential_types = ['harmonic']\n",
        "\n",
        "# Optimizers to compare\n",
        "optimizer_names = [\"Adam\", \"Shampoo\", \"Shampoo-Eigen\"]\n",
        "\n",
        "# Store results for plotting\n",
        "all_loss_histories = {}\n",
        "all_lambda_pde_histories = {}\n",
        "models = {eta: {} for eta in etas}  # Store models for all η and optimizers\n",
        "\n",
        "# Loop through each potential type\n",
        "for potential_type in potential_types:\n",
        "\n",
        "    for optimizer_name in optimizer_names:\n",
        "        print(f\"Training with {optimizer_name} optimizer...\")\n",
        "\n",
        "        # Store results for each optimizer separately\n",
        "        prev_model = None\n",
        "        loss_histories = {}\n",
        "        lambda_pde_histories = {}\n",
        "\n",
        "        for eta in etas:\n",
        "            model_save_path = f\"trained_model_eta_{eta}_{optimizer_name}.pth\"\n",
        "            model, loss_history, lambda_pde_history = train_and_save_pinn(X, N_u=N_u, N_f=N_f, layers=layers, eta=eta,\n",
        "                                                                          epochs=epochs, lb=lb, ub=ub,\n",
        "                                                                          weights=weights, model_save_path=model_save_path,\n",
        "                                                                          potential_type=potential_type, prev_model=prev_model,\n",
        "                                                                          optimizer_name=optimizer_name)\n",
        "            prev_model = prev_model + model  # Store the trained model for the next iteration\n",
        "\n",
        "            # Store models by eta and optimizer - TODO: does this need to be previous model?\n",
        "            # This should be adding a small perturbation to the base case (weighted hermite approximation) an\n",
        "            models[eta][optimizer_name] = model\n",
        "\n",
        "            # Store loss and λ_PDE history under each eta for this optimizer\n",
        "            if potential_type not in all_loss_histories:\n",
        "                all_loss_histories[potential_type] = {}\n",
        "            if potential_type not in all_lambda_pde_histories:\n",
        "                all_lambda_pde_histories[potential_type] = {}\n",
        "\n",
        "            all_loss_histories[potential_type].setdefault(eta, {})[optimizer_name] = loss_history\n",
        "            all_lambda_pde_histories[potential_type].setdefault(eta, {})[optimizer_name] = lambda_pde_history\n",
        "\n",
        "    # Predict and plot the solutions for each optimizer separately\n",
        "    predict_and_plot(models, etas, optimizer_names, X_test, save_path='plots/predicted_solutions_{potential_type}.png',\n",
        "                     potential_type=potential_type, prev_prediction=prev_model)\n",
        "\n",
        "    # Plot the loss history for all etas\n",
        "    plot_loss_history(all_loss_histories[potential_type], etas, optimizer_names,\n",
        "                      save_path='plots/loss_history_{potential_type}.png', potential_type=potential_type)\n",
        "\n",
        "    # Plot lambda_pde history for all etas\n",
        "    plot_lambda_pde(all_lambda_pde_histories[potential_type], etas, optimizer_names,\n",
        "                    save_path=f'plots/lambda_pde_{potential_type}.png', potential_type=potential_type)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}