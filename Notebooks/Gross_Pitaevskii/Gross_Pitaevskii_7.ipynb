{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3nYdXyrr-Z7h",
      "metadata": {
        "id": "3nYdXyrr-Z7h"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyDOE"
      ],
      "metadata": {
        "id": "v_KlamxQniKl"
      },
      "id": "v_KlamxQniKl",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "BMnx60Sj-Z7j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnx60Sj-Z7j",
        "outputId": "5b1f342e-9699-401c-ddab-38df11bd5c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from pyDOE import lhs\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "if device == 'cuda': print(torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sine Activation"
      ],
      "metadata": {
        "id": "HtJJPYF_guud"
      },
      "id": "HtJJPYF_guud"
    },
    {
      "cell_type": "code",
      "source": [
        "class SineActivation(nn.Module):\n",
        "    \"\"\" Sine Activation function. \"\"\"\n",
        "    def forward(self, input):\n",
        "            return torch.sin(input)"
      ],
      "metadata": {
        "id": "dkmb4P35gw8f"
      },
      "id": "dkmb4P35gw8f",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "hglIb1ul-Z7n",
      "metadata": {
        "id": "hglIb1ul-Z7n"
      },
      "source": [
        "# Physics Informed Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "NFVJh4jB-Z7o",
      "metadata": {
        "id": "NFVJh4jB-Z7o"
      },
      "outputs": [],
      "source": [
        "class PINN(nn.Module):\n",
        "    \"\"\"\n",
        "    Physics-Informed Neural Network (PINN) for solving the Gross-Pitaevskii equation (GPE).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    layers : nn.ModuleList\n",
        "        List of neural network layers.\n",
        "    ub : torch.Tensor\n",
        "        Upper bound for input normalization.\n",
        "    lb : torch.Tensor\n",
        "        Lower bound for input normalization.\n",
        "    adaptive_bc_scale : torch.nn.Parameter\n",
        "        Learnable scaling factor for boundary condition loss.\n",
        "    hbar : torch.nn.Parameter\n",
        "        Learnable or fixed parameter for Planck's constant.\n",
        "    m : torch.nn.Parameter\n",
        "        Learnable parameter for particle mass.\n",
        "    g : torch.nn.Parameter\n",
        "        Learnable parameter for interaction strength.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, ub, lb, hbar=1.0, m=1.0, g=1.0):\n",
        "        \"\"\"\n",
        "        Initializes the PINN model with given layer sizes and boundary conditions.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layers : list\n",
        "            List of integers specifying the number of units in each layer.\n",
        "        ub : list or numpy.ndarray\n",
        "            Upper bounds of the input domain for feature normalization.\n",
        "        lb : list or numpy.ndarray\n",
        "            Lower bounds of the input domain for feature normalization.\n",
        "        hbar : float, optional\n",
        "            Planck's constant, default is the physical value in J⋅Hz−1.\n",
        "        m : float\n",
        "            Particle mass, scaled is 1.0.\n",
        "        g : float\n",
        "            Interaction strength, scaled is 1.0.\n",
        "        \"\"\"\n",
        "        super(PINN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.ub = torch.tensor(ub, dtype=torch.float32, device=device)\n",
        "        self.lb = torch.tensor(lb, dtype=torch.float32, device=device)\n",
        "\n",
        "        self.adaptive_bc_scale = nn.Parameter(torch.tensor(0.1, device=device))  # Adaptive weighting for BC loss (adjusted 09/29/2024)\n",
        "        self.hbar = hbar  # Planck's constant (fixed or learnable?)\n",
        "        self.m = m # Particle mass (fixed)\n",
        "        self.g = g # Interaction strength (fixed)\n",
        "\n",
        "        # Define network layers\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.layers.append(nn.Linear(layers[i], layers[i + 1]))\n",
        "            if i < len(layers) - 2:\n",
        "                self.layers.append(SineActivation()) # Replaced LeakyReLU/Tanh with Sine on 09/29/2024\n",
        "\n",
        "        self.activation = SineActivation() # Replaced LeakyReLU/Tanh with Sine on 09/29/2024\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\" Initialize weights using Xavier initialization. \"\"\"\n",
        "        for m in self.layers:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 1e-2)  # Perturb bias from zero (Added 09/29/24)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the neural network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor for the neural network.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output of the neural network after forward pass.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure lb and ub are broadcastable to the shape of x\n",
        "        lb = self.lb.view(1, -1)  # Ensure lb shape is (1, num_features)\n",
        "        ub = self.ub.view(1, -1)  # Ensure ub shape is (1, num_features)\n",
        "\n",
        "        # Normalize the inputs\n",
        "        x = (x - lb) / (ub - lb)\n",
        "\n",
        "        # Use gradient checkpointing for layers to save memory\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i % 2 == 0:  # Apply checkpointing only to Linear layers\n",
        "                x = checkpoint(layer, x, use_reentrant=False)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        x = x / torch.sqrt(torch.sum(x ** 2)) # Normalize to L^2 norm = 1\n",
        "\n",
        "        # Add small perturbation to avoid converging to the trivial zero solution (added on 09/29/2024)\n",
        "        perturbation = 1e-1 * torch.randn_like(x)\n",
        "        x += perturbation\n",
        "        return x\n",
        "\n",
        "    def test(self, X_u_test_tensor):\n",
        "        \"\"\"\n",
        "        Tests the model on the test data and computes the relative L2 norm of the error.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_u_test_tensor : torch.Tensor\n",
        "            Input tensor for the test data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        error_vec : torch.Tensor\n",
        "            The relative L2 norm of the error.\n",
        "        u_pred : numpy.ndarray\n",
        "            The predicted output reshaped as a 2D array.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure the test data requires gradients\n",
        "        X_u_test_tensor.requires_grad_(True)\n",
        "\n",
        "        # Use mixed precision during inference\n",
        "        with torch.cuda.amp.autocast():\n",
        "            u_pred = self.forward(X_u_test_tensor)\n",
        "            u_ground_truth, _ = self.get_ground_state(X_u_test_tensor)\n",
        "\n",
        "        # Compute relative L2 norm of the error\n",
        "        error_vec = torch.linalg.norm(u_pred - u_ground_truth) / torch.linalg.norm(u_ground_truth)\n",
        "\n",
        "        # Reshape the predicted output to a 2D array\n",
        "        u_pred_reshaped = u_pred.cpu().detach().numpy().reshape((num_grid_pts, num_grid_pts), order='F')\n",
        "\n",
        "        return error_vec, u_pred_reshaped\n",
        "\n",
        "    def loss_BC(self, x_bc, y_bc):\n",
        "        \"\"\"\n",
        "        Computes the boundary condition (BC) loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_bc : torch.Tensor\n",
        "            Boundary condition input data.\n",
        "        y_bc : torch.Tensor\n",
        "            Boundary condition output (true) values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Scaled mean squared error (MSE) loss for boundary conditions.\n",
        "        \"\"\"\n",
        "\n",
        "        # Predict values for boundary points\n",
        "        u_pred = self.forward(x_bc)\n",
        "\n",
        "        # Ensure boundary conditions are enforced by making y_bc a zero tensor\n",
        "        y_bc = torch.zeros_like(u_pred)\n",
        "\n",
        "        # Adaptive scaling for boundary loss\n",
        "        bc_loss = self.adaptive_bc_scale * torch.mean((u_pred - y_bc) ** 2)\n",
        "\n",
        "        return bc_loss\n",
        "\n",
        "    def riesz_loss(self, predictions, inputs):\n",
        "        \"\"\"\n",
        "        Computes the Riesz energy loss for the Gross-Pitaevskii equation:\n",
        "\n",
        "        E(u) = (1/2) ∫_Ω |∇u|² + V(x)|u|² + (η/2) |u|⁴ dx\n",
        "\n",
        "        where:\n",
        "        - u is the predicted solution,\n",
        "        - V(x) is the potential function,\n",
        "        - η is a constant parameter,\n",
        "        - ∇u represents the gradient of u.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : torch.Tensor\n",
        "            Model predictions.\n",
        "        inputs : torch.Tensor\n",
        "            Input points.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Riesz energy loss.\n",
        "        \"\"\"\n",
        "\n",
        "        u = predictions / torch.sqrt(torch.sum(predictions ** 2)) # Normalize to L^2 norm = 1\n",
        "\n",
        "        if not inputs.requires_grad:\n",
        "            inputs = inputs.clone().detach().requires_grad_(True)\n",
        "        gradients = torch.autograd.grad(outputs=predictions, inputs=inputs,\n",
        "                                        grad_outputs=torch.ones_like(predictions),\n",
        "                                        create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        # Add the potential term V(x) * |u|^2 to the energy functional\n",
        "        V = self.compute_potential(inputs).unsqueeze(1)  # Computes the potential V(x) over the domain\n",
        "\n",
        "        # The 0.5 * self.g * u ** 4 comes from Dirichlet's theorem (minimizing the energy functional)\n",
        "        riesz_energy = torch.mean(gradients ** 2 + V * u ** 2 + 0.5 * self.g * u ** 4)\n",
        "\n",
        "        # Regularize to avoid trivial zero solution (added on 09/29/24)\n",
        "        epsilon = 1e-4  # Small regularization coefficient\n",
        "        riesz_energy += epsilon * torch.mean(torch.abs(u))\n",
        "        return riesz_energy\n",
        "\n",
        "    def pde_loss(self, inputs, predictions):\n",
        "        \"\"\"\n",
        "        Computes the PDE loss for the 2D Gross-Pitaevskii equation:\n",
        "\n",
        "        -∇²u + V(x)u + η|u|²u - λu = 0\n",
        "\n",
        "        The loss is based on the residual of the equation after solving for `u` and computing\n",
        "        the smallest eigenvalue `λ` using the energy functional.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            Input points (x, y).\n",
        "        predictions : torch.Tensor\n",
        "            Predicted output from the network, representing the wave function ψ.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pde_loss: torch.Tensor\n",
        "            Constant representing the Gross-Pitaevskii PDE loss.\n",
        "        pde_residual: torch.Tensor\n",
        "            Tensor representing the residual of the Gross-Pitaevskii PDE.\n",
        "        \"\"\"\n",
        "\n",
        "        u = predictions / torch.sqrt(torch.sum(predictions ** 2)) # Normalize to L^2 norm = 1\n",
        "\n",
        "        # Compute gradients\n",
        "        u_x = torch.autograd.grad(u, inputs, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "\n",
        "        # Second derivatives of the Laplacian (∇²u)\n",
        "        u_xx = torch.autograd.grad(u_x[:, 0], inputs, grad_outputs=torch.ones_like(u_x[:, 0]), create_graph=True)[0][:, 0]\n",
        "        u_yy = torch.autograd.grad(u_x[:, 1], inputs, grad_outputs=torch.ones_like(u_x[:, 1]), create_graph=True)[0][:, 1]\n",
        "        laplacian_u = u_xx + u_yy\n",
        "\n",
        "        # Compute λ directly from the energy functional as the average energy per unit u.\n",
        "        lambda_pde = torch.mean(laplacian_u + self.g * torch.abs(u ** 2) * u) / torch.mean(u ** 2) #  λ is the smallest eigenvalue of the system\n",
        "\n",
        "        # Residual of the PDE (Gross-Pitaevskii equation)\n",
        "        pde_residual = -laplacian_u + self.g * torch.abs(u ** 2) * u - (lambda_pde * u)\n",
        "\n",
        "        # Regularization: See https://arxiv.org/abs/2010.05075\n",
        "\n",
        "        # Regularization term 1: L_f = 1 / (f(x, λ))^2, penalizes the network if the PDE residual is close to zero to\n",
        "        # avoid trivial eigenfunctions\n",
        "        L_f = 1 / (torch.mean(pde_residual ** 2) + 1e-6)  # Add small constant to avoid division by zero\n",
        "\n",
        "        # Regularization term 2: L_λ = 1 / λ^2, penalizes small eigenvalues λ, ensuring non-trivial eigenvalues\n",
        "        L_lambda = 1 / (lambda_pde ** 2 + 1e-6)\n",
        "\n",
        "        # Regularization term 3: L_drive = e^(-λ + c), encourages λ to grow, preventing collapse to small values\n",
        "        c = 1.0  # Tunable\n",
        "        L_drive = torch.exp(-lambda_pde + c)\n",
        "\n",
        "        # PDE loss as the residual plus regularization\n",
        "        pde_loss = torch.mean(pde_residual ** 2) + L_f + L_lambda #+ L_drive\n",
        "\n",
        "        return pde_loss, pde_residual\n",
        "\n",
        "    def loss(self, x_bc, y_bc, x_to_train_f, current_epoch):\n",
        "        \"\"\"\n",
        "        Computes the total loss combining BC loss, PDE loss (Gross Pitaevskii), and Riesz loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_bc : torch.Tensor\n",
        "            Boundary condition input data.\n",
        "        y_bc : torch.Tensor\n",
        "            Boundary condition true values.\n",
        "        x_to_train_f : torch.Tensor\n",
        "            Input points for PDE training.\n",
        "        current_epoch : int\n",
        "        The current training epoch to control the boundary condition scaling over time.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Total loss combining BC, PDE, and Riesz losses.\n",
        "        \"\"\"\n",
        "\n",
        "        # Decrease boundary loss weight over time as needed (added 09/29/204)\n",
        "        epoch_factor = min(1, current_epoch / 500)  # Increase weight gradually over first 500 epochs\n",
        "        loss_u = epoch_factor * self.adaptive_bc_scale * self.loss_BC(x_bc, y_bc)  # Boundary loss\n",
        "\n",
        "        # Add weights for Riesz loss (added on 09/29/24)\n",
        "        alpha = 1.0  # Weight for Riesz loss\n",
        "\n",
        "        predictions = self.forward(x_to_train_f)\n",
        "\n",
        "        # PDE loss (Gross Pitaevskii equation)\n",
        "        loss_pde, _ = self.pde_loss(x_to_train_f, predictions)\n",
        "\n",
        "        # Riesz energy loss\n",
        "        loss_riesz = self.riesz_loss(predictions, x_to_train_f) # Script E in paper\n",
        "\n",
        "        # TODO: Use Riez loss to compute lambda (see after equation (2.5) in paper)\n",
        "        #lambda = constant in paper\n",
        "\n",
        "        # Add a norm regularization term to prevent trivial solutions\n",
        "        norm_constraint = 1e-3 * torch.mean(predictions ** 2)  # Penalize zero solutions\n",
        "\n",
        "        total_loss = loss_u + loss_pde + alpha * loss_riesz + norm_constraint\n",
        "        return total_loss\n",
        "\n",
        "    def compute_potential(self, inputs):\n",
        "        \"\"\"\n",
        "        Compute the harmonic potential V(x) over the domain.\n",
        "\n",
        "        V(x) = 0.5 * omega^2 * (x^2 + y^2)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            The input spatial coordinates (x, y) as a 2D tensor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        V : torch.Tensor\n",
        "            The potential evaluated at each input point.\n",
        "        \"\"\"\n",
        "        x = inputs[:, 0]\n",
        "        y = inputs[:, 1]\n",
        "\n",
        "        # Set omega (tune as needed)\n",
        "        omega = 1.0\n",
        "\n",
        "        # Harmonic potential\n",
        "        V = 0.5 * omega ** 2 * (x ** 2 + y ** 2)\n",
        "\n",
        "        return V\n",
        "\n",
        "    def get_ground_state(self, x):\n",
        "        \"\"\"\n",
        "        Returns the ground state (u vector) and its corresponding energy (lambda scalar).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input points to evaluate the wave function.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        u : torch.Tensor\n",
        "            Ground state wave function values.\n",
        "        lambda_min : float\n",
        "            Corresponding lowest energy value.\n",
        "        \"\"\"\n",
        "\n",
        "        u = self.forward(x)\n",
        "\n",
        "        energy, _ = self.pde_loss(x, u)\n",
        "        return u, energy.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Grid"
      ],
      "metadata": {
        "id": "FzMOyXAj7jey"
      },
      "id": "FzMOyXAj7jey"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_grid(num_grid_pts=256, n_dim=2):\n",
        "    \"\"\"\n",
        "    Create an n-dimensional grid of points as a NumPy array in a memory-efficient way.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_grid_pts : int, optional\n",
        "        The number of grid points along each dimension (default is 256).\n",
        "    n_dim : int, optional\n",
        "        The number of dimensions (default is 2).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    grid : np.ndarray\n",
        "        n-dimensional grid points as a NumPy array.\n",
        "    axis_points : list of np.ndarray\n",
        "        List of 1D arrays of points for every dimension.\n",
        "    \"\"\"\n",
        "    # Form 1D arrays for every dimension\n",
        "    axis_points = [np.linspace(0, np.pi, num_grid_pts) for _ in range(n_dim)]\n",
        "\n",
        "    # Generate a meshgrid up to n_dim\n",
        "    grids = np.meshgrid(*axis_points, indexing='ij', sparse=False)\n",
        "\n",
        "    return grids, axis_points"
      ],
      "metadata": {
        "id": "GyMrRGdE7lB2"
      },
      "id": "GyMrRGdE7lB2",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Training Data"
      ],
      "metadata": {
        "id": "B0dvfxBOmPlR"
      },
      "id": "B0dvfxBOmPlR"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_data(N_u, N_f, lb, ub, num_grid_pts, X, Y):\n",
        "    \"\"\"\n",
        "    Prepare boundary condition data and collocation points for training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_u : int\n",
        "        Number of boundary condition points to select.\n",
        "    N_f : int\n",
        "        Number of collocation points for the physics-informed model.\n",
        "    lb : np.Tensor\n",
        "        Lower bound of the domain.\n",
        "    ub : np.Tensor\n",
        "        Upper bound of the domain.\n",
        "    num_grid_pts : int\n",
        "        Number of grid points.\n",
        "    X : np.Tensor\n",
        "        X grid of points.\n",
        "    Y : np.Tensor\n",
        "        Y grid of points.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_f_train : np.Tensor\n",
        "        Combined collocation points and boundary points as training data.\n",
        "    X_u_train : np.Tensor\n",
        "        Selected boundary condition points.\n",
        "    u_train : np.Tensor\n",
        "        Corresponding boundary condition values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract boundary points and values from all four edges\n",
        "    leftedge_x = np.hstack((X[:, 0][:, None], Y[:, 0][:, None]))\n",
        "    leftedge_u = np.zeros((num_grid_pts, 1))\n",
        "\n",
        "    rightedge_x = np.hstack((X[:, -1][:, None], Y[:, -1][:, None]))\n",
        "    rightedge_u = np.zeros((num_grid_pts, 1))\n",
        "\n",
        "    topedge_x = np.hstack((X[0, :][:, None], Y[0, :][:, None]))\n",
        "    topedge_u = np.zeros((num_grid_pts, 1))\n",
        "\n",
        "    bottomedge_x = np.hstack((X[-1, :][:, None], Y[-1, :][:, None]))\n",
        "    bottomedge_u = np.zeros((num_grid_pts, 1))\n",
        "\n",
        "    # Combine all edge points\n",
        "    all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x])\n",
        "    all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u])\n",
        "\n",
        "    # Randomly select N_u points from boundary\n",
        "    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False)\n",
        "\n",
        "    # Select the corresponding training points and u values\n",
        "    X_u_train = all_X_u_train[idx[0:N_u], :]  # Boundary points (x, t)\n",
        "    u_train = all_u_train[idx[0:N_u], :]      # Corresponding u values\n",
        "\n",
        "    # Generate N_f collocation points using Latin Hypercube Sampling\n",
        "    X_f = lb + (ub - lb) * lhs(2, N_f)  # Generates points in the domain [lb, ub]\n",
        "\n",
        "    # Combine collocation points with boundary points\n",
        "    X_f_train = np.vstack((X_f, X_u_train))\n",
        "\n",
        "    return X_f_train, X_u_train, u_train"
      ],
      "metadata": {
        "id": "FXsGZVhYmS1t"
      },
      "id": "FXsGZVhYmS1t",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Test Data"
      ],
      "metadata": {
        "id": "TLSegRLP7dT1"
      },
      "id": "TLSegRLP7dT1"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_test_data(X, Y):\n",
        "    \"\"\"\n",
        "    Prepare test data by flattening the 2D grids and stacking them column-wise.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.ndarray\n",
        "        2D grid points in the x-dimension as a NumPy array.\n",
        "    Y : np.ndarray\n",
        "        2D grid points in the y-dimension as a NumPy array.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_u_test : np.ndarray\n",
        "        Test data prepared by stacking the flattened x and y grids.\n",
        "    lb : np.ndarray\n",
        "        Lower bound for the domain (boundary conditions).\n",
        "    ub : np.ndarray\n",
        "        Upper bound for the domain (boundary conditions).\n",
        "    \"\"\"\n",
        "    # Flatten the grids and stack them into a 2D array\n",
        "    X_u_test = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
        "\n",
        "    # Domain bounds as NumPy arrays\n",
        "    lb = np.array([0, 0], dtype=np.float32)\n",
        "    ub = np.array([np.pi, np.pi], dtype=np.float32)\n",
        "\n",
        "    return X_u_test, lb, ub"
      ],
      "metadata": {
        "id": "OK_8iiyU7gLU"
      },
      "id": "OK_8iiyU7gLU",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training PINN Mixed Precision"
      ],
      "metadata": {
        "id": "oRFVmKAiCuYW"
      },
      "id": "oRFVmKAiCuYW"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pinn_mixed_precision(model, optimizer, scheduler, x_bc, y_bc, x_to_train_f, epochs):\n",
        "    \"\"\"\n",
        "    Training loop for the PINN model with mixed precision.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : PINN\n",
        "        PINN model to be trained.\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        Optimizer for training.\n",
        "    scheduler : torch.optim.lr_scheduler\n",
        "        Learning rate scheduler.\n",
        "    x_bc : torch.Tensor\n",
        "        Boundary condition input data.\n",
        "    y_bc : torch.Tensor\n",
        "        Boundary condition output data.\n",
        "    x_to_train_f : torch.Tensor\n",
        "        Input points for PDE training.\n",
        "    epochs : int\n",
        "        Number of training epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    scaler = torch.amp.GradScaler('cuda')  # Mixed precision training\n",
        "    energy_progress = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):  # Use mixed precision for forward pass\n",
        "            loss = model.loss(x_bc, y_bc, x_to_train_f, current_epoch=epoch)\n",
        "\n",
        "        # Make loss a scalar\n",
        "        #loss = loss.mean() #  average of all individual losses\n",
        "        loss = loss.sum() # total sum of all losses.\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch: {epoch}/{epochs}, '\n",
        "                  f'Loss: {loss.item()}, '\n",
        "                  f'hbar: {model.hbar}, '\n",
        "                  f'm: {model.m.real}, '\n",
        "                  f'g: {model.g.real}, '\n",
        "                  f'Adaptive BC: {model.adaptive_bc_scale.item()}')\n",
        "\n",
        "        # Get the lowest energy ground state after each epoch\n",
        "        u, energy = model.get_ground_state(x_to_train_f)\n",
        "        energy_progress.append(energy)\n",
        "\n",
        "    # Plot energy progress\n",
        "    plot_energy_progress(energy_progress)"
      ],
      "metadata": {
        "id": "QdFiIjnbCwPQ"
      },
      "id": "QdFiIjnbCwPQ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training PINN Hybrid"
      ],
      "metadata": {
        "id": "LrS3t0PRmZbU"
      },
      "id": "LrS3t0PRmZbU"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pinn_hybrid(model, adam_optimizer, lbfgs_optimizer, scheduler, x_bc, y_bc, x_to_train_f, epochs_adam, epochs_lbfgs):\n",
        "    \"\"\"\n",
        "    Hybrid training loop for the PINN model using Adam with mixed precision followed by LBFGS. Plots training error.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : PINN\n",
        "        The PINN model to be trained.\n",
        "    adam_optimizer : torch.optim.Optimizer\n",
        "        Adam optimizer for initial training.\n",
        "    lbfgs_optimizer : torch.optim.Optimizer\n",
        "        LBFGS optimizer for fine-tuning.\n",
        "    scheduler : torch.optim.lr_scheduler\n",
        "        Learning rate scheduler.\n",
        "    x_bc : torch.Tensor\n",
        "        Boundary condition input data.\n",
        "    y_bc : torch.Tensor\n",
        "        Boundary condition output data.\n",
        "    x_to_train_f : torch.Tensor\n",
        "        Input points for PDE training.\n",
        "    epochs_adam : int\n",
        "        Number of epochs for Adam optimization.\n",
        "    epochs_lbfgs : int\n",
        "        Number of epochs for LBFGS optimization.\n",
        "    \"\"\"\n",
        "\n",
        "    scaler = torch.amp.GradScaler()  # Mixed precision scaler\n",
        "\n",
        "    # Initialize lists to track progress\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_metrics = []  # Could be used for errors\n",
        "    steps = []\n",
        "\n",
        "    # Ensure requires_grad=True for input tensors\n",
        "    x_bc = x_bc.clone().detach().requires_grad_(True)\n",
        "    y_bc = y_bc.clone().detach().requires_grad_(True)\n",
        "    x_to_train_f = x_to_train_f.clone().detach().requires_grad_(True)\n",
        "\n",
        "    # Adam optimization phase with mixed precision\n",
        "    for epoch in range(epochs_adam):\n",
        "        model.train()\n",
        "        adam_optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            loss = model.loss(x_bc, y_bc, x_to_train_f, current_epoch=epoch)\n",
        "\n",
        "        loss = loss.mean()  # mean of all losses\n",
        "\n",
        "        # Gradient clipping - Prevents exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(adam_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "\n",
        "            # Append training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Evaluation on test data\n",
        "            #model.eval()\n",
        "            #with torch.no_grad():\n",
        "                #error_vec, u_pred = model.test(X_u_test_tensor)\n",
        "                #test_loss = torch.mean((torch.tensor(u_pred) - X_u_test_tensor) ** 2).item()\n",
        "                #test_error = error_vec.item()\n",
        "\n",
        "            # Append test loss and error\n",
        "            test_losses.append(0)\n",
        "            test_metrics.append(0)\n",
        "            steps.append(epoch)\n",
        "\n",
        "            # Print out epoch and losses\n",
        "            print(f'Epoch {epoch}/{epochs_adam}, '\n",
        "                  f'Train Loss: {loss.item():.8f}')\n",
        "\n",
        "    # LBFGS optimization phase (full precision)\n",
        "    def closure():\n",
        "        lbfgs_optimizer.zero_grad()\n",
        "        loss = model.loss(x_bc, y_bc, x_to_train_f, current_epoch=epoch)\n",
        "\n",
        "        loss = loss.sum()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    for _ in range(epochs_lbfgs):\n",
        "        lbfgs_optimizer.step(closure)\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # Plot the training progress after training is done\n",
        "    plot_training_progress(train_losses, test_losses, test_metrics, steps)"
      ],
      "metadata": {
        "id": "nN2zcRyjmdz8"
      },
      "id": "nN2zcRyjmdz8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bOjuHdzAhib-",
      "metadata": {
        "id": "bOjuHdzAhib-"
      },
      "source": [
        "# *Solution Plot*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5fRTCiyL-Z7q",
      "metadata": {
        "id": "5fRTCiyL-Z7q"
      },
      "outputs": [],
      "source": [
        "def plot_solution(X_test, u_pred):\n",
        "    \"\"\"\n",
        "    Plots the predicted solution u_pred for the Gross-Pitaevskii equation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_test : np.ndarray\n",
        "        Test data (2D grid points) used for predictions.\n",
        "    u_pred : np.ndarray\n",
        "        Predicted solution u_pred from the neural network.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Reshape X_test to 2D arrays for plotting\n",
        "    X = X_test[:, 0].reshape((num_grid_pts, num_grid_pts))\n",
        "    Y = X_test[:, 1].reshape((num_grid_pts, num_grid_pts))\n",
        "\n",
        "    # Plot the predicted solution as a contour plot\n",
        "    #plt.contourf(X, Y, u_pred, levels=50, cmap='viridis')\n",
        "    plt.pcolor(X, Y, u_pred, cmap='viridis')\n",
        "    plt.colorbar(label='u_pred')\n",
        "    plt.title('Predicted Solution $u_{pred}$ for Gross-Pitaevskii Equation')\n",
        "    plt.xlabel('$x$')\n",
        "    plt.ylabel('$y$')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Energy Progress"
      ],
      "metadata": {
        "id": "r37iJCmhmi87"
      },
      "id": "r37iJCmhmi87"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_energy_progress(energy_list):\n",
        "    \"\"\"\n",
        "    Plots the energy progress over training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    energy_list : list\n",
        "        List of energy values recorded during training.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(energy_list, label='Lowest Energy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Energy')\n",
        "    plt.title('Lowest Energy Ground State During Training')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jUQf-bBAmktF"
      },
      "id": "jUQf-bBAmktF",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Training Progress"
      ],
      "metadata": {
        "id": "1PJx32g5iRlM"
      },
      "id": "1PJx32g5iRlM"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_progress(train_losses, test_losses, test_metrics, steps):\n",
        "    \"\"\"\n",
        "    Plot the training and test losses, along with the test metric, over the course of training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_losses : list\n",
        "        List of training loss values recorded at each training step.\n",
        "    test_losses : list\n",
        "        List of test loss values recorded at each evaluation step.\n",
        "    test_metrics : list\n",
        "        List of test metrics (such as error or accuracy) recorded during training.\n",
        "    steps : list\n",
        "        List of step numbers corresponding to the recorded losses and metrics.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.plot(steps, train_losses, label='Train Loss', color='blue', linestyle='-', marker='o')\n",
        "\n",
        "    # Plot test loss\n",
        "    plt.plot(steps, test_losses, label='Test Loss', color='red', linestyle='--', marker='x')\n",
        "\n",
        "    # Plot test error (or other metrics)\n",
        "    plt.plot(steps, test_metrics, label='Test Error', color='green', linestyle='-.', marker='s')\n",
        "\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Loss/Error')\n",
        "    plt.legend()\n",
        "    plt.title('Training Progress')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.savefig('training_progress_gpe_2.png')"
      ],
      "metadata": {
        "id": "BnxgWpg8iUOm"
      },
      "id": "BnxgWpg8iUOm",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot PDE Residual"
      ],
      "metadata": {
        "id": "JfbZ4XYP8CGw"
      },
      "id": "JfbZ4XYP8CGw"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pde_residual(model, X_test):\n",
        "    \"\"\"\n",
        "    Plots the residual of the PDE, indicating how well the predicted solution satisfies the GPE equation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : PINN\n",
        "        Trained PINN model.\n",
        "    X_test : torch.Tensor\n",
        "        Input test points for computing the residual.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure X_test requires gradients\n",
        "    X_test.requires_grad_(True)\n",
        "\n",
        "    # Perform forward pass to get predicted solution (without torch.no_grad())\n",
        "    u_pred = model(X_test)\n",
        "\n",
        "    # Now compute pde_loss with gradients\n",
        "    _, pde_residual = model.pde_loss(X_test, u_pred)\n",
        "\n",
        "    # Convert tensors to numpy arrays\n",
        "    X_test = X_test.cpu().detach().numpy()\n",
        "    pde_residual = pde_residual.cpu().detach().numpy()\n",
        "\n",
        "    # Plot residual\n",
        "    plt.contour(X_test[:, 0], X_test[:, 1], pde_residual, cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.title('PDE Residual')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MWZHfvIz8D82"
      },
      "id": "MWZHfvIz8D82",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7RmggB2e-Z7r",
      "metadata": {
        "id": "7RmggB2e-Z7r"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify number of grid points and number of dimensions\n",
        "num_grid_pts = 32\n",
        "nDim = 2\n",
        "\n",
        "# Prepare test data\n",
        "grids, axis_points = create_grid(num_grid_pts=num_grid_pts, n_dim=nDim)\n",
        "X, Y = grids[0], grids[1]\n",
        "x_1, x_2 = axis_points[0], axis_points[1]\n",
        "X_u_test, lb, ub = prepare_test_data(X, Y)\n",
        "\n",
        "N_u = 100  # Number of boundary points\n",
        "N_f = 1000  # Number of collocation points\n",
        "X_f_train_np_array, X_u_train_np_array, u_train_np_array = prepare_training_data(N_u, N_f, lb, ub, num_grid_pts, X, Y)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors and move to GPU (if available)\n",
        "X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)  # Collocation points\n",
        "X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)  # Boundary condition points\n",
        "u_train = torch.from_numpy(u_train_np_array).float().to(device)  # Boundary condition values\n",
        "X_u_test_tensor = torch.from_numpy(X_u_test).float().to(device)  # Test data for boundary conditions\n",
        "f_hat = torch.zeros(X_f_train.shape[0], 1).to(device)  # Zero tensor for the GPE equation residual\n",
        "\n",
        "# Model parameters\n",
        "layers = [2, 256, 256, 256, 1]  # Neural network layers\n",
        "epochs_adam = 1000\n",
        "epochs_lbfgs = 500\n",
        "\n",
        "# Initialize the model\n",
        "model = PINN(layers, ub=ub, lb=lb).to(device)\n",
        "\n",
        "# Print the neural network architecture\n",
        "print(model)\n",
        "\n",
        "# Optimizers and scheduler\n",
        "adam_optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-6,\n",
        "                            amsgrad=False)\n",
        "lbfgs_optimizer = optim.LBFGS(model.parameters(), max_iter=500, tolerance_grad=1e-5, tolerance_change=1e-9,\n",
        "                              history_size=100)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(adam_optimizer, patience=200, factor=0.5, verbose=True)\n",
        "\n",
        "# Train the model using the hybrid approach\n",
        "train_pinn_hybrid(model, adam_optimizer, lbfgs_optimizer, scheduler, X_u_train, u_train, X_f_train, epochs_adam,epochs_lbfgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omooYBDom6IJ",
        "outputId": "2534f071-a06f-4bff-e4f2-dbde33bf8b3d"
      },
      "id": "omooYBDom6IJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINN(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=2, out_features=256, bias=True)\n",
            "    (1): SineActivation()\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): SineActivation()\n",
            "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (5): SineActivation()\n",
            "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            "  (activation): SineActivation()\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Train Loss: 2899.16625977\n",
            "Epoch 100/1000, Train Loss: 10.87106323\n",
            "Epoch 200/1000, Train Loss: 5.28349018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test accuracy\n",
        "error_vec, u_pred = model.test(X_u_test_tensor)\n",
        "print(f'Test Error: {error_vec:.5f}')"
      ],
      "metadata": {
        "id": "l6WN5CgFhXQ4"
      },
      "id": "l6WN5CgFhXQ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot PDE Residual\n",
        "plot_pde_residual(model, X_u_test_tensor)"
      ],
      "metadata": {
        "id": "pnrTEGdmnIYK"
      },
      "id": "pnrTEGdmnIYK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot solution\n",
        "plot_solution(X_u_test_tensor.detach().cpu().numpy(), u_pred)"
      ],
      "metadata": {
        "id": "I4kiNWZxnLO8"
      },
      "id": "I4kiNWZxnLO8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}