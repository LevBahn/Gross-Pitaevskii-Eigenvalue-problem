{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3nYdXyrr-Z7h",
      "metadata": {
        "id": "3nYdXyrr-Z7h"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "nWzkEvVJ-4Qg",
      "metadata": {
        "id": "nWzkEvVJ-4Qg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "BMnx60Sj-Z7j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnx60Sj-Z7j",
        "outputId": "0875aca7-d867-4923-88e6-e0430c414949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as autograd         # computation graph\n",
        "from torch import Tensor                  # tensor node in the computation graph\n",
        "import torch.nn as nn                     # neural networks\n",
        "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
        "import time\n",
        "from pyDOE import lhs         #Latin Hypercube Sampling\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker\n",
        "\n",
        "#Set default dtype to float32\n",
        "torch.set_default_dtype(torch.float)\n",
        "\n",
        "#PyTorch random number generator\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Random number generators in other libraries\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)\n",
        "\n",
        "if device == 'cuda': print(torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R2aJDgIn-Z7l",
      "metadata": {
        "id": "R2aJDgIn-Z7l"
      },
      "source": [
        "# Data Preparation - Helper Functions\n",
        "\n",
        "We create several helper functions for the test data to compare against the solution produced by the PINN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "Cbwn3TXA-Z7m",
      "metadata": {
        "id": "Cbwn3TXA-Z7m"
      },
      "outputs": [],
      "source": [
        "def create_grid(num_grid_pts=256):\n",
        "    \"\"\"\n",
        "    Create a 2D grid of points and return them as NumPy arrays along with the\n",
        "    1D arrays for each axis.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_grid_pts : int, optional\n",
        "        The number of grid points along each dimension (default is 256).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : np.ndarray\n",
        "        2D grid points in the x-dimension as a NumPy array.\n",
        "    Y : np.ndarray\n",
        "        2D grid points in the y-dimension as a NumPy array.\n",
        "    x_1 : np.ndarray\n",
        "        1D array of points in the x-dimension.\n",
        "    x_2 : np.ndarray\n",
        "        1D array of points in the y-dimension.\n",
        "    \"\"\"\n",
        "    x_1 = np.linspace(0, np.pi, num_grid_pts)\n",
        "    x_2 = np.linspace(0, np.pi, num_grid_pts)\n",
        "    X, Y = np.meshgrid(x_1, x_2)\n",
        "\n",
        "    return X, Y, x_1, x_2\n",
        "\n",
        "\n",
        "def prepare_test_data(X, Y):\n",
        "    \"\"\"\n",
        "    Prepare test data by flattening the 2D grids and stacking them column-wise.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.ndarray\n",
        "        2D grid points in the x-dimension as a NumPy array.\n",
        "    Y : np.ndarray\n",
        "        2D grid points in the y-dimension as a NumPy array.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_u_test : np.ndarray\n",
        "        Test data prepared by stacking the flattened x and y grids.\n",
        "    lb : np.ndarray\n",
        "        Lower bound for the domain (boundary conditions).\n",
        "    ub : np.ndarray\n",
        "        Upper bound for the domain (boundary conditions).\n",
        "    \"\"\"\n",
        "    # Flatten the grids and stack them into a 2D array\n",
        "    X_u_test = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
        "\n",
        "    # Domain bounds as NumPy arrays\n",
        "    lb = np.array([0, 0], dtype=np.float32)\n",
        "    ub = np.array([np.pi, np.pi], dtype=np.float32)\n",
        "\n",
        "    return X_u_test, lb, ub\n",
        "\n",
        "\n",
        "def prepare_training_data(lb, ub, num_points=1000):\n",
        "    \"\"\"\n",
        "    Prepare training data by generating random points within the domain\n",
        "    and computing the ground truth based on the analytical solution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lb : np.ndarray\n",
        "        Lower bound for the domain (boundary conditions).\n",
        "    ub : np.ndarray\n",
        "        Upper bound for the domain (boundary conditions).\n",
        "    num_points : int, optional\n",
        "        The number of random points to generate for training (default is 1000).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train : np.ndarray\n",
        "        Randomly generated training points within the domain.\n",
        "    u_train : np.ndarray\n",
        "        Ground truth training values based on the analytical solution.\n",
        "    \"\"\"\n",
        "    # Generate random points within the domain as training data\n",
        "    X_train = lb + (ub - lb) * np.random.rand(num_points, 2)\n",
        "\n",
        "    # Placeholder for actual training labels using a simple analytical solution\n",
        "    u_train = np.sin(X_train[:, 0]) * np.sin(X_train[:, 1])\n",
        "\n",
        "    return X_train, u_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pECfukjT-Z7m",
      "metadata": {
        "id": "pECfukjT-Z7m"
      },
      "source": [
        "# Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "I8iAgH6D-Z7m",
      "metadata": {
        "id": "I8iAgH6D-Z7m"
      },
      "outputs": [],
      "source": [
        "def prepare_training_data(N_u, N_f, lb, ub, usol, X, Y):\n",
        "    \"\"\"\n",
        "    Prepare boundary condition data and collocation points for training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_u : int\n",
        "        Number of boundary condition points to select.\n",
        "    N_f : int\n",
        "        Number of collocation points for the physics-informed model.\n",
        "    lb : np.Tensor\n",
        "        Lower bound of the domain.\n",
        "    ub : np.Tensor\n",
        "        Upper bound of the domain.\n",
        "    usol : np.Tensor\n",
        "        Analytical solution of the PDE.\n",
        "    X : np.Tensor\n",
        "        X grid of points.\n",
        "    Y : np.Tensor\n",
        "        Y grid of points.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_f_train : np.Tensor\n",
        "        Combined collocation points and boundary points as training data.\n",
        "    X_u_train : np.Tensor\n",
        "        Selected boundary condition points.\n",
        "    u_train : np.Tensor\n",
        "        Corresponding boundary condition values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract boundary points and values from all four edges\n",
        "    leftedge_x = np.hstack((X[:, 0][:, None], Y[:, 0][:, None]))\n",
        "    leftedge_u = usol[:, 0][:, None]\n",
        "\n",
        "    rightedge_x = np.hstack((X[:, -1][:, None], Y[:, -1][:, None]))\n",
        "    rightedge_u = usol[:, -1][:, None]\n",
        "\n",
        "    topedge_x = np.hstack((X[0, :][:, None], Y[0, :][:, None]))\n",
        "    topedge_u = usol[0, :][:, None]\n",
        "\n",
        "    bottomedge_x = np.hstack((X[-1, :][:, None], Y[-1, :][:, None]))\n",
        "    bottomedge_u = usol[-1, :][:, None]\n",
        "\n",
        "    # Combine all edge points\n",
        "    all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x])\n",
        "    all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u])\n",
        "\n",
        "    # Randomly select N_u points from boundary\n",
        "    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False)\n",
        "\n",
        "    # Select the corresponding training points and u values\n",
        "    X_u_train = all_X_u_train[idx[0:N_u], :]  # Boundary points (x, t)\n",
        "    u_train = all_u_train[idx[0:N_u], :]      # Corresponding u values\n",
        "\n",
        "    # Generate N_f collocation points using Latin Hypercube Sampling\n",
        "    X_f = lb + (ub - lb) * lhs(2, N_f)  # Generates points in the domain [lb, ub]\n",
        "\n",
        "    # Combine collocation points with boundary points\n",
        "    X_f_train = np.vstack((X_f, X_u_train))\n",
        "\n",
        "    return X_f_train, X_u_train, u_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hglIb1ul-Z7n",
      "metadata": {
        "id": "hglIb1ul-Z7n"
      },
      "source": [
        "# Physics Informed Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "NFVJh4jB-Z7o",
      "metadata": {
        "id": "NFVJh4jB-Z7o"
      },
      "outputs": [],
      "source": [
        "class SequentialModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom sequential neural network model for solving boundary condition (BC) and\n",
        "    partial differential equation (PDE) loss functions using PyTorch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : list\n",
        "        A list defining the number of nodes in each layer of the network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        Initializes the SequentialModel with the specified layers, activation function,\n",
        "        loss function, and weight initialization.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layers : list\n",
        "            A list of integers where each element defines the number of neurons\n",
        "            in the respective layer.\n",
        "        \"\"\"\n",
        "        super().__init__()  # Call the parent class (nn.Module) initializer\n",
        "\n",
        "        # LeakyReLU activation function\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "        # Mean squared error (MSE) loss function\n",
        "        self.loss_function = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # L1 loss function\n",
        "        self.l1loss_function = nn.L1Loss(reduction='mean')\n",
        "\n",
        "        # Initialize the network as a list of linear layers\n",
        "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
        "\n",
        "        # Xavier normal initialization for weights and setting biases to zero\n",
        "        for i in range(len(layers) - 1):\n",
        "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
        "            nn.init.zeros_(self.linears[i].bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network. Scales the input features and passes them through\n",
        "        the layers of the model, applying the activation function after each layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor or numpy array\n",
        "            Input tensor or array to be processed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output of the model.\n",
        "        \"\"\"\n",
        "        # Convert numpy array to tensor if needed\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.from_numpy(x)\n",
        "\n",
        "        # Convert lower and upper bounds to tensors\n",
        "        u_b = torch.from_numpy(ub).float().to(device)\n",
        "        l_b = torch.from_numpy(lb).float().to(device)\n",
        "\n",
        "        # Feature scaling\n",
        "        x = (x - l_b) / (u_b - l_b)\n",
        "\n",
        "        # Convert input to float\n",
        "        a = x.float()\n",
        "\n",
        "        # Pass through each linear layer with activation\n",
        "        for i in range(len(self.linears) - 1):\n",
        "            z = self.linears[i](a)\n",
        "            a = self.activation(z)\n",
        "\n",
        "        # Final layer without activation\n",
        "        a = self.linears[-1](a)\n",
        "\n",
        "        return a\n",
        "\n",
        "\n",
        "    def loss_regu(self, x, y=None):\n",
        "            \"\"\"\n",
        "            Computes the L1 regularization loss and optionally includes a loss\n",
        "            between model predictions and ground truth.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            x : torch.Tensor\n",
        "                Input tensor.\n",
        "            y : torch.Tensor, optional\n",
        "                Ground truth tensor, if you want to compute an additional loss with predictions.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            torch.Tensor\n",
        "                The total loss including L1 regularization and optionally prediction error.\n",
        "            \"\"\"\n",
        "            # Compute L1 regularization loss\n",
        "            l1_loss = 0.0\n",
        "            for param in self.parameters():\n",
        "                l1_loss += torch.sum(torch.abs(param))\n",
        "            l1_loss = 1e-8 * l1_loss\n",
        "\n",
        "            # If y is provided, compute an additional loss between model predictions and ground truth\n",
        "            if y is not None:\n",
        "                predictions = self.forward(x)\n",
        "                additional_loss = self.loss_function(predictions, y)\n",
        "                return l1_loss + additional_loss\n",
        "\n",
        "            return l1_loss\n",
        "\n",
        "    def riesz_loss(self, predictions, inputs):\n",
        "        \"\"\"\n",
        "        Compute the L^2 energy of the Riesz functional.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : torch.Tensor\n",
        "            The predicted outputs from the model.\n",
        "        inputs : torch.Tensor\n",
        "            The input tensor that the model was trained on.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The computed L^2 loss for the Riesz functional.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute the gradients (derivatives) of the predictions with respect to the inputs\n",
        "        gradients = torch.autograd.grad(outputs=predictions, inputs=inputs,\n",
        "                                        grad_outputs=torch.ones_like(predictions),\n",
        "                                        create_graph=True)[0]\n",
        "\n",
        "        # Compute the L^2 norm of the gradients (Riesz functional energy)\n",
        "        riesz_energy = torch.sum(gradients ** 2)\n",
        "\n",
        "        # Return the mean of the Riesz energy as the loss\n",
        "        return torch.mean(riesz_energy)\n",
        "\n",
        "    def loss_BC(self, x, y):\n",
        "        \"\"\"\n",
        "        Computes the boundary condition (BC) loss using MSE between the model's predictions\n",
        "        and ground truth.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor.\n",
        "        y : torch.Tensor\n",
        "            Ground truth tensor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Loss value for the boundary condition.\n",
        "        \"\"\"\n",
        "        return self.loss_function(self.forward(x), y)\n",
        "\n",
        "    def loss_PDE(self, x_to_train_f, k):\n",
        "        \"\"\"\n",
        "        Computes the partial differential equation (PDE) loss using automatic differentiation\n",
        "        to calculate the second-order derivatives.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_to_train_f : torch.Tensor\n",
        "            Input tensor for training PDE.\n",
        "        k : int\n",
        "            Wave number of the Helmholtz equation.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Loss value for the PDE.\n",
        "        \"\"\"\n",
        "        x_1_f = x_to_train_f[:, [0]]\n",
        "        x_2_f = x_to_train_f[:, [1]]\n",
        "\n",
        "        g = x_to_train_f.clone()\n",
        "        g.requires_grad = True\n",
        "\n",
        "        # Forward pass through the network\n",
        "        u = self.forward(g)\n",
        "\n",
        "        # Compute first-order derivatives\n",
        "        u_x = autograd.grad(u, g, torch.ones([x_to_train_f.shape[0], 1]).to(device),\n",
        "                            retain_graph=True, create_graph=True)[0]\n",
        "\n",
        "        # Compute second-order derivatives\n",
        "        u_xx = autograd.grad(u_x, g, torch.ones(x_to_train_f.shape).to(device),\n",
        "                             create_graph=True)[0]\n",
        "        u_xx_1 = u_xx[:, [0]]\n",
        "        u_xx_2 = u_xx[:, [1]]\n",
        "\n",
        "        # Define the PDE residual\n",
        "        q = k**2 * torch.sin(k * x_1_f) * torch.sin(k * x_2_f)\n",
        "        f = u_xx_1 + u_xx_2 + k**2 * u - q\n",
        "\n",
        "        # PDE loss\n",
        "        return self.loss_function(f, f_hat)\n",
        "\n",
        "    def loss(self, x, y, x_to_train_f, k):\n",
        "        \"\"\"\n",
        "        Computes the total loss by combining boundary condition (BC) loss and\n",
        "        partial differential equation (PDE) loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor for BC.\n",
        "        y : torch.Tensor\n",
        "            Ground truth tensor for BC.\n",
        "        x_to_train_f : torch.Tensor\n",
        "            Input tensor for PDE.\n",
        "        k : int\n",
        "            Wave number of the Helmholtz equation.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Total loss value.\n",
        "        \"\"\"\n",
        "        loss_r = self.loss_regu(x, y)\n",
        "        loss_u = self.loss_BC(x, y)\n",
        "        loss_f = self.loss_PDE(x_to_train_f, k)\n",
        "\n",
        "        # Ensure input tensor requires gradient\n",
        "        x_to_train_f.requires_grad = True\n",
        "\n",
        "        # Add the Riesz loss to the total loss\n",
        "        predictions = self.forward(x_to_train_f)\n",
        "        loss_k = self.riesz_loss(predictions, x_to_train_f)\n",
        "\n",
        "        return loss_u + loss_f + loss_k\n",
        "\n",
        "    def closure(self):\n",
        "        \"\"\"\n",
        "        Defines the closure function for the optimizer to minimize the loss. This function is\n",
        "        used during the optimization process.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The computed loss value.\n",
        "        \"\"\"\n",
        "        adam_optimizer.zero_grad()\n",
        "\n",
        "        # Compute total loss\n",
        "        loss_val = self.loss(X_u_train, u_train, X_f_train)\n",
        "\n",
        "        # Test the model and get error metrics\n",
        "        error_vec, _ = self.test()\n",
        "\n",
        "        print(loss, error_vec)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss_val.backward()\n",
        "\n",
        "        return loss_val\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"\n",
        "        Tests the model on the test data and computes the relative L2 norm of the error.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        error_vec : torch.Tensor\n",
        "            The relative L2 norm of the error.\n",
        "        u_pred : numpy.ndarray\n",
        "            The predicted output reshaped as a 2D array.\n",
        "        \"\"\"\n",
        "        # Model prediction on test data\n",
        "        u_pred = self.forward(X_u_test_tensor)\n",
        "\n",
        "        # Compute relative L2 norm of the error\n",
        "        error_vec = torch.linalg.norm((u - u_pred), 2) / torch.linalg.norm(u, 2)\n",
        "\n",
        "        # Reshape the predicted output to a 2D array\n",
        "        u_pred = np.reshape(u_pred.cpu().detach().numpy(), (num_grid_pts, num_grid_pts), order='F')\n",
        "\n",
        "        return error_vec, u_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bOjuHdzAhib-",
      "metadata": {
        "id": "bOjuHdzAhib-"
      },
      "source": [
        "# *Solution Plot*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5fRTCiyL-Z7q",
      "metadata": {
        "id": "5fRTCiyL-Z7q"
      },
      "outputs": [],
      "source": [
        "def solutionplot(u_pred, X_u_train, u_train):\n",
        "    \"\"\"\n",
        "    Plots the ground truth solution, predicted solution, and absolute error between them.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    u_pred : numpy.ndarray\n",
        "        Predicted solution values from the model.\n",
        "    X_u_train : numpy.ndarray\n",
        "        Training points used for boundary conditions (not plotted but included for context).\n",
        "    u_train : numpy.ndarray\n",
        "        Corresponding boundary condition values (not plotted but included for context).\n",
        "    \"\"\"\n",
        "\n",
        "    # Ground truth solution plot\n",
        "    fig_1 = plt.figure(1, figsize=(18, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.pcolor(x_1, x_2, usol, cmap='jet')  # Plot the ground truth solution 'usol' using a color map\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(r'$x_1$', fontsize=18)\n",
        "    plt.ylabel(r'$x_2$', fontsize=18)\n",
        "    plt.title('Ground Truth $u(x_1,x_2)$', fontsize=15)\n",
        "\n",
        "    # Predicted solution plot\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.pcolor(x_1, x_2, u_pred, cmap='jet')  # Plot the predicted solution 'u_pred' using the same color map\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(r'$x_1$', fontsize=18)\n",
        "    plt.ylabel(r'$x_2$', fontsize=18)\n",
        "    plt.title('Predicted $\\hat u(x_1,x_2)$', fontsize=15)\n",
        "\n",
        "    # Absolute error plot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.pcolor(x_1, x_2, np.abs(usol - u_pred), cmap='jet')  # Plot the absolute error between ground truth and prediction\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(r'$x_1$', fontsize=18)\n",
        "    plt.ylabel(r'$x_2$', fontsize=18)\n",
        "    plt.title(r'Absolute error $|u(x_1,x_2)- \\hat u(x_1,x_2)|$', fontsize=15)\n",
        "\n",
        "    plt.tight_layout()  # Adjust subplots to fit into the figure area cleanly\n",
        "\n",
        "    # Save the figure as a high-resolution image file\n",
        "    plt.savefig('Helmholtz_non_stiff.png', dpi=500, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nqbXpNHXoBU_",
      "metadata": {
        "id": "nqbXpNHXoBU_"
      },
      "source": [
        "# LBFGS Closure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "VnvY-M_FoD2W",
      "metadata": {
        "id": "VnvY-M_FoD2W"
      },
      "outputs": [],
      "source": [
        "def LBFGS_closure():\n",
        "    \"\"\"\n",
        "    Computes the loss and its gradients for use with the LBFGS optimizer.\n",
        "\n",
        "    This closure function is necessary for optimizers like LBFGS which require\n",
        "    multiple evaluations of the function. It performs the following:\n",
        "    - Resets gradients to zero.\n",
        "    - Calculates the loss using the physics-informed neural network (PINN) model.\n",
        "    - Backpropagates the gradients of the loss.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : torch.Tensor\n",
        "        The computed loss value.\n",
        "    \"\"\"\n",
        "    # Zero out the gradients of the optimizer before backpropagation\n",
        "    lbfgs_optimizer.zero_grad()\n",
        "\n",
        "    # Compute the loss using the physics-informed neural network (PINN)\n",
        "    loss = PINN.loss(X_u_train, u_train, X_f_train, k)\n",
        "\n",
        "    # Perform backpropagation to compute the gradients of the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Return the loss value to the optimizer\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7RmggB2e-Z7r",
      "metadata": {
        "id": "7RmggB2e-Z7r"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "l6Wiv5Zi-Z7r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "l6Wiv5Zi-Z7r",
        "outputId": "8c90e056-8f8e-4279-b66a-4170e9354863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequentialModel(\n",
            "  (activation): LeakyReLU(negative_slope=0.01)\n",
            "  (loss_function): MSELoss()\n",
            "  (l1loss_function): L1Loss()\n",
            "  (linears): ModuleList(\n",
            "    (0): Linear(in_features=2, out_features=200, bias=True)\n",
            "    (1-3): 3 x Linear(in_features=200, out_features=200, bias=True)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Adam - Iteration 0: Loss 367.16754150390625, Error 2.07586669921875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "you can only change requires_grad flags of leaf variables.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2443ccdbd45d>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Calculate the total loss (boundary condition loss + physics-informed loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_f_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Zero the gradient buffers of all parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-a045d84ad54a>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, y, x_to_train_f, k)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mloss_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_regu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mloss_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_PDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_to_train_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Ensure input tensor requires gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-a045d84ad54a>\u001b[0m in \u001b[0;36mloss_PDE\u001b[0;34m(self, x_to_train_f, k)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_to_train_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Forward pass through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables."
          ]
        }
      ],
      "source": [
        "# Create grid and prepare test data\n",
        "num_grid_pts=256\n",
        "X, Y, x_1, x_2 = create_grid(num_grid_pts=num_grid_pts)  # The default is num_grid_pts=256\n",
        "X_u_test, lb, ub = prepare_test_data(X, Y)\n",
        "\n",
        "# Analytical solution of the PDE\n",
        "k = 2*np.pi\n",
        "usol = np.sin(k * X) * np.sin(k * Y)\n",
        "\n",
        "# Flatten the solution in Fortran-like order and reshape it\n",
        "u_true = usol.flatten()[:, None]\n",
        "\n",
        "# Number of training points and collocation points\n",
        "N_u = 500  # Total number of data points for 'u', used to train the model on boundary conditions\n",
        "N_f = 10000  # Total number of collocation points for training the physics-informed part of the model\n",
        "\n",
        "X_f_train_np_array, X_u_train_np_array, u_train_np_array = prepare_training_data(N_u, N_f, lb, ub, usol, X, Y)\n",
        "\n",
        "# Convert the numpy arrays to PyTorch tensors and move them to the GPU (if available)\n",
        "X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)  # Collocation points\n",
        "X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)  # Boundary condition points\n",
        "u_train = torch.from_numpy(u_train_np_array).float().to(device)  # Boundary condition values\n",
        "X_u_test_tensor = torch.from_numpy(X_u_test).float().to(device)  # Test data for boundary conditions\n",
        "u = torch.from_numpy(u_true).float().to(device)  # True solution values (ground truth for testing)\n",
        "f_hat = torch.zeros(X_f_train.shape[0], 1).to(device)  # Zero tensor for the physics equation residual\n",
        "\n",
        "# Neural network architecture definition\n",
        "\n",
        "# Input layer with 2 nodes, 4 hidden layers with 200 nodes, and an output layer with 1 node\n",
        "layers = np.array([2, 200, 200,200, 200, 1])\n",
        "\n",
        "# Initialize the neural network model\n",
        "PINN = SequentialModel(layers)\n",
        "\n",
        "# Move the model to the GPU (if available)\n",
        "PINN.to(device)\n",
        "\n",
        "# Print a summary of the neural network architecture\n",
        "print(PINN)\n",
        "\n",
        "# Store the parameters of the neural network for optimization\n",
        "params = list(PINN.parameters())\n",
        "\n",
        "# Optimization Section\n",
        "\n",
        "# Adam Optimizer\n",
        "adam_optimizer = optim.Adam(PINN.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08,\n",
        "                          weight_decay=1e-4, amsgrad=False)\n",
        "\n",
        "# L-BFGS Optimizer (for fine-tuning)\n",
        "lbfgs_optimizer = optim.LBFGS(PINN.parameters(), max_iter=500, tolerance_grad=1e-5, tolerance_change=1e-9,\n",
        "                              history_size=100)\n",
        "\n",
        "start_time = time.time()  # Start the timer to measure training time\n",
        "\n",
        "# Number of iterations for Adam optimizer\n",
        "adam_iter = 2000\n",
        "\n",
        "# Adam training loop\n",
        "for i in range(adam_iter):\n",
        "    # Calculate the total loss (boundary condition loss + physics-informed loss)\n",
        "    loss = PINN.loss(X_u_train, u_train, X_f_train, k)\n",
        "\n",
        "    # Zero the gradient buffers of all parameters\n",
        "    adam_optimizer.zero_grad()\n",
        "\n",
        "    # Backpropagation to calculate gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model parameters using the optimizer\n",
        "    adam_optimizer.step()\n",
        "\n",
        "    # Print loss and error every 200 iterations\n",
        "    if i % (adam_iter // 10) == 0:\n",
        "        error_vec, _ = PINN.test()  # Evaluate the model on test data\n",
        "        print(f\"Adam - Iteration {i}: Loss {loss.item()}, Error {error_vec.item()}\")\n",
        "\n",
        "# L-BFGS fine-tuning and optimizer\n",
        "lbfgs_optimizer.step(LBFGS_closure)\n",
        "\n",
        "# After L-BFGS optimization\n",
        "error_vec, u_pred = PINN.test()\n",
        "print(f'L-BFGS Test Error: {error_vec.item()}')\n",
        "\n",
        "# Measure the elapsed training time\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.2f' % (elapsed))\n",
        "\n",
        "# Evaluate the model accuracy on test data\n",
        "error_vec, u_pred = PINN.test()\n",
        "print('Test Error: %.5f' % (error_vec))\n",
        "\n",
        "# Plot the ground truth, predicted solution, and error\n",
        "solutionplot(u_pred, X_u_train, u_train)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}