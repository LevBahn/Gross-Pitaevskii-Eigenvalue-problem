{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzud5_5f-EJ-"
      },
      "source": [
        "# **Re**lative **Lo**ss **B**alancing with **Ra**ndom **Lo**okbacks for Kirchhoff PDE\n",
        "This notebook implements the concepts from the [Multi-Objective Loss Balancing for Physics-Informed Deep Learning paper](https://arxiv.org/abs/2110.09813) and [Improving PINNs through Adaptive Loss Balancing medium article](https://medium.com/p/55662759e701). It showcases the gains in performance when applying Loss Balancing Schemes to PINN training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFnikg8NH6E3"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "from typing import Tuple, Callable, List, Union\n",
        "from tensorflow.experimental.numpy import isclose\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions\n",
        "We start by defining a few utility functions that will be useful later. The first, *compute_derivatives*, computes all the derivatives necessary to formulate the Kirchhoff PINN objective (it is a fourth order differential equation), while *compute_moments* is used for computing the boundary conditions.\n",
        "\n"
      ],
      "metadata": {
        "id": "_fiikxfJk_UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_derivatives(x, y, u):\n",
        "    \"\"\"\n",
        "    Computes the derivatives of `u` with respect to `x` and `y`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : tf.Tensor\n",
        "        The x-coordinate of the collocation points, of shape (batch_size, 1).\n",
        "    y : tf.Tensor\n",
        "        The y-coordinate of the collocation points, of shape (batch_size, 1).\n",
        "    u : tf.Tensor\n",
        "        The prediction made by the PINN, of shape (batch_size, 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        The derivatives of `u` with respect to `x`, `y`, `xx`, `yy`, `xy`.\n",
        "    \"\"\"\n",
        "    dudx, dudy = tf.gradients(u, [x, y])\n",
        "    dudxx = tf.gradients(dudx, x)[0]\n",
        "    dudyy = tf.gradients(dudy, y)[0]\n",
        "    dudxxx, dudxxy = tf.gradients(dudxx, [x, y])\n",
        "    dudyyy = tf.gradients(dudyy, y)[0]\n",
        "    dudxxxx = tf.gradients(dudxxx, x)[0]\n",
        "    dudxxyy = tf.gradients(dudxxy, y)[0]\n",
        "    dudyyyy = tf.gradients(dudyyy, y)[0]\n",
        "    return dudxx, dudyy, dudxxxx, dudyyyy, dudxxyy\n",
        "\n",
        "\n",
        "def compute_moments(D, nue, dudxx, dudyy):\n",
        "    \"\"\"\n",
        "    Computes the moments along the x and y axes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    D : float\n",
        "        The flexural stiffness.\n",
        "    nue : float\n",
        "        Poisson's ratio.\n",
        "    dudxx : tf.Tensor\n",
        "        The second-order derivative of `u` with respect to `x`, of shape (batch_size, 1).\n",
        "    dudyy : tf.Tensor\n",
        "        The second-order derivative of `u` with respect to `y`, of shape (batch_size, 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        The moments along the x and y axes.\n",
        "    \"\"\"\n",
        "    mx = -D * (dudxx + nue * dudyy)\n",
        "    my = -D * (nue * dudxx + dudyy)\n",
        "    return mx, my\n"
      ],
      "metadata": {
        "id": "pYiE6aZcuL4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTbTpsj5w-wY"
      },
      "source": [
        "## Define Kirchhoff PDE Class\n",
        "This class represents a plate under the [Kirchhoff plate bending PDE](https://en.wikipedia.org/wiki/Kirchhoff%E2%80%93Love_plate_theory). It provides a set of utility functions to train a Physics Informed Neural Network (PINN) [link text](https://arxiv.org/pdf/1711.10566.pdf) on the Kirchhoff PDE. The class has functions that generate training and validation data, calculate loss, and visualize the results of the PINN's predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPS = 1e-7\n",
        "\n",
        "class KirchhoffPDE:\n",
        "    \"\"\"\n",
        "    Class representing a Kirchhoff plate, providing several methods for training a Physics-Informed Neural Network.\n",
        "    \"\"\"\n",
        "    def __init__(self, p: Callable[[tf.Tensor, tf.Tensor], tf.Tensor], u_val: Callable[[tf.Tensor, tf.Tensor], tf.Tensor],\n",
        "                 T: float, nue: float, E: float, H: float, W: float):\n",
        "        \"\"\"\n",
        "        Initialize the KirchhoffPDE class.\n",
        "\n",
        "        PARAMETERS\n",
        "        ----------------\n",
        "        p : Callable[[tf.Tensor, tf.Tensor], tf.Tensor]\n",
        "            The load function, taking x and y coordinates as inputs and returning the load.\n",
        "        u_val : Callable[[tf.Tensor, tf.Tensor], tf.Tensor]\n",
        "            A function to validate the predictions.\n",
        "        T : float\n",
        "            Thickness of the plate.\n",
        "        nue : float\n",
        "            Poisson's ratio.\n",
        "        E : float\n",
        "            Young's modulus.\n",
        "        W : float\n",
        "            Width of the plate.\n",
        "        H : float\n",
        "            Height of the plate.\n",
        "        \"\"\"\n",
        "        self.p = p\n",
        "        self.u_val = u_val\n",
        "        self.T = T\n",
        "        self.nue = nue\n",
        "        self.E = E\n",
        "        self.D = (E * T**3) / (12 * (1 - nue**2)) # flexural stiffnes of the plate\n",
        "        self.H = H\n",
        "        self.W = W\n",
        "        self.num_terms = 3\n",
        "\n",
        "    def training_batch(self, batch_size_domain:int=800, batch_size_boundary:int=100) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        Generates a batch of collocation points by randomly sampling `batch_size_domain` points inside the domain\n",
        "        and `batch_size_boundary` points on each of the four boundaries.\n",
        "\n",
        "        PARAMETERS\n",
        "        --------------------\n",
        "        batch_size_domain : int\n",
        "            number of points to be sampled inside of the domain\n",
        "        batch_size_boundary : int\n",
        "            number of points to be sampled on each of the four boundaries\n",
        "        \"\"\"\n",
        "        x_in = tf.random.uniform(shape=(batch_size_domain, 1), minval=0, maxval=self.W)\n",
        "        x_b1 = tf.zeros(shape=(batch_size_boundary, 1))\n",
        "        x_b2 = tf.zeros(shape=(batch_size_boundary, 1)) + self.W\n",
        "        x_b3 = tf.random.uniform(shape=(batch_size_boundary, 1), minval=0, maxval=self.W)\n",
        "        x_b4 = tf.random.uniform(shape=(batch_size_boundary, 1), minval=0, maxval=self.W)\n",
        "        x = tf.concat([x_in, x_b1, x_b2, x_b3, x_b4], axis=0)\n",
        "\n",
        "        y_in = tf.random.uniform(shape=(batch_size_domain, 1), minval=0, maxval=self.H)\n",
        "        y_b1 = tf.random.uniform(shape=(batch_size_boundary, 1), minval=0, maxval=self.H)\n",
        "        y_b2 = tf.random.uniform(shape=(batch_size_boundary, 1), minval=0, maxval=self.H)\n",
        "        y_b3 = tf.zeros(shape=(batch_size_boundary, 1))\n",
        "        y_b4 = tf.zeros(shape=(batch_size_boundary, 1)) + self.H\n",
        "        y = tf.concat([y_in, y_b1, y_b2, y_b3, y_b4], axis=0)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def get_train_dataset(self, batch_size_domain:int=800, batch_size_boundary:int=100):\n",
        "        \"\"\"\n",
        "        Creates a tf.data.Dataset generator for training.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size_domain : int\n",
        "            number of points to be sampled inside of the domain. Default is 800.\n",
        "        batch_size_boundary : int\n",
        "            number of points to be sampled on each of the four boundaries. Default is 100.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tf.data.Dataset\n",
        "            A `tf.data.Dataset` generator for training.\n",
        "        \"\"\"\n",
        "        def generator():\n",
        "            while True:\n",
        "                xy = tf.concat(self.training_batch(batch_size_domain, batch_size_boundary), axis=-1)\n",
        "                yield xy, xy\n",
        "\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            generator,\n",
        "            output_types=(tf.float32, tf.float32),\n",
        "            output_shapes=((None, 2), (None, 2))\n",
        "        )\n",
        "\n",
        "    def validation_batch(self, grid_width:int=64, grid_height:int=64):\n",
        "        \"\"\"\n",
        "        Generates a grid of points that can easily be used to generate an image of the plate,\n",
        "        where each point is a pixel.\n",
        "\n",
        "        PARAMETERS\n",
        "        ----------\n",
        "        grid_width : int\n",
        "            width of the grid\n",
        "        grid_height : int\n",
        "            height of the grid\n",
        "        \"\"\"\n",
        "        x, y = np.mgrid[0:self.W:complex(0, grid_width), 0:self.H:complex(0, grid_height)]\n",
        "        x = tf.cast(x.reshape(grid_width * grid_height, 1), dtype=tf.float32)\n",
        "        y = tf.cast(y.reshape(grid_width * grid_height, 1), dtype=tf.float32)\n",
        "        u = self.u_val(x, y)\n",
        "        return x, y, u\n",
        "\n",
        "    def compute_loss(self, x, y, preds, eval=False):\n",
        "        \"\"\"\n",
        "        Computes the physics-informed loss for Kirchhoff's plate bending equation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : tf.Tensor of shape (batch_size, 1)\n",
        "            x coordinate of the points in the current batch\n",
        "        y : tf.Tensor of shape (batch_size, 1)\n",
        "            y coordinate of the points in the current batch\n",
        "        preds : tf.Tensor of shape (batch_size, 6)\n",
        "            predictions made by our PINN (dim 0) as well as dudxx (dim 1), dudyy (dim 2),\n",
        "            dudxxxx (dim 3), dudxxyy (dim 4), dudyyyy (dim 5)\n",
        "        \"\"\"\n",
        "\n",
        "        # governing equation loss\n",
        "        f = preds[:, 3:4] + 2 * preds[:, 4:5] + preds[:, 5:6] - self.p(x, y) / self.D\n",
        "        L_f = f**2\n",
        "\n",
        "        # determine which points are on the boundaries of the domain\n",
        "        # if a point is on either of the boundaries, its value is 1 and 0 otherwise\n",
        "        x_lower = tf.cast(isclose(x, 0.     , rtol=0., atol=EPS), dtype=tf.float32)\n",
        "        x_upper = tf.cast(isclose(x, self.W, rtol=0., atol=EPS), dtype=tf.float32)\n",
        "        y_lower = tf.cast(isclose(y, 0.     , rtol=0., atol=EPS), dtype=tf.float32)\n",
        "        y_upper = tf.cast(isclose(y, self.H, rtol=0., atol=EPS), dtype=tf.float32)\n",
        "\n",
        "        # compute 0th order boundary condition loss\n",
        "        L_b0 = ((x_lower + x_upper + y_lower + y_upper) * preds[:, :1])**2\n",
        "        # compute 2nd order boundary condition loss\n",
        "        mx, my = compute_moments(self.D, self.nue, preds[:, 1:2], preds[:, 2:3])\n",
        "        L_b2 = ((x_lower + x_upper) * mx)**2 + ((y_lower + y_upper) * my)**2\n",
        "\n",
        "        if eval:\n",
        "            L_u = (self.u_val(x, y) - preds[:, 0:1])**2\n",
        "            return L_f, L_b0, L_b2, L_u\n",
        "\n",
        "        return L_f, L_b0, L_b2\n",
        "\n",
        "    @tf.function\n",
        "    def __validation_results(self, pinn: tf.keras.Model, image_width: int = 64, image_height: int = 64):\n",
        "        \"\"\"Computes the validation results for the given model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        pinn : tf.keras.Model\n",
        "            A TensorFlow Keras model instance.\n",
        "        image_width : int\n",
        "            The width of the image (defaults to 64).\n",
        "        image_height : int\n",
        "            The height of the image (defaults to 64).\n",
        "\n",
        "        Returns:\n",
        "        u_real : tf.Tensor\n",
        "            A tensor containing the real displacement.\n",
        "        u_pred : tf.Tensor\n",
        "            A tensor containing the predicted displacement.\n",
        "        mx : tf.Tensor\n",
        "            A tensor containing the x-component of the moments.\n",
        "        my : tf.Tensor\n",
        "            A tensor containing the y-component of the moments.\n",
        "        f : tf.Tensor\n",
        "            A tensor containing the governing equation.\n",
        "        p : tf.Tensor\n",
        "            A tensor containing the pressure.\n",
        "        \"\"\"\n",
        "        x, y, u_real = self.validation_batch(image_width, image_height)\n",
        "        pred = pinn(tf.concat([x, y], axis=-1), training=False)\n",
        "        u_pred, dudxx, dudyy, dudxxxx, dudyyyy, dudxxyy = pred[:, 0:1], pred[:, 1:2], pred[:, 2:3], pred[:, 3:4], pred[:, 4:5], pred[:, 5:6]\n",
        "        mx, my = compute_moments(self.D, self.nue, dudxx, dudyy)\n",
        "        f = dudxxxx + 2 * dudxxyy + dudyyyy\n",
        "        p = self.p(x, y)\n",
        "        return u_real, u_pred, mx, my, f, p\n",
        "\n",
        "\n",
        "    def visualise(self, pinn: tf.keras.Model = None, image_width: int = 64, image_height: int = 64):\n",
        "        \"\"\"\n",
        "        If no model is provided, visualises only the load distribution on the plate.\n",
        "        Otherwise, visualizes the results of the given model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        pinn : tf.keras.Model\n",
        "            A TensorFlow Keras model instance.\n",
        "        image_width : int\n",
        "            The width of the image (defaults to 64).\n",
        "        image_height : int\n",
        "            The height of the image (defaults to 64).\n",
        "        \"\"\"\n",
        "        if pinn is None:\n",
        "            x, y, u_real = self.validation_batch(image_width, image_height)\n",
        "            load = self.p(x, y).numpy().reshape(image_width, image_height)\n",
        "            fig, axs = plt.subplots(1, 2, figsize=(8, 3.2), dpi=100)\n",
        "            self.__show_image(\n",
        "                load,\n",
        "                axis=axs[0],\n",
        "                title='Load distribution on the plate',\n",
        "                z_label='$\\\\left[\\\\frac{NM}{m^2}\\\\right]$'\n",
        "            )\n",
        "            self.__show_image(\n",
        "                u_real.numpy().reshape(image_width, image_height),\n",
        "                axis=axs[1],\n",
        "                title='Deformation',\n",
        "                z_label='[m]'\n",
        "            )\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            u_real, u_pred, mx, my, f, p = self.__validation_results(pinn, image_width, image_height)\n",
        "            u_real = u_real.numpy().reshape(image_width, image_height)\n",
        "            u_pred = u_pred.numpy().reshape(image_width, image_height)\n",
        "            mx = mx.numpy().reshape(image_width, image_height)\n",
        "            my = my.numpy().reshape(image_width, image_height)\n",
        "            f = f.numpy().reshape(image_width, image_height)\n",
        "            p = p.numpy().reshape(image_width, image_height)\n",
        "\n",
        "            fig, axs = plt.subplots(3, 2, figsize=(9.5, 12))\n",
        "            self.__show_image(u_pred, axs[0, 0], 'Predicted Displacement (m)')\n",
        "            self.__show_image((u_pred - u_real)**2, axs[0, 1], 'Squared Error Displacement')\n",
        "            self.__show_image(mx, axs[1, 0], 'Moments mx')\n",
        "            self.__show_image(my, axs[1, 1], 'Moments my')\n",
        "            self.__show_image(f, axs[2, 0], 'Governing Equation')\n",
        "            self.__show_image((f - p)**2, axs[2, 1], 'Squared Error Governing Equation')\n",
        "\n",
        "            # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "            for ax in axs.flat:\n",
        "                ax.label_outer()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    def __show_image(self, img:np.array, axis:plt.axis=None, title:str='', x_label='x [m]', y_label='y [m]', z_label=''):\n",
        "        if axis is None:\n",
        "             _, axis = plt.subplots(1, 1, figsize=(4, 3.2), dpi=100)\n",
        "        im = axis.imshow(np.rot90(img, k=3), cmap='plasma', origin='lower', aspect='auto')\n",
        "        cb = plt.colorbar(im, label=z_label, ax=axis)\n",
        "        axis.set_xticks([0, img.shape[0]-1])\n",
        "        axis.set_xticklabels([0, self.W])\n",
        "        axis.set_yticks([0, img.shape[1]-1])\n",
        "        axis.set_yticklabels([0, self.H])\n",
        "        axis.set_xlabel(x_label)\n",
        "        axis.set_ylabel(y_label)\n",
        "        axis.set_title(title)\n",
        "        return im"
      ],
      "metadata": {
        "id": "2XXSE7o_Ovxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kirchhoff PINN Loss Function"
      ],
      "metadata": {
        "id": "AjJZMkIkmkyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Default Kirchhoff PDE Loss\n",
        "This class inherits from the keras Loss class and can be used in the keras API (i.e. model.fit()) for training our Kirchhoff PINN."
      ],
      "metadata": {
        "id": "hLormYLgsULd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KirchhoffLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    Kirchhoff Loss for plate bending physics-informed neural network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    plate: KirchhoffPDE\n",
        "        The KirchhoffPDE object representing the plate bending physics.\n",
        "    name: str, optional\n",
        "        The name of the loss, by default 'ReLoBRaLoKirchhoffLoss'\n",
        "    \"\"\"\n",
        "    def __init__(self, plate:KirchhoffPDE, name='KirchhoffLoss'):\n",
        "        super().__init__(name=name)\n",
        "        self.plate = plate\n",
        "\n",
        "    def call(self, xy, preds):\n",
        "        x, y = xy[:, :1], xy[:, 1:]\n",
        "        L_f, L_b0, L_b2 = self.plate.compute_loss(x, y, preds)\n",
        "        loss = L_f + L_b0 + L_b2\n",
        "        return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "QGhpCI5kDElJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLoBRaLo Loss Balancing Objective\n",
        "This class inherits from the KirchhoffLoss class and balances the contributions towards the total loss by scaling the terms L_0, L_b0 and L_b2 according to the ReLoBRaLo loss balancing scheme.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\lambda_i^{\\textit{bal}}(t, t') = k\\cdot\\frac{\\operatorname{exp}\\left(\\frac{L_i(t)}{\\tau L_i(t')}\\right)}{\\sum_{j=1}^k \\operatorname{exp} \\left(\\frac{L_j(t)}{\\tau L_j(t')} \\right)}, \\; i \\in \\{1, \\dots, k\\}\\\\\n",
        "&\\lambda_{i}^{\\textit{hist}}(t) = \\rho\\lambda_i(t-1) + (1-\\rho)\\lambda_i^{\\textit{bal}}(t, 0))\\\\\n",
        "&\\lambda_i(t) = \\alpha\\lambda_{i}^{\\textit{hist}} + (1-\\alpha)\\lambda_i^{\\textit{bal}}(t, t-1)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is the exponential decay rate, $\\rho$ is a Bernoulli random variable and $\\mathbb{E}[\\rho]$ should be chosen close to 1. The intermediate step $\\lambda_i^{\\textit{bal}}(t, t')$ calculates scalings based on the relative improvements of each term between time steps $t'$ and $t$. The following step $\\lambda_{i}^{\\textit{hist}}(t)$ defines, whether the scalings calculated in the previous time step ($\\rho$ evaluates to 1) or the relative improvements since the beginning of training ($\\rho$ evaluates to 0) should be carried forward. Note that this concept of randomly retaining or discarding the history of scalings is what we denote as \"random lookbacks\". Finally, the scaling $\\lambda_i(t)$ for term $i$ is obtained by means of an exponential decay, where $\\alpha$ controls the weight given to past scalings versus the scalings calculated in the current time step."
      ],
      "metadata": {
        "id": "xJGqdqeim52j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLoBRaLoKirchhoffLoss(KirchhoffLoss):\n",
        "    \"\"\"\n",
        "    Class for the ReLoBRaLo Kirchhoff Loss.\n",
        "    This class extends the Kirchhoff Loss to have dynamic weighting for each term in the calculation of the loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, plate:KirchhoffPDE, alpha:float=0.999, temperature:float=1., rho:float=0.9999,\n",
        "                 name='ReLoBRaLoKirchhoffLoss'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        plate : KirchhoffPDE\n",
        "            An instance of KirchhoffPDE class containing the `compute_loss` function.\n",
        "        alpha, optional : float\n",
        "            Controls the exponential weight decay rate.\n",
        "            Value between 0 and 1. The smaller, the more stochasticity.\n",
        "            0 means no historical information is transmitted to the next iteration.\n",
        "            1 means only first calculation is retained. Defaults to 0.999.\n",
        "        temperature, optional : float\n",
        "            Softmax temperature coefficient. Controlls the \"sharpness\" of the softmax operation.\n",
        "            Defaults to 1.\n",
        "        rho, optional : float\n",
        "            Probability of the Bernoulli random variable controlling the frequency of random lookbacks.\n",
        "            Value berween 0 and 1. The smaller, the fewer lookbacks happen.\n",
        "            0 means lambdas are always calculated w.r.t. the initial loss values.\n",
        "            1 means lambdas are always calculated w.r.t. the loss values in the previous training iteration.\n",
        "            Defaults to 0.9999.\n",
        "        \"\"\"\n",
        "        super().__init__(plate, name=name)\n",
        "        self.plate = plate\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.rho = rho\n",
        "        self.call_count = tf.Variable(0, trainable=False, dtype=tf.int16)\n",
        "\n",
        "        self.lambdas = [tf.Variable(1., trainable=False) for _ in range(plate.num_terms)]\n",
        "        self.last_losses = [tf.Variable(1., trainable=False) for _ in range(plate.num_terms)]\n",
        "        self.init_losses = [tf.Variable(1., trainable=False) for _ in range(plate.num_terms)]\n",
        "\n",
        "    def call(self, xy, preds):\n",
        "        x, y = xy[:, :1], xy[:, 1:]\n",
        "        losses = [tf.reduce_mean(loss) for loss in self.plate.compute_loss(x, y, preds)]\n",
        "\n",
        "        # in first iteration (self.call_count == 0), drop lambda_hat and use init lambdas, i.e. lambda = 1\n",
        "        #   i.e. alpha = 1 and rho = 1\n",
        "        # in second iteration (self.call_count == 1), drop init lambdas and use only lambda_hat\n",
        "        #   i.e. alpha = 0 and rho = 1\n",
        "        # afterwards, default procedure (see paper)\n",
        "        #   i.e. alpha = self.alpha and rho = Bernoully random variable with p = self.rho\n",
        "        alpha = tf.cond(tf.equal(self.call_count, 0),\n",
        "                lambda: 1.,\n",
        "                lambda: tf.cond(tf.equal(self.call_count, 1),\n",
        "                                lambda: 0.,\n",
        "                                lambda: self.alpha))\n",
        "        rho = tf.cond(tf.equal(self.call_count, 0),\n",
        "              lambda: 1.,\n",
        "              lambda: tf.cond(tf.equal(self.call_count, 1),\n",
        "                              lambda: 1.,\n",
        "                              lambda: tf.cast(tf.random.uniform(shape=()) < self.rho, dtype=tf.float32)))\n",
        "\n",
        "        # compute new lambdas w.r.t. the losses in the previous iteration\n",
        "        lambdas_hat = [losses[i] / (self.last_losses[i] * self.temperature + EPS) for i in range(len(losses))]\n",
        "        lambdas_hat = tf.nn.softmax(lambdas_hat - tf.reduce_max(lambdas_hat)) * tf.cast(len(losses), dtype=tf.float32)\n",
        "\n",
        "        # compute new lambdas w.r.t. the losses in the first iteration\n",
        "        init_lambdas_hat = [losses[i] / (self.init_losses[i] * self.temperature + EPS) for i in range(len(losses))]\n",
        "        init_lambdas_hat = tf.nn.softmax(init_lambdas_hat - tf.reduce_max(init_lambdas_hat)) * tf.cast(len(losses), dtype=tf.float32)\n",
        "\n",
        "        # use rho for deciding, whether a random lookback should be performed\n",
        "        new_lambdas = [(rho * alpha * self.lambdas[i] + (1 - rho) * alpha * init_lambdas_hat[i] + (1 - alpha) * lambdas_hat[i]) for i in range(len(losses))]\n",
        "        self.lambdas = [var.assign(tf.stop_gradient(lam)) for var, lam in zip(self.lambdas, new_lambdas)]\n",
        "\n",
        "        # compute weighted loss\n",
        "        loss = tf.reduce_sum([lam * loss for lam, loss in zip(self.lambdas, losses)])\n",
        "\n",
        "        # store current losses in self.last_losses to be accessed in the next iteration\n",
        "        self.last_losses = [var.assign(tf.stop_gradient(loss)) for var, loss in zip(self.last_losses, losses)]\n",
        "        # in first iteration, store losses in self.init_losses to be accessed in next iterations\n",
        "        first_iteration = tf.cast(self.call_count < 1, dtype=tf.float32)\n",
        "        self.init_losses = [var.assign(tf.stop_gradient(loss * first_iteration + var * (1 - first_iteration))) for var, loss in zip(self.init_losses, losses)]\n",
        "\n",
        "        self.call_count.assign_add(1)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "Jx-i0rnim59F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics for logging"
      ],
      "metadata": {
        "id": "lkZtmWG2rr0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Metric for logging Kirchhoff Loss Terms"
      ],
      "metadata": {
        "id": "b62cHWlBsRjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KirchhoffMetric(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    Kirchhoff metric to log the values of each loss term, i.e. L_f, L_b0 and L_b2.\n",
        "    \"\"\"\n",
        "    def __init__(self, plate: KirchhoffPDE, name='kirchhoff_metric', **kwargs):\n",
        "        \"\"\"Initialize Kirchhoff metric with a KirchhoffPDE instance and metric name.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        plate : KirchhoffPDE\n",
        "            Instance of the KirchhoffPDE.\n",
        "        name : str, optional\n",
        "            Name of the metric. Defaults to 'kirchhoff_metric'.\n",
        "        \"\"\"\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.plate = plate\n",
        "        self.L_f_mean = self.add_weight(name='L_f_mean', initializer='zeros')\n",
        "        self.L_b0_mean = self.add_weight(name='L_b0_mean', initializer='zeros')\n",
        "        self.L_b2_mean = self.add_weight(name='L_b2_mean', initializer='zeros')\n",
        "        self.L_u_mean = self.add_weight(name='L_u_mean', initializer='zeros')\n",
        "\n",
        "    def update_state(self, xy, y_pred, sample_weight=None):\n",
        "        x, y = xy[:, :1], xy[:, 1:]\n",
        "        L_f, L_b0, L_b2, L_u = self.plate.compute_loss(x, y, y_pred, eval=True)\n",
        "        self.L_f_mean.assign(tf.reduce_mean(L_f[:, 0], axis=0))\n",
        "        self.L_b0_mean.assign(tf.reduce_mean(L_b0[:, 0], axis=0))\n",
        "        self.L_b2_mean.assign(tf.reduce_mean(L_b2[:, 0], axis=0))\n",
        "        self.L_u_mean.assign(tf.reduce_mean(L_u[:, 0], axis=0))\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.L_f_mean.assign(0)\n",
        "        self.L_b0_mean.assign(0)\n",
        "        self.L_b2_mean.assign(0)\n",
        "        self.L_u_mean.assign(0)\n",
        "\n",
        "    def result(self):\n",
        "        return {'L_f': self.L_f_mean, 'L_b0': self.L_b0_mean, 'L_b2': self.L_b2_mean, 'L_u': self.L_u_mean}"
      ],
      "metadata": {
        "id": "ezTtMM_0gfWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Metric for logging ReLoBRaLo weights"
      ],
      "metadata": {
        "id": "Nz0ayhJ8sIZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLoBRaLoLambdaMetric(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    A custom TensorFlow metric class to monitor the lambdas of the ReLoBRaLoKirchhoffLoss.\n",
        "    \"\"\"\n",
        "    def __init__(self, loss:ReLoBRaLoKirchhoffLoss, name='relobralo_lambda_metric', **kwargs):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        loss : ReLoBRaLoKirchhoffLoss)\n",
        "            The ReLoBRaLoKirchhoffLoss object that holds the lambdas.\n",
        "        name : str, optional)\n",
        "            The name of the metric. Defaults to 'relobralo_lambda_metric'.\n",
        "        \"\"\"\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.loss = loss\n",
        "        self.L_f_lambda_mean = self.add_weight(name='L_f_lambda_mean', initializer='zeros')\n",
        "        self.L_b0_lambda_mean = self.add_weight(name='L_b0_lambda_mean', initializer='zeros')\n",
        "        self.L_b2_lambda_mean = self.add_weight(name='L_b2_lambda_mean', initializer='zeros')\n",
        "\n",
        "    def update_state(self, xy, y_pred, sample_weight=None):\n",
        "        L_f_lambda, L_b0_lambda, L_b2_lambda = self.loss.lambdas\n",
        "        self.L_f_lambda_mean.assign(L_f_lambda)\n",
        "        self.L_b0_lambda_mean.assign(L_b0_lambda)\n",
        "        self.L_b2_lambda_mean.assign(L_b2_lambda)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.L_f_lambda_mean.assign(0)\n",
        "        self.L_b0_lambda_mean.assign(0)\n",
        "        self.L_b2_lambda_mean.assign(0)\n",
        "\n",
        "    def result(self):\n",
        "        return {'L_f_lambda': self.L_f_lambda_mean, 'L_b0_lambda': self.L_b0_lambda_mean, 'L_b2_lambda': self.L_b2_lambda_mean}"
      ],
      "metadata": {
        "id": "YrSAlYcGsPC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PINN Model"
      ],
      "metadata": {
        "id": "r6uOmYiAsZ2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KirchhoffPINN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    This class is a implementation of a physics-informed neural network (PINN)\n",
        "    for the Kirchhoff plate bending partial differential equation (PDE).\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_widths: List[int]=[64, 64, 64], activation: Union[str, Callable]=tf.keras.activations.swish, **kwargs):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer_widths : List[int], optional\n",
        "            List of integers representing the widths of the hidden layers in the model.\n",
        "        activation : Union[str, Callable], optional\n",
        "            Activation function to be applied in each layer.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_sequence = [tf.keras.layers.Dense(width, activation=activation, kernel_initializer='glorot_normal') for width in layer_widths]\n",
        "        self.layer_sequence.append(tf.keras.layers.Dense(1, kernel_initializer='glorot_normal'))\n",
        "\n",
        "    def call(self, xy, training=None, mask=None):\n",
        "        x, y = xy[:, :1], xy[:, 1:]\n",
        "\n",
        "        u = Concatenate()([x, y])\n",
        "        for layer in self.layer_sequence:\n",
        "            u = layer(u)\n",
        "\n",
        "        dudxx, dudyy, dudxxxx, dudyyyy, dudxxyy = compute_derivatives(x, y, u)\n",
        "\n",
        "        return tf.concat([u, dudxx, dudyy, dudxxxx, dudyyyy, dudxxyy], axis=-1)"
      ],
      "metadata": {
        "id": "GhdeXm6gCmeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem definition\n",
        "The Kirchhoff PDE states that the fourth order derivative of the plate's deformation u is equal to the load p divided by a constant D. In order to obtain an analytical solution, we can define a load with a sinusoidal distribution:\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\begin{array}{@{}l@{}}\n",
        "        p(x, y) = p_0\\sin\\left(\\frac{x\\pi}{W}\\right)\\sin\\left(\\frac{y\\pi}{H}\\right)\\\\\n",
        "        u(x, y) = \\frac{p_0}{\\pi^4 D (W^{-2} + H^{-2})^2}\\sin\\left(\\frac{x\\pi}{W}\\right)\\sin\\left(\\frac{y\\pi}{H}\\right)\n",
        "    \\end{array}\n",
        "\\end{equation}\n",
        "$$\n",
        "We consider a concrete plate of width $a = 1'000$ cm, height $b = 1'000$ cm, base load $p_0 = 15$ MN/cm^2, Young's modulus $E = 30,000$ MN/m^2, plate thickness $T = 0.2$m and Poisson's ratio of $\\nu = 0.2$."
      ],
      "metadata": {
        "id": "vAsiRlEusgLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = 10\n",
        "H = 10\n",
        "T = 0.2\n",
        "E = 30000\n",
        "nue = 0.2\n",
        "p0 = 0.15\n",
        "D = (E * T**3) / (12 * (1 - nue**2)) # flexural stiffnes of the plate\n",
        "\n",
        "load = lambda x, y: p0 * tf.math.sin(x * np.pi / W) * tf.math.sin(y * np.pi / H)\n",
        "u_val = lambda x, y: p0 / (np.pi**4 * D * (W**-2 + H**-2)**2) * tf.math.sin(x * np.pi / W) * tf.math.sin(y * np.pi / H)\n",
        "plate = KirchhoffPDE(p=load, u_val=u_val, T=T, nue=nue, E=E, W=W, H=H)\n",
        "plate.visualise()"
      ],
      "metadata": {
        "id": "MyX59mMSJRI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train without loss balancing\n",
        "Now that the PDE has been defined, we can build the model as well as the loss function. We are first using the default KirchhoffLoss and will compare it to the ReLoBRaLoKirchhoffLoss later."
      ],
      "metadata": {
        "id": "TH7S6dbnuw4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinn = KirchhoffPINN()\n",
        "loss = KirchhoffLoss(plate)\n",
        "pinn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss, metrics=[KirchhoffMetric(plate)])"
      ],
      "metadata": {
        "id": "AsYZRXP_uw-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = pinn.fit(\n",
        "    plate.get_train_dataset(),\n",
        "    epochs=1000,\n",
        "    steps_per_epoch=1,\n",
        "    callbacks=[\n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.1, patience=30, min_delta=0, verbose=True),\n",
        "        EarlyStopping(monitor='loss', patience=100, restore_best_weights=True, verbose=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "mkq2GK-lC7NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the progress of the several loss terms as well as the error against the analytical solution."
      ],
      "metadata": {
        "id": "uCbPSqNw1Mqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 4.5), dpi=100)\n",
        "plt.plot(np.log(h.history['L_f']), label='$L_f$ governing equation')\n",
        "plt.plot(np.log(h.history['L_b0']), label='$L_{b0}$ Dirichlet boundaries')\n",
        "plt.plot(np.log(h.history['L_b2']), label='$L_{b2}$ Moment boundaries')\n",
        "plt.plot(np.log(h.history['L_u']), label='$L_u$ analytical solution')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Log-loss')\n",
        "plt.title('Default Loss')\n",
        "plt.title('Loss evolution Kirchhoff PDE')\n",
        "plt.savefig('kirchhoff_loss_unscaled')"
      ],
      "metadata": {
        "id": "tAGKrQF5JJWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually inspect the error distribution on the physical domain."
      ],
      "metadata": {
        "id": "AHMaZIzi1RNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plate.visualise(pinn)"
      ],
      "metadata": {
        "id": "er6QebwvbB59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with ReLoBRaLo"
      ],
      "metadata": {
        "id": "ZI7IB3HfvTJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relobralo_pinn = KirchhoffPINN()\n",
        "relobralo_loss = ReLoBRaLoKirchhoffLoss(plate, temperature=0.1, rho=0.99, alpha=0.999)\n",
        "relobralo_pinn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=relobralo_loss, metrics=[KirchhoffMetric(plate), ReLoBRaLoLambdaMetric(relobralo_loss)])"
      ],
      "metadata": {
        "id": "hSmRNrjLvFXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_relobralo = relobralo_pinn.fit(\n",
        "    plate.get_train_dataset(),\n",
        "    epochs=1000,\n",
        "    steps_per_epoch=100,\n",
        "    callbacks=[\n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.1, patience=30, min_delta=0, verbose=True),\n",
        "        EarlyStopping(monitor='loss', patience=100, restore_best_weights=True, verbose=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "hoRv-tUvvFXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the progress of the several loss terms as well as the error against the analytical solution."
      ],
      "metadata": {
        "id": "qk9D73Q_1OZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 4.5), dpi=100)\n",
        "plt.plot(np.log(h_relobralo.history['L_f']), label='$L_f$ governing equation')\n",
        "plt.plot(np.log(h_relobralo.history['L_b0']), label='$L_{b0}$ Dirichlet boundaries')\n",
        "plt.plot(np.log(h_relobralo.history['L_b2']), label='$L_{b2}$ Moment boundaries')\n",
        "plt.plot(np.log(h_relobralo.history['L_u']), label='$L_u$ analytical solution')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Log-loss')\n",
        "plt.ylim([-17, -4])\n",
        "plt.title('Loss evolution Kirchhoff PDE\\nwith ReLoBRaLo')\n",
        "plt.savefig('kirchhoff_loss_relobralo')"
      ],
      "metadata": {
        "id": "50yUto-yJlmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the progress of the scalings $\\lambda$."
      ],
      "metadata": {
        "id": "w0VtQnB61WvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 4.5), dpi=100)\n",
        "plt.plot(h_relobralo.history['L_f_lambda'], label='$\\lambda_f$ governing equation')\n",
        "plt.plot(h_relobralo.history['L_b0_lambda'], label='$\\lambda_{b0}$ Dirichlet boundaries')\n",
        "plt.plot(h_relobralo.history['L_b2_lambda'], label='$\\lambda_{b2}$ Moment boundaries')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('scalings $\\lambda$')\n",
        "plt.title('ReLoBRaLo weights on Kirchhoff PDE')\n",
        "plt.savefig('kirchhoff_lambdas_relobralo')"
      ],
      "metadata": {
        "id": "w-4Uiof21H79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually inspect the error distribution on the physical domain."
      ],
      "metadata": {
        "id": "oBLKmU5p1Sbh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk85BkoosMKL"
      },
      "source": [
        "plate.visualise(relobralo_pinn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeLt17mSkVAz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}