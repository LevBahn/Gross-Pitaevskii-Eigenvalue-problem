{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzud5_5f-EJ-"
      },
      "source": [
        "# **Re**lative **Lo**ss **B**alancing with **Ra**ndom **Lo**okbacks for Helmholtz PDE\n",
        "This notebook implements the concepts from the [Multi-Objective Loss Balancing for Physics-Informed Deep Learning paper](https://arxiv.org/abs/2110.09813) and [Improving PINNs through Adaptive Loss Balancing medium article](https://medium.com/p/55662759e701). It showcases the gains in performance when applying Loss Balancing Schemes to PINN training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFnikg8NH6E3"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "from typing import Tuple, Callable, List, Union\n",
        "from tensorflow.experimental.numpy import isclose\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions\n",
        "We start by defining a few utility functions that will be useful later. The first, *compute_derivatives*, computes all the derivatives necessary to formulate the Helmholtz PINN objective (it is a second order differential equation).\n",
        "\n"
      ],
      "metadata": {
        "id": "_fiikxfJk_UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_derivatives(x, y, u):\n",
        "    \"\"\"\n",
        "    Computes the derivatives of `u` with respect to `x` and `y`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : tf.Tensor\n",
        "        The x-coordinate of the collocation points, of shape (batch_size, 1).\n",
        "    y : tf.Tensor\n",
        "        The y-coordinate of the collocation points, of shape (batch_size, 1).\n",
        "    u : tf.Tensor\n",
        "        The prediction made by the PINN, of shape (batch_size, 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        The second order derivatives of `u` with respect to `x`, `y`.\n",
        "    \"\"\"\n",
        "    dudx, dudy = tf.gradients(u, [x, y])\n",
        "    dudxx = tf.gradients(dudx, x)[0]\n",
        "    dudyy = tf.gradients(dudy, y)[0]\n",
        "    return dudxx, dudyy"
      ],
      "metadata": {
        "id": "pYiE6aZcuL4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTbTpsj5w-wY"
      },
      "source": [
        "## Define Helmholtz PDE Class\n",
        "\n",
        "The Helmholtz equation is a partial differential equation that describes the propagation of waves in a medium. It is a second-order equation and named after the German physicist Hermann von Helmholtz. We can define the problem such that there exists an analytical solution, against which we can compare our PINN:\n",
        "\n",
        "$$\n",
        "    f(x, y) = (-\\pi^2 - (4\\pi)^2 + k^2) sin(\\pi x) sin(4 \\pi y) \\\\\n",
        "    u(x, y) = sin(\\pi x) sin(4 \\pi y)\\\\\n",
        "    u(-1, y) = u(1, y) = u(x, -1) = u(x, 1) = 0\\\\\n",
        "$$\n",
        "We consider the case where k = 1.\n",
        "\n",
        "This class represents the [Helmholtz PDE](https://en.wikipedia.org/wiki/Helmholtz_equation). It provides a set of utility functions to train a [Physics Informed Neural Network (PINN)](https://arxiv.org/pdf/1711.10566.pdf) on the Helmholtz PDE. The class has functions that generate training and validation data, calculate loss, and visualize the results of the PINN's predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPS = 1e-7\n",
        "\n",
        "class HelmholtzPDE:\n",
        "    \"\"\"\n",
        "    Class representing a Helmholtz pde, providing several methods for training a Physics-Informed Neural Network.\n",
        "    \"\"\"\n",
        "    def __init__(self, u_val: Callable[[tf.Tensor, tf.Tensor], tf.Tensor]):\n",
        "        \"\"\"\n",
        "        Initialize the HelmholtzPDE class.\n",
        "\n",
        "        PARAMETERS\n",
        "        ----------------\n",
        "        u_val : Callable[[tf.Tensor, tf.Tensor], tf.Tensor]\n",
        "            A function to validate the predictions.\n",
        "        \"\"\"\n",
        "        self.u_val = u_val\n",
        "        self.k = 1\n",
        "        self.num_terms = 2\n",
        "\n",
        "    def training_batch(self, batch_size_domain:int=800, batch_size_boundary:int=100) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        Generates a batch of collocation points by randomly sampling `batch_size_domain` points inside the domain\n",
        "        and `batch_size_boundary` points on each of the four boundaries.\n",
        "\n",
        "        PARAMETERS\n",
        "        --------------------\n",
        "        batch_size_domain : int\n",
        "            number of points to be sampled inside of the domain\n",
        "        batch_size_boundary : int\n",
        "            number of points to be sampled on each of the four boundaries\n",
        "        \"\"\"\n",
        "        x_in =  tf.random.uniform(shape=(batch_size_domain, 1), minval=-1, maxval=1)\n",
        "        x_b1 = -tf.ones(shape=(batch_size_boundary, 1))\n",
        "        x_b2 =  tf.ones(shape=(batch_size_boundary, 1))\n",
        "        x_b3 =  tf.random.uniform(shape=(batch_size_boundary, 1), minval=-1, maxval=1)\n",
        "        x_b4 =  tf.random.uniform(shape=(batch_size_boundary, 1), minval=-1, maxval=1)\n",
        "        x = tf.concat([x_in, x_b1, x_b2, x_b3, x_b4], axis=0)\n",
        "\n",
        "        y_in =  tf.random.uniform(shape=(batch_size_domain, 1), minval=-1, maxval=1)\n",
        "        y_b1 =  tf.random.uniform(shape=(batch_size_boundary, 1), minval=-1, maxval=1)\n",
        "        y_b2 =  tf.random.uniform(shape=(batch_size_boundary, 1), minval=-1, maxval=1)\n",
        "        y_b3 = -tf.ones(shape=(batch_size_boundary, 1))\n",
        "        y_b4 =  tf.ones(shape=(batch_size_boundary, 1))\n",
        "        y = tf.concat([y_in, y_b1, y_b2, y_b3, y_b4], axis=0)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def get_train_dataset(self, batch_size_domain:int=800, batch_size_boundary:int=100):\n",
        "        \"\"\"\n",
        "        Creates a tf.data.Dataset generator for training.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size_domain : int\n",
        "            number of points to be sampled inside of the domain. Default is 800.\n",
        "        batch_size_boundary : int\n",
        "            number of points to be sampled on each of the four boundaries. Default is 100.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tf.data.Dataset\n",
        "            A `tf.data.Dataset` generator for training.\n",
        "        \"\"\"\n",
        "        def generator():\n",
        "            while True:\n",
        "                xy = tf.concat(self.training_batch(batch_size_domain, batch_size_boundary), axis=-1)\n",
        "                yield xy, xy\n",
        "\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            generator,\n",
        "            output_types=(tf.float32, tf.float32),\n",
        "            output_shapes=((None, 2), (None, 2))\n",
        "        )\n",
        "\n",
        "    def validation_batch(self, grid_width:int=64, grid_height:int=64):\n",
        "        \"\"\"\n",
        "        Generates a grid of points that can easily be used to generate an image of the pde,\n",
        "        where each point is a pixel.\n",
        "\n",
        "        PARAMETERS\n",
        "        ----------\n",
        "        grid_width : int\n",
        "            width of the grid\n",
        "        grid_height : int\n",
        "            height of the grid\n",
        "        \"\"\"\n",
        "        x, y = np.mgrid[-1:1:complex(0, grid_width), -1:1:complex(0, grid_height)]\n",
        "        x = tf.cast(x.reshape(grid_width * grid_height, 1), dtype=tf.float32)\n",
        "        y = tf.cast(y.reshape(grid_width * grid_height, 1), dtype=tf.float32)\n",
        "        u = tf.math.sin(np.pi * x) * tf.math.sin(4 * np.pi * y)\n",
        "        return x, y, u\n",
        "\n",
        "    def compute_loss(self, x, y, preds, eval=False):\n",
        "        \"\"\"\n",
        "        Computes the physics-informed loss for Helmholtz's PDE.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : tf.Tensor of shape (batch_size, 1)\n",
        "            x coordinate of the points in the current batch\n",
        "        y : tf.Tensor of shape (batch_size, 1)\n",
        "            y coordinate of the points in the current batch\n",
        "        preds : tf.Tensor of shape (batch_size, 6)\n",
        "            predictions made by our PINN (dim 0) as well as dudxx (dim 1), dudyy (dim 2)\n",
        "        \"\"\"\n",
        "\n",
        "        # governing equation loss\n",
        "        L_f = (preds[:, 1:2] + preds[:, 2:3] + self.k**2 * preds[:, 0:1] - \\\n",
        "              (-np.pi**2 - (4 * np.pi)**2 + self.k**2) * tf.math.sin(np.pi * x) * tf.math.sin(4 * np.pi * y))**2\n",
        "\n",
        "        # determine which points are on the boundaries of the domain\n",
        "        # if a point is on either of the boundaries, its value is 1 and 0 otherwise\n",
        "        x_lower = tf.cast(isclose(x, -1, rtol=0., atol=EPS), dtype=tf.float32)\n",
        "        x_upper = tf.cast(isclose(x,  1, rtol=0., atol=EPS), dtype=tf.float32)\n",
        "        y_lower = tf.cast(isclose(y, -1, rtol=0., atol=EPS), dtype=tf.float32)\n",
        "        y_upper = tf.cast(isclose(y,  1, rtol=0., atol=EPS), dtype=tf.float32)\n",
        "\n",
        "        # compute 0th order boundary condition loss\n",
        "        L_b = ((x_lower + x_upper + y_lower + y_upper) * preds[:, 0:1])**2\n",
        "\n",
        "        if eval:\n",
        "            L_u = (tf.math.sin(np.pi*x) * tf.math.sin(4*np.pi*y) - preds[:, 0:1])**2\n",
        "            return L_f, L_b, L_u\n",
        "\n",
        "        return L_f, L_b\n",
        "\n",
        "    @tf.function\n",
        "    def __validation_results(self, pinn: tf.keras.Model, image_width: int = 64, image_height: int = 64):\n",
        "        \"\"\"Computes the validation results for the given model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        pinn : tf.keras.Model\n",
        "            A TensorFlow Keras model instance.\n",
        "        image_width : int\n",
        "            The width of the image (defaults to 64).\n",
        "        image_height : int\n",
        "            The height of the image (defaults to 64).\n",
        "\n",
        "        Returns:\n",
        "        u_real : tf.Tensor\n",
        "            A tensor containing the real displacement.\n",
        "        u_pred : tf.Tensor\n",
        "            A tensor containing the predicted displacement.\n",
        "        f : tf.Tensor\n",
        "            A tensor containing the governing equation.\n",
        "        \"\"\"\n",
        "        x, y, u_real = self.validation_batch(image_width, image_height)\n",
        "        pred = pinn(tf.concat([x, y], axis=-1), training=False)\n",
        "        u_pred, dudxx, dudyy = pred[:, 0:1], pred[:, 1:2], pred[:, 2:3]\n",
        "        sin_xy = tf.math.sin(np.pi*x) * tf.math.sin(4*np.pi*y)\n",
        "        f = (dudxx + dudyy + self.k * u_pred - (-np.pi**2 - (4*np.pi)**2 + self.k**2) * sin_xy)**2\n",
        "        return u_real, u_pred, f\n",
        "\n",
        "\n",
        "    def visualise(self, pinn: tf.keras.Model = None, image_width: int = 64, image_height: int = 64):\n",
        "        \"\"\"\n",
        "        If no model is provided, visualises only the load distribution on the plate.\n",
        "        Otherwise, visualizes the results of the given model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        pinn : tf.keras.Model\n",
        "            A TensorFlow Keras model instance.\n",
        "        image_width : int\n",
        "            The width of the image (defaults to 64).\n",
        "        image_height : int\n",
        "            The height of the image (defaults to 64).\n",
        "        \"\"\"\n",
        "        if pinn is None:\n",
        "            x, y, u_real = self.validation_batch(image_width, image_height)\n",
        "            self.__show_image(u_real.numpy().reshape(image_width, image_height))\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            u_real, u_pred, f = self.__validation_results(pinn, image_width, image_height)\n",
        "            u_real = u_real.numpy().reshape(image_width, image_height)\n",
        "            u_pred = u_pred.numpy().reshape(image_width, image_height)\n",
        "            f = f.numpy().reshape(image_width, image_height)\n",
        "\n",
        "            fig, axs = plt.subplots(2, 2, figsize=(8, 7))\n",
        "            self.__show_image(u_pred, axs[0, 0], 'Predicted')\n",
        "            self.__show_image((u_pred - u_real)**2, axs[0, 1], 'Squared Error')\n",
        "            self.__show_image(f, axs[1, 0], 'Governing Equation')\n",
        "            self.__show_image(f**2, axs[1, 1], 'Squared Error Governing Equation')\n",
        "\n",
        "            # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "            for ax in axs.flat:\n",
        "                ax.label_outer()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    def __show_image(self, img:np.array, axis:plt.axis=None, title:str='', x_label='x', y_label='y', z_label=''):\n",
        "        if axis is None:\n",
        "             _, axis = plt.subplots(1, 1, figsize=(4, 3.2), dpi=100)\n",
        "        im = axis.imshow(np.rot90(img, k=3), cmap='plasma', origin='lower', aspect='auto')\n",
        "        cb = plt.colorbar(im, label=z_label, ax=axis)\n",
        "        axis.set_xticks([0, img.shape[0]-1])\n",
        "        axis.set_xticklabels([-1, 1])\n",
        "        axis.set_yticks([0, img.shape[1]-1])\n",
        "        axis.set_yticklabels([-1, 1])\n",
        "        axis.set_xlabel(x_label)\n",
        "        axis.set_ylabel(y_label)\n",
        "        axis.set_title(title)\n",
        "        return im"
      ],
      "metadata": {
        "id": "2XXSE7o_Ovxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helmholtz PINN Loss Function"
      ],
      "metadata": {
        "id": "AjJZMkIkmkyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Default Helmholtz PDE Loss\n",
        "This class inherits from the keras Loss class and can be used in the keras API (i.e. model.fit()) for training our Helmholtz PINN."
      ],
      "metadata": {
        "id": "hLormYLgsULd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HelmholtzLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    Helmholtz Loss for physics-informed neural network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pde: HelmholtzPDE\n",
        "        The HelmholtzPDE object representing the PDE\n",
        "        The name of the loss, by default 'ReLoBRaLoHelmholtzLoss'\n",
        "    \"\"\"\n",
        "    def __init__(self, pde:HelmholtzPDE, name='HelmholtzLoss'):\n",
        "        super().__init__(name=name)\n",
        "        self.pde = pde\n",
        "\n",
        "    def call(self, xy, preds):\n",
        "        x, y = xy[:, :1], xy[:, 1:]\n",
        "        L_f, L_b = self.pde.compute_loss(x, y, preds)\n",
        "        loss = L_f + L_b\n",
        "        return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "QGhpCI5kDElJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLoBRaLo Loss Balancing Objective\n",
        "This class inherits from the HelmholtzLoss class and balances the contributions towards the total loss by scaling the terms L_0, L_b0 and L_b2 according to the ReLoBRaLo loss balancing scheme.\n",
        "\n",
        "$$\n",
        "\\lambda_i^{\\textit{bal}}(t, t') = k\\cdot\\frac{\\operatorname{exp}\\left(\\frac{L_i(t)}{\\tau L_i(t')}\\right)}{\\sum_{j=1}^k \\operatorname{exp} \\left(\\frac{L_j(t)}{\\tau L_j(t')} \\right)}, \\; i \\in \\{1, \\dots, k\\}\\\\\n",
        "\\lambda_{i}^{\\textit{hist}}(t) = \\rho\\lambda_i(t-1) + (1-\\rho)\\lambda_i^{\\textit{bal}}(t, 0))\\\\\n",
        "\\lambda_i(t) = \\alpha\\lambda_{i}^{\\textit{hist}} + (1-\\alpha)\\lambda_i^{\\textit{bal}}(t, t-1)\\\\\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is the exponential decay rate, $\\rho$ is a Bernoulli random variable and $\\mathbb{E}[\\rho]$ should be chosen close to 1. The intermediate step $\\lambda_i^{\\textit{bal}}(t, t')$ calculates scalings based on the relative improvements of each term between time steps $t'$ and $t$. The following step $\\lambda_{i}^{\\textit{hist}}(t)$ defines, whether the scalings calculated in the previous time step ($\\rho$ evaluates to 1) or the relative improvements since the beginning of training ($\\rho$ evaluates to 0) should be carried forward. Note that this concept of randomly retaining or discarding the history of scalings is what we denote as \"random lookbacks\". Finally, the scaling $\\lambda_i(t)$ for term $i$ is obtained by means of an exponential decay, where $\\alpha$ controls the weight given to past scalings versus the scalings calculated in the current time step."
      ],
      "metadata": {
        "id": "xJGqdqeim52j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLoBRaLoHelmholtzLoss(HelmholtzLoss):\n",
        "    \"\"\"\n",
        "    Class for the ReLoBRaLo Helmholtz Loss.\n",
        "    This class extends the Helmholtz Loss to have dynamic weighting for each term in the calculation of the loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, pde:HelmholtzPDE, alpha:float=0.999, temperature:float=1., rho:float=0.9999,\n",
        "                 name='ReLoBRaLoHelmholtzLoss'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        pde : HelmholtzPDE\n",
        "            An instance of HelmholtzPDE class containing the `compute_loss` function.\n",
        "        alpha, optional : float\n",
        "            Controls the exponential weight decay rate.\n",
        "            Value between 0 and 1. The smaller, the more stochasticity.\n",
        "            0 means no historical information is transmitted to the next iteration.\n",
        "            1 means only first calculation is retained. Defaults to 0.999.\n",
        "        temperature, optional : float\n",
        "            Softmax temperature coefficient. Controlls the \"sharpness\" of the softmax operation.\n",
        "            Defaults to 1.\n",
        "        rho, optional : float\n",
        "            Probability of the Bernoulli random variable controlling the frequency of random lookbacks.\n",
        "            Value berween 0 and 1. The smaller, the fewer lookbacks happen.\n",
        "            0 means lambdas are always calculated w.r.t. the initial loss values.\n",
        "            1 means lambdas are always calculated w.r.t. the loss values in the previous training iteration.\n",
        "            Defaults to 0.9999.\n",
        "        \"\"\"\n",
        "        super().__init__(pde, name=name)\n",
        "        self.pde = pde\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.rho = rho\n",
        "        self.call_count = tf.Variable(0, trainable=False, dtype=tf.int16)\n",
        "\n",
        "        self.lambdas = [tf.Variable(1., trainable=False) for _ in range(pde.num_terms)]\n",
        "        self.last_losses = [tf.Variable(1., trainable=False) for _ in range(pde.num_terms)]\n",
        "        self.init_losses = [tf.Variable(1., trainable=False) for _ in range(pde.num_terms)]\n",
        "\n",
        "    def call(self, xy, preds):\n",
        "        x, y = xy[:, :1], xy[:, 1:]\n",
        "        losses = [tf.reduce_mean(loss) for loss in self.pde.compute_loss(x, y, preds)]\n",
        "\n",
        "        # in first iteration (self.call_count == 0), drop lambda_hat and use init lambdas, i.e. lambda = 1\n",
        "        #   i.e. alpha = 1 and rho = 1\n",
        "        # in second iteration (self.call_count == 1), drop init lambdas and use only lambda_hat\n",
        "        #   i.e. alpha = 0 and rho = 1\n",
        "        # afterwards, default procedure (see paper)\n",
        "        #   i.e. alpha = self.alpha and rho = Bernoully random variable with p = self.rho\n",
        "        alpha = tf.cond(tf.equal(self.call_count, 0),\n",
        "                lambda: 1.,\n",
        "                lambda: tf.cond(tf.equal(self.call_count, 1),\n",
        "                                lambda: 0.,\n",
        "                                lambda: self.alpha))\n",
        "        rho = tf.cond(tf.equal(self.call_count, 0),\n",
        "              lambda: 1.,\n",
        "              lambda: tf.cond(tf.equal(self.call_count, 1),\n",
        "                              lambda: 1.,\n",
        "                              lambda: tf.cast(tf.random.uniform(shape=()) < self.rho, dtype=tf.float32)))\n",
        "\n",
        "        # compute new lambdas w.r.t. the losses in the previous iteration\n",
        "        lambdas_hat = [losses[i] / (self.last_losses[i] * self.temperature + EPS) for i in range(len(losses))]\n",
        "        lambdas_hat = tf.nn.softmax(lambdas_hat - tf.reduce_max(lambdas_hat)) * tf.cast(len(losses), dtype=tf.float32)\n",
        "\n",
        "        # compute new lambdas w.r.t. the losses in the first iteration\n",
        "        init_lambdas_hat = [losses[i] / (self.init_losses[i] * self.temperature + EPS) for i in range(len(losses))]\n",
        "        init_lambdas_hat = tf.nn.softmax(init_lambdas_hat - tf.reduce_max(init_lambdas_hat)) * tf.cast(len(losses), dtype=tf.float32)\n",
        "\n",
        "        # use rho for deciding, whether a random lookback should be performed\n",
        "        new_lambdas = [(rho * alpha * self.lambdas[i] + (1 - rho) * alpha * init_lambdas_hat[i] + (1 - alpha) * lambdas_hat[i]) for i in range(len(losses))]\n",
        "        self.lambdas = [var.assign(tf.stop_gradient(lam)) for var, lam in zip(self.lambdas, new_lambdas)]\n",
        "\n",
        "        # compute weighted loss\n",
        "        loss = tf.reduce_sum([lam * loss for lam, loss in zip(self.lambdas, losses)])\n",
        "\n",
        "        # store current losses in self.last_losses to be accessed in the next iteration\n",
        "        self.last_losses = [var.assign(tf.stop_gradient(loss)) for var, loss in zip(self.last_losses, losses)]\n",
        "        # in first iteration, store losses in self.init_losses to be accessed in next iterations\n",
        "        first_iteration = tf.cast(self.call_count < 1, dtype=tf.float32)\n",
        "        self.init_losses = [var.assign(tf.stop_gradient(loss * first_iteration + var * (1 - first_iteration))) for var, loss in zip(self.init_losses, losses)]\n",
        "\n",
        "        self.call_count.assign_add(1)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "Jx-i0rnim59F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics for logging"
      ],
      "metadata": {
        "id": "lkZtmWG2rr0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Metric for logging Helmholtz Loss Terms"
      ],
      "metadata": {
        "id": "b62cHWlBsRjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HelmholtzMetric(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    Helmholtz metric to log the values of each loss term, i.e. L_f and L_b.\n",
        "    \"\"\"\n",
        "    def __init__(self, pde: HelmholtzPDE, name='Helmholtz_metric', **kwargs):\n",
        "        \"\"\"Initialize Helmholtz metric with a HelmholtzPDE instance and metric name.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        pde : HelmholtzPDE\n",
        "            Instance of the HelmholtzPDE.\n",
        "        name : str, optional\n",
        "            Name of the metric. Defaults to 'Helmholtz_metric'.\n",
        "        \"\"\"\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.pde = pde\n",
        "        self.L_f_mean = self.add_weight(name='L_f_mean', initializer='zeros')\n",
        "        self.L_b_mean = self.add_weight(name='L_b_mean', initializer='zeros')\n",
        "        self.L_u_mean = self.add_weight(name='L_u_mean', initializer='zeros')\n",
        "\n",
        "    def update_state(self, xy, y_pred, sample_weight=None):\n",
        "        x, y = xy[:, :1], xy[:, 1:]\n",
        "        L_f, L_b, L_u = self.pde.compute_loss(x, y, y_pred, eval=True)\n",
        "        self.L_f_mean.assign(tf.reduce_mean(L_f[:, 0], axis=0))\n",
        "        self.L_b_mean.assign(tf.reduce_mean(L_b[:, 0], axis=0))\n",
        "        self.L_u_mean.assign(tf.reduce_mean(L_u[:, 0], axis=0))\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.L_f_mean.assign(0)\n",
        "        self.L_b_mean.assign(0)\n",
        "        self.L_u_mean.assign(0)\n",
        "\n",
        "    def result(self):\n",
        "        return {'L_f': self.L_f_mean, 'L_b': self.L_b_mean, 'L_u': self.L_u_mean}"
      ],
      "metadata": {
        "id": "ezTtMM_0gfWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Metric for logging ReLoBRaLo weights"
      ],
      "metadata": {
        "id": "Nz0ayhJ8sIZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLoBRaLoLambdaMetric(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    A custom TensorFlow metric class to monitor the lambdas of the ReLoBRaLoHelmholtzLoss.\n",
        "    \"\"\"\n",
        "    def __init__(self, loss:ReLoBRaLoHelmholtzLoss, name='relobralo_lambda_metric', **kwargs):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        loss : ReLoBRaLoHelmholtzLoss)\n",
        "            The ReLoBRaLoHelmholtzLoss object that holds the lambdas.\n",
        "        name : str, optional)\n",
        "            The name of the metric. Defaults to 'relobralo_lambda_metric'.\n",
        "        \"\"\"\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.loss = loss\n",
        "        self.L_f_lambda_mean = self.add_weight(name='L_f_lambda_mean', initializer='zeros')\n",
        "        self.L_b_lambda_mean = self.add_weight(name='L_b_lambda_mean', initializer='zeros')\n",
        "\n",
        "    def update_state(self, xy, y_pred, sample_weight=None):\n",
        "        L_f_lambda, L_b_lambda = self.loss.lambdas\n",
        "        self.L_f_lambda_mean.assign(L_f_lambda)\n",
        "        self.L_b_lambda_mean.assign(L_b_lambda)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.L_f_lambda_mean.assign(0)\n",
        "        self.L_b_lambda_mean.assign(0)\n",
        "\n",
        "    def result(self):\n",
        "        return {'L_f_lambda': self.L_f_lambda_mean, 'L_b_lambda': self.L_b_lambda_mean}"
      ],
      "metadata": {
        "id": "YrSAlYcGsPC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PINN Model"
      ],
      "metadata": {
        "id": "r6uOmYiAsZ2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HelmholtzPINN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    This class is a implementation of a physics-informed neural network (PINN)\n",
        "    for the Helmholtz partial differential equation (PDE).\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_widths: List[int]=[64, 64, 64], activation: Union[str, Callable]='tanh', **kwargs):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer_widths : List[int], optional\n",
        "            List of integers representing the widths of the hidden layers in the model.\n",
        "        activation : Union[str, Callable], optional\n",
        "            Activation function to be applied in each layer.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_sequence = [tf.keras.layers.Dense(width, activation=activation, kernel_initializer='glorot_normal') for width in layer_widths]\n",
        "        self.layer_sequence.append(tf.keras.layers.Dense(1, kernel_initializer='glorot_normal'))\n",
        "\n",
        "    def call(self, xy, training=None, mask=None):\n",
        "        x, y = xy[:, :1], xy[:, 1:]\n",
        "\n",
        "        u = Concatenate()([x, y])\n",
        "        for layer in self.layer_sequence:\n",
        "            u = layer(u)\n",
        "\n",
        "        dudxx, dudyy = compute_derivatives(x, y, u)\n",
        "\n",
        "        return tf.concat([u, dudxx, dudyy], axis=-1)"
      ],
      "metadata": {
        "id": "GhdeXm6gCmeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem definition\n",
        "Define the problem such that there exists an analytical solution, against which we can compare our PINN:\n",
        "\n",
        "$$\n",
        "    u(x, y) = sin(\\pi x) sin(4 \\pi y)\\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "vAsiRlEusgLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_val = lambda x, y: tf.math.sin(x * np.pi) * tf.math.sin(4 * y * np.pi)\n",
        "pde = HelmholtzPDE(u_val=u_val)\n",
        "pde.visualise()"
      ],
      "metadata": {
        "id": "MyX59mMSJRI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train without loss balancing\n",
        "Now that the PDE has been defined, we can build the model as well as the loss function. We are first using the default HelmholtzLoss and will compare it to the ReLoBRaLoHelmholtzLoss later."
      ],
      "metadata": {
        "id": "TH7S6dbnuw4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinn = HelmholtzPINN()\n",
        "loss = HelmholtzLoss(pde)\n",
        "pinn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss, metrics=[HelmholtzMetric(pde)])"
      ],
      "metadata": {
        "id": "AsYZRXP_uw-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = pinn.fit(\n",
        "    pde.get_train_dataset(),\n",
        "    epochs=1000,\n",
        "    steps_per_epoch=100,\n",
        "    callbacks=[\n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10, min_delta=0, verbose=True),\n",
        "        EarlyStopping(monitor='loss', patience=32, restore_best_weights=True, verbose=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "mkq2GK-lC7NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the progress of the several loss terms as well as the error against the analytical solution."
      ],
      "metadata": {
        "id": "IZBG10HMz0pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 4.5), dpi=100)\n",
        "plt.plot(np.log(h.history['L_f'])[:180], label='$L_f$ governing equation')\n",
        "plt.plot(np.log(h.history['L_b'])[:180], label='$L_{b}$ Dirichlet boundaries')\n",
        "plt.plot(np.log(h.history['L_u'])[:180], label='$L_u$ analytical solution')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Log-loss')\n",
        "plt.title('Loss evolution Helmholtz PDE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tAGKrQF5JJWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually inspect the error distribution on the physical domain."
      ],
      "metadata": {
        "id": "MseANbikz7Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pde.visualise(pinn)"
      ],
      "metadata": {
        "id": "HA48IVbbbtYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with ReLoBRaLo"
      ],
      "metadata": {
        "id": "ZI7IB3HfvTJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "helmholtz_pinn = HelmholtzPINN()\n",
        "relobralo_loss = ReLoBRaLoHelmholtzLoss(pde, temperature=0.01, rho=0.99, alpha=0.999)\n",
        "helmholtz_pinn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=relobralo_loss, metrics=[HelmholtzMetric(pde), ReLoBRaLoLambdaMetric(relobralo_loss)])"
      ],
      "metadata": {
        "id": "hSmRNrjLvFXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_relobralo = helmholtz_pinn.fit(\n",
        "    pde.get_train_dataset(),\n",
        "    epochs=1000,\n",
        "    steps_per_epoch=100,\n",
        "    callbacks=[\n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10, min_delta=0, verbose=True),\n",
        "        EarlyStopping(monitor='loss', patience=32, restore_best_weights=True, verbose=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "hoRv-tUvvFXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the progress of the several loss terms as well as the error against the analytical solution."
      ],
      "metadata": {
        "id": "OeSzC3QY0Ir7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 4.5), dpi=100)\n",
        "plt.plot(np.log(h.history['L_f']), label='$L_f$ governing equation')\n",
        "plt.plot(np.log(h.history['L_b']), label='$L_{b}$ Dirichlet boundaries')\n",
        "plt.plot(np.log(h.history['L_u']), label='$L_u$ analytical solution')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Log-loss')\n",
        "plt.title('Loss evolution Helmholtz PDE\\nwith ReLoBRaLo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "50yUto-yJlmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the progress of the scalings $\\lambda$."
      ],
      "metadata": {
        "id": "Nw-7yh180VXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 4.5), dpi=100)\n",
        "plt.plot(h_relobralo.history['L_f_lambda'], label='$\\lambda_f$ governing equation')\n",
        "plt.plot(np.array(h_relobralo.history['L_b_lambda']), label='$\\lambda_{b}$ boundary condition')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('scalings $\\lambda_i$')\n",
        "plt.title('ReLoBRaLo weights on Helmholtz PDE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ltut7yzc0TTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually inspect the error distribution on the physical domain."
      ],
      "metadata": {
        "id": "-51leBia0KEj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk85BkoosMKL"
      },
      "source": [
        "pde.visualise(helmholtz_pinn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vi1oQgurhjBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}